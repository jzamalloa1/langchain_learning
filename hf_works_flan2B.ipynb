{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import HuggingFace models into LangChain\n",
    "\n",
    "We'll test incorporate models from Huggingfance using the HuggingFaceHub API calls. In this particular example, we'll test out the Flan2B models\n",
    "\n",
    "Inspiration taken from: https://www.youtube.com/watch?v=VW5LBavIfY4&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial loading and running of a HF-hosted model through Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike before, we need to use Huggingface tokens to get access its models. We can obtain these tokens freely after registering through HF: https://huggingface.co/docs/hub/security-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a couple of different versions of Flan models: UL2 and T5 models. Flan UL2 is based on the T5 architecture. More info at:\n",
    "- UL2: https://huggingface.co/docs/transformers/model_doc/flan-ul2\n",
    "- T5: https://huggingface.co/docs/transformers/model_doc/flan-t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a higher temperature makes the model output more creative (too high and may hallucinate), and a lower temperature makes the output more deterministic\n",
    "\n",
    "<u>Note:</u> Documentation on model card recommends to add \"device_map='auto'\" when running pytorch model. This is attempted by introducing argument under \"kwargs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_ul2 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-ul2\",\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.1,\n",
    "        \"max_new_tokens\":256,\n",
    "        \"device_map\":\"auto\",\n",
    "        \"verbose\":\"True\"\n",
    "    }\n",
    ")\n",
    "\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xl\",\n",
    "    model_kwargs={\n",
    "        \"temperature\":0,\n",
    "        \"device_map\":\"auto\",\n",
    "        \"verbose\":\"True\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load conversation chains as beffore. We'll be using the ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the conversatin chain and interact with the LLM Agent as in the \"Conversation_Memory\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=flan_ul2,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I'm El Brian\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi, I'm El Brian. What can I help you with?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, I'm El Brian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the same functionality, like memory storage. Nothing's changed, just the LLM being used under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, I'm El Brian\n",
      "AI: Hi, I'm El Brian. What can I help you with?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm El Brian\n",
      "AI: Hi, I'm El Brian. What can I help you with?\n",
      "Human: How are you doing this evening?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm good. How about you?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"How are you doing this evening?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm El Brian\n",
      "AI: Hi, I'm El Brian. What can I help you with?\n",
      "Human: How are you doing this evening?\n",
      "AI: I'm good. How about you?\n",
      "Human: I need some help with travel plans within the US in the summer\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I can help you with that. What city are you interested in visiting?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I need some help with travel plans within the US in the summer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm El Brian\n",
      "AI: Hi, I'm El Brian. What can I help you with?\n",
      "Human: How are you doing this evening?\n",
      "AI: I'm good. How about you?\n",
      "Human: I need some help with travel plans within the US in the summer\n",
      "AI: I can help you with that. What city are you interested in visiting?\n",
      "Human: The midwest, any recommendations for a low budget travel?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I can help you with that. What city are you interested in visiting?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"The midwest, any recommendations for a low budget travel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm El Brian\n",
      "AI: Hi, I'm El Brian. What can I help you with?\n",
      "Human: How are you doing this evening?\n",
      "AI: I'm good. How about you?\n",
      "Human: I need some help with travel plans within the US in the summer\n",
      "AI: I can help you with that. What city are you interested in visiting?\n",
      "Human: The midwest, any recommendations for a low budget travel?\n",
      "AI: I can help you with that. What city are you interested in visiting?\n",
      "Human: I'm interested in travelling to North Carolina, any recommendations on where to stay?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I can help you with that. What city are you interested in visiting?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm interested in travelling to North Carolina, any recommendations on where to stay?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar responses to what we got before but more limited than gpt-3.5-turbo. For the purposes of this exercise, that is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token counting\n",
    "Let's consider for a moment that we are intersted on learning more about the number of tokens being used by the tokenizer in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the tokenizer for Flan UL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.35k/2.35k [00:00<00:00, 1.46MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 7.61MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.43M/2.43M [00:00<00:00, 16.1MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 665/665 [00:00<00:00, 3.39MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 7.59MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing a sample input, we can see the input IDs used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2018, 6, 27, 31, 51, 1289, 7798, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Hi, I'm El Brian\"\n",
    "\n",
    "print(tokenizer([test_input]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the actual token break-down. This makes it easy for us to count the number of tokens in our input sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hi', ',', '▁I', \"'\", 'm', '▁El', '▁Brian']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the memory, the text that is passed into the \"History\" portion of our LLM calls. So that now we can try to count at the tokens in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi, I'm El Brian\\nAI: Hi, I'm El Brian. What can I help you with?\\nHuman: How are you doing this evening?\\nAI: I'm good. How about you?\\nHuman: I need some help with travel plans within the US in the summer\\nAI: I can help you with that. What city are you interested in visiting?\\nHuman: The midwest, any recommendations for a low budget travel?\\nAI: I can help you with that. What city are you interested in visiting?\\nHuman: I'm interested in travelling to North Carolina, any recommendations on where to stay?\\nAI: I can help you with that. What city are you interested in visiting?\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we count, we need to consider that there is also an actual prompt being pass into the LLM call, so we need to find a way to include that as well\n",
    "\n",
    "We'll extract that by formatting the conversation prompt so that it includes the history (memory buffer). \"the next input\" here is just a dummy text to run the prompt and see what the entirity of text would have been like (this dummy text could be anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm El Brian\n",
      "AI: Hi, I'm El Brian. What can I help you with?\n",
      "Human: How are you doing this evening?\n",
      "AI: I'm good. How about you?\n",
      "Human: I need some help with travel plans within the US in the summer\n",
      "AI: I can help you with that. What city are you interested in visiting?\n",
      "Human: The midwest, any recommendations for a low budget travel?\n",
      "AI: I can help you with that. What city are you interested in visiting?\n",
      "Human: I'm interested in travelling to North Carolina, any recommendations on where to stay?\n",
      "AI: I can help you with that. What city are you interested in visiting?\n",
      "Human: the next input\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.format(input=\"the next input\", history=memory.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
