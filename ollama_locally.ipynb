{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing how to Run Ollama locally\n",
    "\n",
    "Testing dummy runs, prompting with LCEL, function calling and function calling with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser, JsonKeyOutputFunctionsParser\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import requests, json, os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy run with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model. Make sure to have followed instructions [here](https://github.com/jmorganca/ollama) to get started with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_llm = \"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize, but I\\'m a large language model, I cannot provide an answer to the question \"what is langsmith\" as it is not a valid or recognized term in any field of study. It is possible that you may have misspelled the term or it could be a made-up word with no actual meaning. Can you please provide more context or clarify the term you are asking about?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_template = \"\"\"\n",
    "\n",
    "Using internal knowledgre, answer in short the following question\n",
    "\n",
    "> {question}\n",
    "\n",
    "---\n",
    "if the question cannot be answered using your internal knowledge, simply summarize the text. Include all factual information, numbers, stats, etc.\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(summary_template)\n",
    "\n",
    "\n",
    "chain = summary_prompt | ChatOllama(model=ollama_llm, temperature=0) | StrOutputParser()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One Piece is a popular Japanese manga and anime series created by Eiichiro Oda. The story follows the adventures of Monkey D. Luffy, a young boy who becomes a pirate after eating the Gum-Gum Fruit, which gives him the ability to stretch and manipulate his body like rubber.\\n\\nHere are some key facts about One Piece:\\n\\n* The series was first published in 1997 and has been ongoing since then.\\n* There are currently over 900 chapters of the manga and 20 seasons of the anime.\\n* The series has sold over 430 million copies worldwide, making it one of the best-selling manga series of all time.\\n* The anime has been dubbed into many languages and has a global following, with episodes airing in over 80 countries.\\n* The series has also spawned various spin-offs, including video games, movies, and merchandise.\\n* The main character, Monkey D. Luffy, is known for his unique abilities and his strong sense of justice, which drives him to search for the ultimate treasure known as \"One Piece\" in order to become the Pirate King.\\n* Other notable characters include Roronoa Zoro, a skilled swordsman and the first member to join Luffy\\'s crew; Usopp, a talented marksman and storyteller; Sanji, a skilled cook and ladies\\' man; Tony Tony Chopper, a reindeer doctor who can transform into a human-sized version of himself; Nico Robin, an archaeologist and historian who can also transform into a giant bird; Franky, a cyborg shipwright who builds and repairs ships; and Brook, a skeleton musician who can play musical instruments with his bones.\\n* The series is known for its blend of action, adventure, comedy, and drama, as well as its colorful and imaginative world-building.\\n\\nI hope this information helps! Let me know if you have any other questions.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is One Piece?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the above works just as with the OpenAI model tested before [here](https://github.com/jzamalloa1/langchain_learning/blob/main/ra.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to interact with it using information from the web as a RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://blog.langchain.dev/announcing-langsmith/\" # example url to play with\n",
    "\n",
    "page_content = scrape_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Summarize the following question based on the context:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM applications. It provides a range of features to help developers build robust and reliable LLM-powered applications, including:\\n\\n1. Debugging: LangSmith gives developers full visibility into model inputs and output of every step in the chain of events, making it easy to experiment with new chains and prompt templates and spot the source of unexpected results, errors, or latency issues.\\n2. Testing: LangSmith integrates seamlessly with open-source evaluation modules, including heuristic evaluations and LLM evaluations. It also provides tools for creating datasets from traces and uploading them manually.\\n3. Evaluating: LangSmith provides a clear interface for letting developers easily see the inputs and outputs for each data point, which can help build intuition when interacting with LLMs.\\n4. Monitoring: LangSmith tracks system-level performance (such as latency and cost) and user feedback to help developers actively track performance and optimize it based on feedback.\\n\\nLangSmith is designed to be a single, fully-integrated hub for all LLM development needs, making it easier for teams to manage their LLM applications and build robust and reliable products.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ChatOllama(model=ollama_llm, temperature=0) | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\":\"what is langsmith\",\n",
    "    \"context\":page_content\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling in native"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a small example of Ollama using function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "        \"fav_food\": {\n",
    "            \"title\": \"Fav Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_template = [\n",
    "    HumanMessage(content=\"Use the following JSON schema when answering questions about people:\"),\n",
    "    HumanMessage(content=json.dumps(json_schema, indent=2)),\n",
    "    HumanMessage(content=\"Tell me about the following person: El Bryan is a guy who lives around the corner. He is 35 and likes Korean food\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n\"name\": \"El Bryan\",\\n\"age\": 35,\\n\"fav_food\": \"Korean food\"\\n}')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatOllama(\n",
    "    model=\"llama2\",\n",
    "    format=\"json\",\n",
    ")\n",
    "\n",
    "chat_response = chat_model(message_template)\n",
    "\n",
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'El Bryan', 'age': 35, 'fav_food': 'Korean food'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(chat_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Tell some key facts about {person} based on the context:\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "function_calls = [\n",
    "    {\n",
    "        \"name\": \"Person\",\n",
    "        \"description\": \"Identifying information about a person.\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": \n",
    "        {\n",
    "            \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "            \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "            \"fav_food\": {\n",
    "                \"title\": \"Fav Food\",\n",
    "                \"description\": \"The person's favorite food\",\n",
    "                \"type\": \"string\",\n",
    "                },\n",
    "        },\n",
    "        \"required\": [\"name\", \"age\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "ollama_model = ChatOllama(model=\"llama2\", temperature=0)\n",
    "\n",
    "person_chain = prompt | ollama_model.bind(function_call={\"name\":\"Person\"}, functions=function_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Ollama call failed with status code 400. Details: invalid options: function_call, functions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ollama_locally.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ollama_locally.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m person_chain\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ollama_locally.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39mperson\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mRoberto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ollama_locally.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m      \u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mDamian, who was 37, found that chinese food was best. Roberto is another guy who likes indian food better. Roberto is 41.\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ollama_locally.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ollama_locally.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain_core/runnables/base.py:1470\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1469\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1470\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1471\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1472\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1473\u001b[0m             patch_config(\n\u001b[1;32m   1474\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1475\u001b[0m             ),\n\u001b[1;32m   1476\u001b[0m         )\n\u001b[1;32m   1477\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain_core/runnables/base.py:2870\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m   2865\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2866\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   2867\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2868\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2869\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[0;32m-> 2870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   2871\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2872\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_configs(config),\n\u001b[1;32m   2873\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs},\n\u001b[1;32m   2874\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:160\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    151\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    157\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    159\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    161\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    162\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    163\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    164\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    165\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    166\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    167\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    168\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    169\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:481\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    474\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    475\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    479\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    480\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 481\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:370\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    369\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 370\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    371\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    372\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    373\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    374\u001b[0m ]\n\u001b[1;32m    375\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:360\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    358\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 360\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    361\u001b[0m                 m,\n\u001b[1;32m    362\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    363\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    364\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m         )\n\u001b[1;32m    367\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    368\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:514\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    515\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain/chat_models/ollama.py:98\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m        ])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_messages_as_text(messages)\n\u001b[0;32m---> 98\u001b[0m final_chunk \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_stream_with_aggregation(\n\u001b[1;32m     99\u001b[0m     prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    101\u001b[0m chat_generation \u001b[39m=\u001b[39m ChatGeneration(\n\u001b[1;32m    102\u001b[0m     message\u001b[39m=\u001b[39mAIMessage(content\u001b[39m=\u001b[39mfinal_chunk\u001b[39m.\u001b[39mtext),\n\u001b[1;32m    103\u001b[0m     generation_info\u001b[39m=\u001b[39mfinal_chunk\u001b[39m.\u001b[39mgeneration_info,\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain/llms/ollama.py:185\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    178\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    183\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GenerationChunk:\n\u001b[1;32m    184\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mfor\u001b[39;00m stream_resp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_stream(prompt, stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m    186\u001b[0m         \u001b[39mif\u001b[39;00m stream_resp:\n\u001b[1;32m    187\u001b[0m             chunk \u001b[39m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain/llms/ollama.py:170\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m    169\u001b[0m     optional_detail \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOllama call failed with status code \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Details: \u001b[39m\u001b[39m{\u001b[39;00moptional_detail\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines(decode_unicode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Ollama call failed with status code 400. Details: invalid options: function_call, functions"
     ]
    }
   ],
   "source": [
    "person_chain.invoke(\n",
    "    {\"person\":\"Roberto\",\n",
    "     \"context\":\"Damian, who was 37, found that chinese food was best. Roberto is another guy who likes indian food better. Roberto is 41.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ollama unfortunately cannot take in function calling through LCEL** It can only do it natively. OpenAI however can (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "person_chain = prompt | openai_llm.bind(function_call={\"name\":\"Person\"}, functions=function_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"name\": \"Roberto\",\\n  \"age\": 41,\\n  \"foodPreference\": \"Indian\"\\n}', 'name': 'Person'}})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_chain.invoke(\n",
    "    {\"person\":\"Roberto\",\n",
    "     \"context\":\"Damian, who was 37, found that chinese food was best. Roberto is another guy who likes indian food better. Roberto is 41.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go one step further and **parse the JSON LCEL output** (Documentation [here](https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Roberto', 'age': 41, 'foodPreference': 'Indian'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_chain = prompt | openai_llm.bind(function_call={\"name\":\"Person\"}, functions=function_calls) | JsonOutputFunctionsParser()\n",
    "\n",
    "person_chain.invoke(\n",
    "    {\"person\":\"Roberto\",\n",
    "     \"context\":\"Damian, who was 37, found that chinese food was best. Roberto is another guy who likes indian food better. Roberto is 41.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to specific call on a definition within the function call output you can do the below (I wouldn't recommend this since your output loses information, I would rather do this manually afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_chain = prompt | openai_llm.bind(function_call={\"name\":\"Person\"}, functions=function_calls) | JsonKeyOutputFunctionsParser(key_name=\"age\")\n",
    "\n",
    "person_chain.invoke(\n",
    "    {\"person\":\"Roberto\",\n",
    "     \"context\":\"Damian, who was 37, found that chinese food was best. Roberto is another guy who likes indian food better. Roberto is 41.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
