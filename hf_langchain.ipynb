{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running HF Models through API and Locally in Langchain\n",
    "\n",
    "Here we (1) test the inference API from huggingface to call a model output running in the HF server and (2) learn how we can import that model and run it locally on our macchines. All compatible with Langchanin tools\n",
    "\n",
    "Inspiration taken from: https://www.youtube.com/watch?v=Kn7SX2Mx_Jk&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReadMe material\n",
    "\n",
    "- [Inference in Large Language Models](https://medium.com/@andrew_johnson_4/understanding-inference-in-large-language-models-f4a4a4a736a5) - Insightful definition of text inference for LLMs (no need to pay for entire article, definition in first paragraphs)\n",
    "- [Causal Language Modeling](https://huggingface.co/docs/transformers/tasks/language_modeling). Used for text generation by predicting the next token in a sequence of tokens, unlike the other type of language modeling, masked models.\n",
    "- [Differences between encoder-only, decoder-only and encoder-decoder models](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder) - All of these are sequence-to-sequence models (i.e. <i>seq2seq</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to the inference API first - HuggingfaceHub, that is, non-local model running\n",
    "\n",
    "Recall we tested this first [here](https://github.com/jzamalloa1/langchain_learning/blob/main/hf_works_flan2B.ipynb)\n",
    "\n",
    "This works well for many of the Huggingface hosted models, but doesn't support all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build prompt as we have learned [before](https://github.com/jzamalloa1/langchain_learning/blob/main/agents.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let's think through this step by step\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And instantiate model through HuggingFaceHub and start LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xxl\",\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.1,\n",
    "        \"max_new_tokens\":256,\n",
    "        \"verbose\":\"True\" # Not sure if this is needed\n",
    "    }\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Question: Where does the oldest cat in the world live?\n",
      "\n",
      "Answer: Let's think through this step by step\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The oldest cat in the world is a female named Snowball. Snowball lives in the United States. The United States is a country in North America. So, the answer is the United States.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(\"Where does the oldest cat in the world live?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sort of see that is showing it's train of thought. We can try something a bit more complex below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Question: What is the coldest month in the US?\n",
      "\n",
      "Answer: Let's think through this step by step\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The coldest month in the US is January. The average temperature in January is -2 degrees celsius. So, the answer is January.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(\"What is the coldest month in the US?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw above how one can easily call the HuggingFaceHub API to call models non-locally. Let's try another model to see where limitations might exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n__root__\n  Got invalid task conversational, currently only ('text2text-generation', 'text-generation', 'summarization') are supported (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m blender_llm_model \u001b[39m=\u001b[39m HuggingFaceHub(\n\u001b[1;32m      2\u001b[0m     repo_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/blenderbot-1B-distill\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmax_new_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m256\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mverbose\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mTrue\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m# Not sure if this is needed\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     }\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m blenderbot_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[1;32m     11\u001b[0m     llm\u001b[39m=\u001b[39mblender_llm_model,\n\u001b[1;32m     12\u001b[0m     prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m     13\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n__root__\n  Got invalid task conversational, currently only ('text2text-generation', 'text-generation', 'summarization') are supported (type=value_error)"
     ]
    }
   ],
   "source": [
    "blender_llm_model = HuggingFaceHub(\n",
    "    repo_id=\"facebook/blenderbot-1B-distill\",\n",
    "    model_kwargs={\n",
    "        \"temperature\":0.1,\n",
    "        \"max_new_tokens\":256,\n",
    "        \"verbose\":\"True\" # Not sure if this is needed\n",
    "    }\n",
    ")\n",
    "\n",
    "blenderbot_chain = LLMChain(\n",
    "    llm=blender_llm_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the error above: <u>currently only ('text2text-generation', 'text-generation', 'summarization') are supported</u>. The BlenderBot model is a [conversation AI chatbot type model](https://huggingface.co/docs/transformers/model_doc/blenderbot), which aims to converse with user. This type, unlike text2text and text generation, is not supported by HuggingFaceHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running models locally\n",
    "\n",
    "Some of the advantages of running models locally are: (1) fine-tuning models to own data, (2) can use own GPU and (3) run models that cannot be ran through API (like model above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HuggingfacePipeline class will allow us to run LLMs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a model locally. We are going to use a decoder-encoder model: T5-Flan\n",
    "\n",
    "The AutoModel loader loads the seq2seq model, in this example this is the hybrid decoder-encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 3.13G/3.13G [01:53<00:00, 27.5MB/s]\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 474kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup the <b>pipeline</b> to specify the <u>task</u> (see [pipeline documentation](https://huggingface.co/docs/transformers/pipeline_tutorial) for various tasks), the model and the tokenizer. The task here: <u>text2text is for encoder-decoder usage</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task = \"text2text-generation\", #explore tasks in link above\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to pass it to the Huggingface pipeline to use langchain as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = HuggingFacePipeline(pipeline=pipe) # Ready to be integrate into langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin\n"
     ]
    }
   ],
   "source": [
    "print(llm_model(\"What is the capital of Germany?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can integrate into langchain as we have [previously](https://github.com/jzamalloa1/langchain_learning/blob/main/tools_and_chains.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Question: What is the capital of Germany?\n",
      "\n",
      "Answer: Let's think through this step by step\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin. Berlin is located in Germany. So, the answer is Berlin.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm_chain.predict(question=\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a Decoder model only, like GPT-2. This needs to be setup slightly different. Given that this is a <u>decoder model we'll use the \"text-generation\" task</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Question: What is the capital of Germany?\n",
      "\n",
      "Answer: Let's think through this step by step\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "First of all,\n",
      "\n",
      "We must look at where this capital came from [1,2].\n",
      "\n",
      "This capital came from France. And this was a real financial capital which was part of the French political economy…\n",
      "\n",
      "It started out in a kind of financial crisis [revised version below]. A financial crisis of course meant a major bank failure. For the first time the French nationalised part of the banks. But they didn't try to sell on a huge debt pile – and that was the beginning of the financial markets collapsing.\n",
      "\n",
      "…but France then didn't try to sell off the debt pile. This was because there really wasn't this huge commercial debt pile that they had [revised version below]: their debts were around 1.24% of GDP – and this meant that they were under debt-servicing.\n",
      "\n",
      "[revised version below] – and this meant that they were under debt-servicing. France did buy out French debtors, through the nationalization they had, and this was the first of many cases of nationalization. This was part of the French political economy, which was very much influenced by European economics and very much influenced by the Eurozone. [revised version below]\n",
      "\n",
      "There was an opportunity cost to nationalization… [revised version below]\n",
      "\n",
      "And from this opportunity cost we got [revised version below]\n",
      "\n",
      "And from this it goes to one of the interesting cases – [revised version below]\n",
      "\n",
      "France did put some capital into their national debt, that they had nationalized, but they also did something called the European Stability Mechanism, which meant that Europe subsidised all of these nationalizations, and the nationalization of a German bank was, I mean, just the beginning. This happened over a period of thirty or forty years [revised version below]. It had this amazing impact of introducing capital, and it allowed German banks to be put back together – again. I mean this doesn't actually account for the massive expansion of the German economy afterwards, because there came another stage. Of course it has a very short-term growth phase, but it did have a very long-term impact.\n",
      "\n",
      "So there was a kind of nationalization and also a re-nationalization of a financial institution – we are talking here about the major German banks.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.predict(question=\"What is the capital of Germany?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...not exactly a great model (it is several months old now), but, it fullfil the purpose of being tested locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
