{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning OpenAI Model Exercise\n",
    "\n",
    "We'll finetune an OpenAI model with custom data to create a custom agent\n",
    "\n",
    "Inspiration taken from [here](https://www.youtube.com/watch?v=boHXgQ5eQic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import huggingface_hub\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of datasets ready for loading from the [HuggingfaceHub](https://huggingface.co/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetInfo(id='acronym_identification', author=None, sha='15ef643450d589d5883e289ffadeb03563e80a9e', created_at=datetime.datetime(2022, 3, 2, 23, 29, 22, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 1, 9, 11, 39, 57, tzinfo=datetime.timezone.utc), private=False, gated=False, disabled=False, downloads=1005, likes=17, paperswithcode_id='acronym-identification', tags=['task_categories:token-classification', 'annotations_creators:expert-generated', 'language_creators:found', 'multilinguality:monolingual', 'size_categories:10K<n<100K', 'source_datasets:original', 'language:en', 'license:mit', 'acronym-identification', 'arxiv:2010.14678', 'region:us'], card_data=None, siblings=None),\n",
       " DatasetInfo(id='ade_corpus_v2', author=None, sha='4ba01c71687dd7c996597042449448ea312126cf', created_at=datetime.datetime(2022, 3, 2, 23, 29, 22, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 1, 9, 11, 42, 58, tzinfo=datetime.timezone.utc), private=False, gated=False, disabled=False, downloads=2973, likes=21, paperswithcode_id=None, tags=['task_categories:text-classification', 'task_categories:token-classification', 'task_ids:coreference-resolution', 'task_ids:fact-checking', 'annotations_creators:expert-generated', 'language_creators:found', 'multilinguality:monolingual', 'size_categories:10K<n<100K', 'size_categories:1K<n<10K', 'size_categories:n<1K', 'source_datasets:original', 'language:en', 'license:unknown', 'region:us'], card_data=None, siblings=None)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in huggingface_hub.list_datasets()][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the following example [train dataset](https://huggingface.co/datasets/jamescalam/agent-conversations-retrieval-tool). This training messages were generated by GPT-4\n",
    "\n",
    "**Note:** One can build their own dataset, it is not difficult to implement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.11M/1.11M [00:00<00:00, 12.1MB/s]\n",
      "Generating train split: 270 examples [00:00, 41089.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "new_data = load_dataset(\n",
    "    \"jamescalam/agent-conversations-retrieval-tool\",\n",
    "    split=\"train\" # which split to return\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 270 messages, each containg a list of roles and content\n",
    "\n",
    "**Each message represents a single conversation**. Note that all contain the same system message (which we can modify for our particular chatbot we want to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 270\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first message has a set of exchanges between the user and assistant **roles** with **content** examples of how the assistant should behave when prompted with a type of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Vector Search Tool: This tool allows you to get research information about LLMs.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\ The action to take. Must be one of Vector Search Tool\\n    \"action_input\": string \\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWhat is the main focus of the technical report IDSIA-01-11?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '```json\\n{\\n    \"action\": \"Vector Search Tool\",\\n    \"action_input\": \"Main focus of technical report IDSIA-01-11\"\\n}\\n```'},\n",
       "  {'role': 'user',\n",
       "   'content': '[High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but]'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": \"The main focus of the technical report IDSIA-01-11 is the presentation of a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants for visual object classification.\"\\n}\\n```'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(new_data[0]))\n",
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Vector Search Tool: This tool allows you to get research information about LLMs.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\ The action to take. Must be one of Vector Search Tool\\n    \"action_input\": string \\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWhat is the main focus of the technical report IDSIA-01-11?'}\n"
     ]
    }
   ],
   "source": [
    "print(new_data[0][\"messages\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOLS\n",
      "------\n",
      "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n",
      "\n",
      "> Vector Search Tool: This tool allows you to get research information about LLMs.\n",
      "\n",
      "RESPONSE FORMAT INSTRUCTIONS\n",
      "----------------------------\n",
      "\n",
      "When responding to me, please output a response in one of two formats:\n",
      "\n",
      "**Option 1:**\n",
      "Use this if you want the human to use a tool.\n",
      "Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"action\": string, \\ The action to take. Must be one of Vector Search Tool\n",
      "    \"action_input\": string \\ The input to the action\n",
      "}\n",
      "```\n",
      "\n",
      "**Option #2:**\n",
      "Use this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": string \\ You should put what you want to return to use here\n",
      "}\n",
      "```\n",
      "\n",
      "USER'S INPUT\n",
      "--------------------\n",
      "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
      "\n",
      "What is the main focus of the technical report IDSIA-01-11?\n"
     ]
    }
   ],
   "source": [
    "print(new_data[0][\"messages\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the training data to json1 format\n",
    "\n",
    "**Note on JSON1 extension:** One way to store JSON for SQLite: [link](https://www.beekeeperstudio.io/blog/sqlite-json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 72.81ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1103809"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.to_json(\"TRAINING_DATA/jc_conversations_train.json1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we explored and stored the training data, let's go ahead and train finetune base gpt-3.5\n",
    "\n",
    "**Note:** If you want to check the validity of the data for training you created, you can do so as stated [here](https://cookbook.openai.com/examples/chat_finetuning_data_prep)\n",
    "\n",
    "Check out OpenAI's [finetuning training material](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload our data for training to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_load = client.files.create(\n",
    "    file=open(\"TRAINING_DATA/jc_conversations_train.json1\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-RrC6cpvDUBdq8PA41EumgsG0', bytes=1103809, created_at=1705791884, filename='jc_conversations_train.json1', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'file-RrC6cpvDUBdq8PA41EumgsG0',\n",
       " 'bytes': 1103809,\n",
       " 'created_at': 1705791884,\n",
       " 'filename': 'jc_conversations_train.json1',\n",
       " 'object': 'file',\n",
       " 'purpose': 'fine-tune',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_load.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the file ID and create the finetuning job. We'll **finetune base gpt-3.5-turbo** in this example.\n",
    "\n",
    "This might take some time to train since it might behind a queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetune = client.fine_tuning.jobs.create(training_file=file_load.dict()[\"id\"],\n",
    "                                                model=\"gpt-3.5-turbo\",\n",
    "                                                suffix=\"012024_jc_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ftjob-nUFfwPtcCy6rMJ5ONgi99i4n',\n",
       " 'created_at': 1705793231,\n",
       " 'error': None,\n",
       " 'fine_tuned_model': None,\n",
       " 'finished_at': None,\n",
       " 'hyperparameters': {'n_epochs': 'auto',\n",
       "  'batch_size': 'auto',\n",
       "  'learning_rate_multiplier': 'auto'},\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'object': 'fine_tuning.job',\n",
       " 'organization_id': 'org-HCpyLwy1gf2wzE49sCGQRU79',\n",
       " 'result_files': [],\n",
       " 'status': 'validating_files',\n",
       " 'trained_tokens': None,\n",
       " 'training_file': 'file-RrC6cpvDUBdq8PA41EumgsG0',\n",
       " 'validation_file': None}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_finetune.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your **finetuning status** (here)[https://platform.openai.com/finetune] or use the model ID with the API\n",
    "\n",
    "Check parameter **finished_at**. If None, then it still not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ftjob-nUFfwPtcCy6rMJ5ONgi99i4n',\n",
       " 'created_at': 1705793231,\n",
       " 'error': None,\n",
       " 'fine_tuned_model': None,\n",
       " 'finished_at': None,\n",
       " 'hyperparameters': {'n_epochs': 3,\n",
       "  'batch_size': 1,\n",
       "  'learning_rate_multiplier': 2},\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'object': 'fine_tuning.job',\n",
       " 'organization_id': 'org-HCpyLwy1gf2wzE49sCGQRU79',\n",
       " 'result_files': [],\n",
       " 'status': 'running',\n",
       " 'trained_tokens': None,\n",
       " 'training_file': 'file-RrC6cpvDUBdq8PA41EumgsG0',\n",
       " 'validation_file': None}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve(model_finetune.dict()[\"id\"]).dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it has finished running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ftjob-nUFfwPtcCy6rMJ5ONgi99i4n',\n",
       " 'created_at': 1705793231,\n",
       " 'error': None,\n",
       " 'fine_tuned_model': 'ft:gpt-3.5-turbo-0613:personal:012024-jc-sample:8jFZc3IB',\n",
       " 'finished_at': 1705795315,\n",
       " 'hyperparameters': {'n_epochs': 3,\n",
       "  'batch_size': 1,\n",
       "  'learning_rate_multiplier': 2},\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'object': 'fine_tuning.job',\n",
       " 'organization_id': 'org-HCpyLwy1gf2wzE49sCGQRU79',\n",
       " 'result_files': ['file-Rshy7mfnCMeqTDgLWyoAqiBf'],\n",
       " 'status': 'succeeded',\n",
       " 'trained_tokens': 677358,\n",
       " 'training_file': 'file-RrC6cpvDUBdq8PA41EumgsG0',\n",
       " 'validation_file': None}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve(model_finetune.dict()[\"id\"]).dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-nUFfwPtcCy6rMJ5ONgi99i4n', created_at=1705793231, error=None, fine_tuned_model='ft:gpt-3.5-turbo-0613:personal:012024-jc-sample:8jFZc3IB', finished_at=1705795315, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-HCpyLwy1gf2wzE49sCGQRU79', result_files=['file-Rshy7mfnCMeqTDgLWyoAqiBf'], status='succeeded', trained_tokens=677358, training_file='file-RrC6cpvDUBdq8PA41EumgsG0', validation_file=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve(model_finetune.dict()[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pull the model name itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-0613:personal:012024-jc-sample:8jFZc3IB'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve(model_finetune.dict()[\"id\"]).dict()[\"fine_tuned_model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can take a look at the metrics of the model to see its performance during training, such as its train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epochs': 3, 'batch_size': 1, 'learning_rate_multiplier': 2}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve(model_finetune.dict()[\"id\"]).dict()[\"hyperparameters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the result file names from the finetuned object to retrieve their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file-Rshy7mfnCMeqTDgLWyoAqiBf']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_files = client.fine_tuning.jobs.retrieve(model_finetune.dict()[\"id\"]).dict()[\"result_files\"]\n",
    "result_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind these are metrics from the training procedure, not testing on an external dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/lf090n1d5wz8rqsjs80m1fg80000gn/T/ipykernel_25926/4125317921.py:1: DeprecationWarning: The `.content()` method should be used instead\n",
      "  train_metrics = client.files.retrieve_content(result_files[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy\\n1,0.72572,0.83133,,\\n2,0.64681,0.86667,,\\n3,0.51859,0.89809,,\\n4,0.33327,0.9403,,\\n5,0.22332,0.96815,,\\n6,0.31847,0.9403,,\\n7,0.31428,0.95,,\\n8,0.29801,0.9403,,\\n9,0.53667,0.83333,,\\n10,0.24963,0.97015,,\\n11,0.42805,0.89147,,\\n12,0.32082,0.9,,\\n13,0.52567,0.91803,,\\n14,0.53961,0.91964,,\\n15,0.39994,0.87368,,\\n16,0.42182,0.87719,,\\n17,0.13842,0.97015,,\\n18,0.54742,0.88636,,\\n19,0.14782,0.95082,,\\n20,0.2264,0.9125,,\\n21,0.32489,0.93137,,\\n22,0.43328,0.91139,,\\n23,0.13255,0.96552,,\\n24,0.38074,0.94898,,\\n25,0.27745,0.91549,,\\n26,0.02666,0.98507,,\\n27,0.01851,0.98507,,\\n28,0.23417,0.9469,,\\n29,0.19434,0.9625,,\\n30,0.00821,1.0,,\\n31,0.31149,0.91781,,\\n32,0.08018,0.96809,,\\n33,0.14823,0.95062,,\\n34,0.15808,0.96226,,\\n35,0.12645,0.97101,,\\n36,0.16169,0.95699,,\\n37,0.73251,0.90816,,\\n38,0.15836,0.95294,,\\n39,0.00204,1.0,,\\n40,0.33968,0.87931,,\\n41,0.24433,0.90385,,\\n42,0.39227,0.86735,,\\n43,0.23507,0.9359,,\\n44,0.31601,0.90845,,\\n45,0.44038,0.89157,,\\n46,0.22873,0.93617,,\\n47,0.09475,0.96078,,\\n48,0.44477,0.87943,,\\n49,0.11556,0.98485,,\\n50,0.17515,0.97403,,\\n51,0.11657,0.97826,,\\n52,0.51878,0.92,,\\n53,0.10889,0.97248,,\\n54,0.26412,0.90991,,\\n55,0.09377,0.96739,,\\n56,0.27306,0.91216,,\\n57,0.02121,1.0,,\\n58,0.31447,0.9186,,\\n59,0.14801,0.93939,,\\n60,0.1048,0.98837,,\\n61,0.14951,0.94318,,\\n62,0.14852,0.93939,,\\n63,0.10547,0.97222,,\\n64,0.37961,0.82569,,\\n65,0.15099,0.93846,,\\n66,0.13194,0.94495,,\\n67,0.10568,0.96875,,\\n68,0.36865,0.91071,,\\n69,0.11316,0.97209,,\\n70,0.00361,1.0,,\\n71,0.18628,0.92553,,\\n72,0.15801,0.97059,,\\n73,0.00128,1.0,,\\n74,0.00091,1.0,,\\n75,0.00059,1.0,,\\n76,0.42935,0.91111,,\\n77,0.1049,0.95238,,\\n78,0.00021,1.0,,\\n79,0.24925,0.94059,,\\n80,0.06764,0.97778,,\\n81,0.34891,0.90132,,\\n82,0.56571,0.83193,,\\n83,4e-05,1.0,,\\n84,0.19485,0.92857,,\\n85,0.36894,0.94937,,\\n86,0.05213,0.97368,,\\n87,0.23474,0.91339,,\\n88,0.08363,0.97368,,\\n89,0.33894,0.88636,,\\n90,0.25574,0.92308,,\\n91,0.1813,0.95455,,\\n92,0.24367,0.95652,,\\n93,0.26432,0.91791,,\\n94,0.03805,0.98864,,\\n95,0.21315,0.94949,,\\n96,0.08583,0.975,,\\n97,0.06003,0.97531,,\\n98,0.30824,0.93333,,\\n99,0.13424,0.94118,,\\n100,0.10239,0.94595,,\\n101,0.04273,0.98551,,\\n102,0.07864,0.97143,,\\n103,0.16323,0.96296,,\\n104,0.26789,0.91473,,\\n105,0.09535,0.96667,,\\n106,0.25062,0.9,,\\n107,0.24712,0.9505,,\\n108,0.06798,0.96341,,\\n109,0.21272,0.90598,,\\n110,0.2087,0.95522,,\\n111,0.03601,1.0,,\\n112,0.11721,0.94118,,\\n113,1e-05,1.0,,\\n114,1e-05,1.0,,\\n115,0.24522,0.93396,,\\n116,0.15994,0.94262,,\\n117,0.16971,0.95604,,\\n118,0.16979,0.95604,,\\n119,0.0,1.0,,\\n120,0.25324,0.95238,,\\n121,0.13967,0.94167,,\\n122,0.0,1.0,,\\n123,0.0,1.0,,\\n124,0.13477,0.96403,,\\n125,0.15214,0.95455,,\\n126,0.25365,0.97222,,\\n127,0.2838,0.92135,,\\n128,0.10798,0.95614,,\\n129,0.11947,0.95556,,\\n130,0.31475,0.925,,\\n131,0.15045,0.95,,\\n132,0.0,1.0,,\\n133,0.24428,0.94215,,\\n134,0.12422,0.96154,,\\n135,0.05282,0.98529,,\\n136,0.87958,0.7913,,\\n137,0.0,1.0,,\\n138,0.23864,0.94079,,\\n139,0.09584,0.97802,,\\n140,0.16894,0.95588,,\\n141,0.14841,0.96875,,\\n142,0.0,1.0,,\\n143,0.20878,0.92929,,\\n144,0.13322,0.96491,,\\n145,0.24179,0.9322,,\\n146,0.14331,0.95522,,\\n147,0.18601,0.93421,,\\n148,0.19853,0.92683,,\\n149,0.07846,0.97674,,\\n150,0.11215,0.97333,,\\n151,0.08429,0.96923,,\\n152,0.22899,0.91156,,\\n153,0.20463,0.925,,\\n154,0.19667,0.90244,,\\n155,0.0,1.0,,\\n156,0.13411,0.95575,,\\n157,0.11162,0.94805,,\\n158,0.1244,0.98148,,\\n159,0.37424,0.93023,,\\n160,0.0,1.0,,\\n161,0.06235,0.97333,,\\n162,0.09743,0.9661,,\\n163,0.14394,0.95556,,\\n164,0.1792,0.98,,\\n165,0.04534,0.98361,,\\n166,0.13422,0.94186,,\\n167,0.06861,0.97468,,\\n168,0.31251,0.91358,,\\n169,0.0,1.0,,\\n170,0.0,1.0,,\\n171,0.49028,0.87179,,\\n172,0.097,0.95775,,\\n173,0.37459,0.88298,,\\n174,0.15966,0.96667,,\\n175,0.26275,0.9,,\\n176,0.0,1.0,,\\n177,0.19939,0.94118,,\\n178,0.15286,0.93878,,\\n179,0.05959,0.97872,,\\n180,0.04694,0.98529,,\\n181,0.2201,0.94828,,\\n182,0.15229,0.94872,,\\n183,0.17356,0.94828,,\\n184,0.24064,0.9375,,\\n185,0.16421,0.96875,,\\n186,0.16042,0.9359,,\\n187,0.15873,0.94167,,\\n188,0.24432,0.91026,,\\n189,0.07301,0.94118,,\\n190,0.03465,0.9878,,\\n191,0.08176,0.97382,,\\n192,0.01222,1.0,,\\n193,0.21588,0.97917,,\\n194,0.21376,0.93243,,\\n195,0.11437,0.94805,,\\n196,0.19463,0.94898,,\\n197,0.10128,0.94737,,\\n198,0.11527,0.95,,\\n199,0.2918,0.89189,,\\n200,0.09663,0.97753,,\\n201,0.07625,0.97101,,\\n202,0.1121,0.97674,,\\n203,0.1433,0.97268,,\\n204,0.26683,0.91919,,\\n205,0.11164,0.95946,,\\n206,0.051,0.98795,,\\n207,0.06075,0.97222,,\\n208,0.12571,0.95181,,\\n209,0.08224,0.96471,,\\n210,0.09974,0.96203,,\\n211,0.0862,0.95699,,\\n212,0.06507,0.98507,,\\n213,0.17288,0.92982,,\\n214,0.08566,0.95349,,\\n215,0.16667,0.95161,,\\n216,0.07899,0.96923,,\\n217,0.57646,0.88406,,\\n218,0.07947,0.97351,,\\n219,0.23446,0.92,,\\n220,0.42252,0.88148,,\\n221,0.37948,0.87413,,\\n222,0.34511,0.92683,,\\n223,0.2987,0.94915,,\\n224,0.0,1.0,,\\n225,0.10279,0.95238,,\\n226,0.11274,0.96825,,\\n227,0.14018,0.93636,,\\n228,0.11217,0.96429,,\\n229,0.15403,0.94286,,\\n230,0.23727,0.91525,,\\n231,0.32016,0.9,,\\n232,0.42266,0.90909,,\\n233,0.5469,0.83465,,\\n234,0.28656,0.90833,,\\n235,0.10306,0.95833,,\\n236,0.24608,0.92593,,\\n237,0.07178,0.97368,,\\n238,0.04232,1.0,,\\n239,0.08735,0.96471,,\\n240,0.0924,0.96774,,\\n241,0.06099,0.95833,,\\n242,0.3798,0.90909,,\\n243,0.18743,0.9359,,\\n244,0.27302,0.91503,,\\n245,0.0,1.0,,\\n246,0.0751,0.96,,\\n247,0.40946,0.95062,,\\n248,0.22354,0.9386,,\\n249,0.22597,0.93684,,\\n250,0.25062,0.97101,,\\n251,0.06605,0.97727,,\\n252,0.31355,0.97222,,\\n253,0.13554,0.9569,,\\n254,0.0,1.0,,\\n255,0.36916,0.88667,,\\n256,0.15123,0.95276,,\\n257,0.59864,0.86408,,\\n258,0.05159,0.9823,,\\n259,0.0,1.0,,\\n260,0.05168,0.98261,,\\n261,0.11605,0.97059,,\\n262,0.04984,0.98734,,\\n263,0.30301,0.92708,,\\n264,0.0,1.0,,\\n265,0.07551,0.98571,,\\n266,0.17524,0.93805,,\\n267,0.31194,0.91339,,\\n268,0.06674,0.96341,,\\n269,0.09786,0.97938,,\\n270,0.0,1.0,,\\n271,0.0,1.0,,\\n272,0.10412,0.96491,,\\n273,0.07798,0.96667,,\\n274,0.01348,1.0,,\\n275,0.11435,0.95522,,\\n276,0.12025,0.97297,,\\n277,0.05551,0.97938,,\\n278,0.09846,0.97872,,\\n279,0.04223,0.98,,\\n280,0.07075,0.96667,,\\n281,0.20973,0.925,,\\n282,0.0363,0.98925,,\\n283,0.0,1.0,,\\n284,0.07896,0.9726,,\\n285,0.0,1.0,,\\n286,0.04896,0.97647,,\\n287,0.29429,0.92553,,\\n288,0.0,1.0,,\\n289,0.20838,0.90541,,\\n290,0.11341,0.95522,,\\n291,0.03855,0.98824,,\\n292,0.08506,0.9697,,\\n293,0.18995,0.94667,,\\n294,0.20471,0.93846,,\\n295,0.14521,0.94558,,\\n296,0.03307,0.99153,,\\n297,0.16761,0.91818,,\\n298,0.0,1.0,,\\n299,0.02026,1.0,,\\n300,0.04578,0.9902,,\\n301,0.12944,0.9661,,\\n302,0.0,1.0,,\\n303,0.0804,0.98148,,\\n304,0.10222,0.96491,,\\n305,0.06295,0.98529,,\\n306,0.09518,0.96491,,\\n307,0.36067,0.87179,,\\n308,0.02826,0.9875,,\\n309,0.31967,0.93023,,\\n310,0.01116,1.0,,\\n311,0.02442,0.98214,,\\n312,0.08862,0.96774,,\\n313,0.19801,0.94366,,\\n314,0.30284,0.8951,,\\n315,0.10565,0.95455,,\\n316,0.04886,0.98837,,\\n317,0.06328,0.98,,\\n318,0.11665,0.9596,,\\n319,0.06386,0.98148,,\\n320,0.14062,0.94366,,\\n321,0.08739,0.97436,,\\n322,0.10287,0.97674,,\\n323,0.13005,0.94737,,\\n324,0.10596,0.975,,\\n325,0.18503,0.95,,\\n326,0.0901,0.94505,,\\n327,0.18369,0.95652,,\\n328,0.06644,0.97959,,\\n329,0.10879,0.96341,,\\n330,0.15531,0.98148,,\\n331,0.0219,0.98276,,\\n332,0.02111,1.0,,\\n333,0.11016,0.94488,,\\n334,0.0,1.0,,\\n335,0.13913,0.9505,,\\n336,0.08925,0.97802,,\\n337,0.31366,0.95062,,\\n338,0.19608,0.92929,,\\n339,0.20146,0.93701,,\\n340,0.03164,0.98864,,\\n341,0.36868,0.92562,,\\n342,0.04121,0.97222,,\\n343,0.04805,0.97826,,\\n344,0.07426,0.98649,,\\n345,0.06615,0.97403,,\\n346,0.21556,0.94118,,\\n347,0.29792,0.92562,,\\n348,0.04899,0.98876,,\\n349,0.0526,0.97368,,\\n350,0.07038,0.98824,,\\n351,0.20607,0.94904,,\\n352,0.0,1.0,,\\n353,0.12009,0.98529,,\\n354,0.09735,0.96552,,\\n355,0.11804,0.96875,,\\n356,0.06676,0.97297,,\\n357,0.14179,0.98551,,\\n358,0.5887,0.81739,,\\n359,0.23036,0.9375,,\\n360,0.07596,0.97727,,\\n361,0.35109,0.91304,,\\n362,0.22652,0.98611,,\\n363,0.0,1.0,,\\n364,0.25483,0.95122,,\\n365,0.11622,0.96721,,\\n366,0.03181,0.97674,,\\n367,0.05417,0.9759,,\\n368,0.25909,0.92771,,\\n369,0.09285,0.95575,,\\n370,0.04305,0.98701,,\\n371,0.25143,0.93333,,\\n372,0.01421,1.0,,\\n373,0.06766,0.96226,,\\n374,0.03314,1.0,,\\n375,0.06272,0.98529,,\\n376,0.11312,0.97345,,\\n377,0.07909,0.97273,,\\n378,0.07654,0.97,,\\n379,0.05153,0.97279,,\\n380,0.28084,0.88889,,\\n381,0.08641,0.94872,,\\n382,0.01915,1.0,,\\n383,0.10344,0.9604,,\\n384,0.0427,0.97674,,\\n385,0.02354,0.99338,,\\n386,0.0,1.0,,\\n387,0.0,1.0,,\\n388,0.03652,0.98571,,\\n389,0.0,1.0,,\\n390,0.15677,0.96939,,\\n391,0.15391,0.95105,,\\n392,0.02502,0.9916,,\\n393,0.11869,0.96226,,\\n394,0.31125,0.88725,,\\n395,0.03538,0.98438,,\\n396,0.0,1.0,,\\n397,0.0578,0.98039,,\\n398,0.12969,0.96667,,\\n399,0.18987,0.92308,,\\n400,0.18222,0.95556,,\\n401,0.28775,0.91489,,\\n402,0.04654,0.98039,,\\n403,0.16762,0.94595,,\\n404,0.03418,0.98507,,\\n405,0.15378,0.95098,,\\n406,0.0385,0.98953,,\\n407,0.07579,0.98165,,\\n408,0.09672,0.95745,,\\n409,0.18577,0.92424,,\\n410,0.08886,0.96923,,\\n411,0.01515,1.0,,\\n412,0.0342,1.0,,\\n413,0.01795,0.99363,,\\n414,0.06991,0.98438,,\\n415,0.03572,0.98485,,\\n416,0.37336,0.89076,,\\n417,0.05448,0.97531,,\\n418,0.18541,0.93103,,\\n419,0.1741,0.94286,,\\n420,0.04537,0.97849,,\\n421,0.00958,1.0,,\\n422,0.02148,1.0,,\\n423,0.07502,0.97122,,\\n424,0.12154,0.9625,,\\n425,0.08685,0.97814,,\\n426,0.21517,0.93578,,\\n427,0.2496,0.92763,,\\n428,0.02152,1.0,,\\n429,0.06584,0.98851,,\\n430,0.03681,0.98551,,\\n431,0.01474,1.0,,\\n432,0.23222,0.95,,\\n433,0.05229,0.98519,,\\n434,0.03668,0.99265,,\\n435,0.04094,0.96491,,\\n436,0.1718,0.92857,,\\n437,0.15204,0.94737,,\\n438,0.00553,1.0,,\\n439,0.12529,0.98947,,\\n440,0.00557,1.0,,\\n441,0.0,1.0,,\\n442,0.0,1.0,,\\n443,0.2282,0.9661,,\\n444,0.0,1.0,,\\n445,0.0,1.0,,\\n446,0.01222,1.0,,\\n447,0.03981,0.99,,\\n448,0.0,1.0,,\\n449,0.35811,0.89157,,\\n450,0.11842,0.95192,,\\n451,0.13851,0.97619,,\\n452,0.19397,0.92857,,\\n453,0.0,1.0,,\\n454,0.35416,0.93333,,\\n455,0.08905,0.98361,,\\n456,0.0,1.0,,\\n457,0.0,1.0,,\\n458,0.09507,0.97436,,\\n459,0.0805,0.95833,,\\n460,0.16255,0.94937,,\\n461,0.0,1.0,,\\n462,0.01437,1.0,,\\n463,0.02696,0.98734,,\\n464,0.15642,0.97059,,\\n465,0.16247,0.9697,,\\n466,0.0709,0.98291,,\\n467,0.05386,0.98361,,\\n468,0.02167,1.0,,\\n469,0.04969,0.98425,,\\n470,0.0504,0.97183,,\\n471,0.0,1.0,,\\n472,0.06877,0.98333,,\\n473,0.0,1.0,,\\n474,0.0,1.0,,\\n475,0.18948,0.9321,,\\n476,0.05928,0.98936,,\\n477,0.17513,0.9375,,\\n478,0.05912,0.97368,,\\n479,0.17209,0.92562,,\\n480,0.12615,0.95349,,\\n481,0.01491,1.0,,\\n482,0.15696,0.92793,,\\n483,0.0198,0.98864,,\\n484,0.19095,0.93023,,\\n485,0.00577,1.0,,\\n486,0.06186,0.98837,,\\n487,0.28273,0.93333,,\\n488,0.06206,0.94915,,\\n489,0.02269,0.99074,,\\n490,0.0792,0.98667,,\\n491,0.04999,0.97802,,\\n492,0.03564,0.97333,,\\n493,0.14057,0.96,,\\n494,0.10487,0.97015,,\\n495,0.16383,0.93636,,\\n496,0.16446,0.96053,,\\n497,0.02479,0.98611,,\\n498,0.06044,0.98413,,\\n499,0.00143,1.0,,\\n500,0.1091,0.94872,,\\n501,0.26297,0.92632,,\\n502,0.05343,0.97101,,\\n503,0.0,1.0,,\\n504,0.03142,0.975,,\\n505,0.03718,0.98165,,\\n506,0.02721,1.0,,\\n507,0.0,1.0,,\\n508,0.08159,0.96104,,\\n509,0.21535,0.95349,,\\n510,0.04947,0.98605,,\\n511,0.0,1.0,,\\n512,0.07183,0.9697,,\\n513,0.09646,0.96154,,\\n514,0.13337,0.97959,,\\n515,0.2584,0.9375,,\\n516,0.08298,0.9469,,\\n517,0.04678,0.98734,,\\n518,0.05,0.98,,\\n519,0.0,1.0,,\\n520,0.05958,0.98913,,\\n521,0.01195,1.0,,\\n522,0.0,1.0,,\\n523,0.00836,1.0,,\\n524,0.10891,0.97414,,\\n525,0.08715,0.95935,,\\n526,0.0,1.0,,\\n527,0.0,1.0,,\\n528,0.26526,0.95455,,\\n529,0.04195,0.98765,,\\n530,0.41691,0.86614,,\\n531,0.0,1.0,,\\n532,0.47268,0.8932,,\\n533,0.05704,0.98214,,\\n534,0.02974,0.99115,,\\n535,0.39913,0.93878,,\\n536,0.03902,0.97849,,\\n537,0.0,1.0,,\\n538,0.16245,0.95506,,\\n539,0.20087,0.94937,,\\n540,0.02696,0.98864,,\\n541,0.0,1.0,,\\n542,0.02456,0.9907,,\\n543,0.1048,0.97368,,\\n544,0.05253,0.97345,,\\n545,0.18317,0.94915,,\\n546,0.0212,0.98765,,\\n547,0.07486,0.97333,,\\n548,0.23867,0.90909,,\\n549,0.02249,1.0,,\\n550,0.24104,0.95556,,\\n551,0.03781,0.98701,,\\n552,0.01276,1.0,,\\n553,0.03734,0.99145,,\\n554,0.02564,0.98876,,\\n555,0.00349,1.0,,\\n556,0.07569,0.96491,,\\n557,0.07014,0.98507,,\\n558,0.0,1.0,,\\n559,0.03542,0.98667,,\\n560,0.01356,0.9916,,\\n561,0.00765,1.0,,\\n562,0.02479,1.0,,\\n563,0.18804,0.95455,,\\n564,0.12378,0.96522,,\\n565,0.08505,0.96078,,\\n566,0.04329,0.98148,,\\n567,0.06169,0.98077,,\\n568,0.01088,0.98837,,\\n569,0.02226,0.98165,,\\n570,0.13973,0.94697,,\\n571,0.00994,1.0,,\\n572,0.00503,1.0,,\\n573,0.05905,0.98507,,\\n574,0.02944,1.0,,\\n575,0.23654,0.92562,,\\n576,0.00985,1.0,,\\n577,0.04846,0.98718,,\\n578,0.17688,0.96341,,\\n579,0.01742,0.99476,,\\n580,0.22231,0.94643,,\\n581,0.03523,0.98913,,\\n582,0.11208,0.98734,,\\n583,0.16584,0.96512,,\\n584,0.04189,0.98077,,\\n585,0.0,1.0,,\\n586,0.0103,1.0,,\\n587,0.1655,0.95833,,\\n588,0.09752,0.96629,,\\n589,0.01958,1.0,,\\n590,0.01591,1.0,,\\n591,0.00519,1.0,,\\n592,0.06862,0.97674,,\\n593,0.00742,1.0,,\\n594,0.03462,0.99281,,\\n595,0.06727,0.99115,,\\n596,0.15972,0.97333,,\\n597,0.0,1.0,,\\n598,0.01148,1.0,,\\n599,0.10326,0.97521,,\\n600,0.02239,1.0,,\\n601,0.11144,0.96939,,\\n602,0.04909,0.98718,,\\n603,0.02062,1.0,,\\n604,0.0,1.0,,\\n605,0.02,1.0,,\\n606,0.20735,0.92199,,\\n607,0.0392,0.98485,,\\n608,0.0,1.0,,\\n609,0.1034,0.98947,,\\n610,0.01023,1.0,,\\n611,0.20471,0.93976,,\\n612,0.00753,1.0,,\\n613,0.11793,0.94828,,\\n614,0.13261,0.93243,,\\n615,0.35558,0.93878,,\\n616,0.0,1.0,,\\n617,0.0,1.0,,\\n618,0.24286,0.94186,,\\n619,0.0126,1.0,,\\n620,0.02865,0.99,,\\n621,0.12503,0.95775,,\\n622,0.00673,1.0,,\\n623,0.01355,0.98837,,\\n624,0.00753,1.0,,\\n625,0.03158,0.9823,,\\n626,0.1337,0.97037,,\\n627,0.19127,0.96053,,\\n628,0.09421,0.97414,,\\n629,0.01906,0.98913,,\\n630,0.0,1.0,,\\n631,0.17786,0.94074,,\\n632,0.08322,0.98864,,\\n633,0.01806,1.0,,\\n634,0.46975,0.85217,,\\n635,0.11469,0.97436,,\\n636,0.1551,0.93846,,\\n637,0.00961,1.0,,\\n638,0.08794,0.9697,,\\n639,0.0,1.0,,\\n640,0.0,1.0,,\\n641,0.00707,1.0,,\\n642,0.01327,0.9878,,\\n643,0.00853,1.0,,\\n644,0.0599,0.98649,,\\n645,0.00496,1.0,,\\n646,0.0,1.0,,\\n647,0.00433,1.0,,\\n648,0.00644,1.0,,\\n649,0.25275,0.92308,,\\n650,0.03459,0.9898,,\\n651,0.04653,0.97917,,\\n652,0.12099,0.98148,,\\n653,0.01065,1.0,,\\n654,0.0,1.0,,\\n655,0.00523,1.0,,\\n656,0.07743,0.96396,,\\n657,0.0,1.0,,\\n658,0.08193,0.9604,,\\n659,0.008,1.0,,\\n660,0.06423,0.98864,,\\n661,0.0,1.0,,\\n662,0.17638,0.93976,,\\n663,0.16756,0.94681,,\\n664,0.00266,1.0,,\\n665,0.20702,0.94667,,\\n666,0.10828,0.975,,\\n667,0.0482,0.98387,,\\n668,0.08514,0.97468,,\\n669,0.05527,0.98901,,\\n670,0.0,1.0,,\\n671,0.00716,1.0,,\\n672,0.02401,1.0,,\\n673,0.03693,0.98889,,\\n674,0.01562,1.0,,\\n675,0.21518,0.93137,,\\n676,0.03298,0.98936,,\\n677,0.0833,0.94595,,\\n678,0.24044,0.98667,,\\n679,0.00544,1.0,,\\n680,0.0087,1.0,,\\n681,0.12977,0.93636,,\\n682,0.00642,1.0,,\\n683,0.00132,1.0,,\\n684,0.0159,0.99,,\\n685,0.0,1.0,,\\n686,0.0,1.0,,\\n687,0.00173,1.0,,\\n688,0.12196,0.96914,,\\n689,0.18222,0.94792,,\\n690,0.27381,0.91597,,\\n691,0.12214,0.97959,,\\n692,0.03887,0.98936,,\\n693,0.01634,1.0,,\\n694,0.02714,1.0,,\\n695,0.07922,0.95775,,\\n696,0.25444,0.94203,,\\n697,0.03462,0.98438,,\\n698,0.01636,0.9902,,\\n699,0.0853,0.9625,,\\n700,0.07808,0.97902,,\\n701,0.00586,1.0,,\\n702,0.05628,0.98507,,\\n703,0.0,1.0,,\\n704,0.04448,1.0,,\\n705,0.00084,1.0,,\\n706,0.07935,0.9717,,\\n707,0.20278,0.98611,,\\n708,0.03037,0.9899,,\\n709,0.00329,1.0,,\\n710,0.00104,1.0,,\\n711,0.0,1.0,,\\n712,0.1307,0.94495,,\\n713,0.0,1.0,,\\n714,0.04703,0.99213,,\\n715,0.23266,0.97531,,\\n716,0.00678,1.0,,\\n717,0.07679,0.98551,,\\n718,0.0,1.0,,\\n719,0.0058,1.0,,\\n720,0.15984,0.94574,,\\n721,0.05455,0.97561,,\\n722,0.03058,0.98165,,\\n723,0.06798,0.98551,,\\n724,0.01246,1.0,,\\n725,0.13756,0.95425,,\\n726,0.20708,0.94737,,\\n727,0.14353,0.95455,,\\n728,0.06249,0.97143,,\\n729,0.03869,0.98333,,\\n730,0.0,1.0,,\\n731,0.0182,0.9875,,\\n732,0.0,1.0,,\\n733,0.19005,0.93939,,\\n734,0.11267,0.96825,,\\n735,0.0,1.0,,\\n736,0.0087,1.0,,\\n737,0.0,1.0,,\\n738,0.06746,0.98907,,\\n739,0.11604,0.97059,,\\n740,0.01199,1.0,,\\n741,0.02054,0.98519,,\\n742,0.01879,1.0,,\\n743,0.05481,0.9703,,\\n744,0.01411,1.0,,\\n745,0.08652,0.96939,,\\n746,0.02692,0.98824,,\\n747,0.00262,1.0,,\\n748,0.12442,0.95833,,\\n749,0.0163,1.0,,\\n750,0.00761,1.0,,\\n751,0.13216,0.94904,,\\n752,0.35523,0.88189,,\\n753,0.00445,1.0,,\\n754,0.0797,0.98246,,\\n755,0.00668,1.0,,\\n756,0.0,1.0,,\\n757,0.01468,1.0,,\\n758,0.07389,0.97674,,\\n759,0.0,1.0,,\\n760,0.1187,0.94488,,\\n761,0.05452,0.98851,,\\n762,0.05611,0.96053,,\\n763,0.04187,0.98901,,\\n764,0.06108,0.98276,,\\n765,0.01595,0.98824,,\\n766,0.0344,0.98529,,\\n767,0.00041,1.0,,\\n768,0.02981,0.97297,,\\n769,0.23024,0.92562,,\\n770,0.09353,0.96599,,\\n771,0.00218,1.0,,\\n772,0.0,1.0,,\\n773,0.0,1.0,,\\n774,0.00322,1.0,,\\n775,0.0,1.0,,\\n776,0.09385,0.98,,\\n777,0.0655,0.98462,,\\n778,0.39313,0.92233,,\\n779,0.0,1.0,,\\n780,0.01893,1.0,,\\n781,0.01269,1.0,,\\n782,0.0029,1.0,,\\n783,0.0,1.0,,\\n784,0.0183,1.0,,\\n785,0.01792,1.0,,\\n786,0.0,1.0,,\\n787,0.01006,0.99254,,\\n788,0.14185,0.94286,,\\n789,0.02296,0.98246,,\\n790,0.04414,0.98214,,\\n791,0.03389,0.98485,,\\n792,0.01912,0.98969,,\\n793,0.08232,0.97403,,\\n794,0.0045,1.0,,\\n795,0.00204,1.0,,\\n796,0.01516,1.0,,\\n797,0.02271,0.98246,,\\n798,0.05087,0.98148,,\\n799,0.12784,0.9798,,\\n800,0.04752,0.98374,,\\n801,0.04567,0.98077,,\\n802,0.00082,1.0,,\\n803,0.08168,0.96491,,\\n804,0.02532,0.98333,,\\n805,0.06625,0.98529,,\\n806,0.0457,0.97541,,\\n807,0.00174,1.0,,\\n808,0.00246,1.0,,\\n809,0.02496,1.0,,\\n810,0.01769,1.0,,\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics = client.files.retrieve_content(result_files[0])\n",
    "\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mean_token_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.72572</td>\n",
       "      <td>0.83133</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.64681</td>\n",
       "      <td>0.86667</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.51859</td>\n",
       "      <td>0.89809</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.33327</td>\n",
       "      <td>0.9403</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.22332</td>\n",
       "      <td>0.96815</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>807</td>\n",
       "      <td>0.00174</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>808</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>809</td>\n",
       "      <td>0.02496</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>810</td>\n",
       "      <td>0.01769</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>811 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    step train_loss train_accuracy valid_loss valid_mean_token_accuracy\n",
       "0      1    0.72572        0.83133                                     \n",
       "1      2    0.64681        0.86667                                     \n",
       "2      3    0.51859        0.89809                                     \n",
       "3      4    0.33327         0.9403                                     \n",
       "4      5    0.22332        0.96815                                     \n",
       "..   ...        ...            ...        ...                       ...\n",
       "806  807    0.00174            1.0                                     \n",
       "807  808    0.00246            1.0                                     \n",
       "808  809    0.02496            1.0                                     \n",
       "809  810    0.01769            1.0                                     \n",
       "810            None           None       None                      None\n",
       "\n",
       "[811 rows x 5 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table = pd.DataFrame([i.split(\",\") for i in train_metrics.split(\"\\n\")][1:], \n",
    "                           columns=[i.split(\",\") for i in train_metrics.split(\"\\n\")][0])\n",
    "\n",
    "train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/IPython/core/formatters.py:922\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    920\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     method()\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/plotly/basedatatypes.py:834\u001b[0m, in \u001b[0;36mBaseFigure._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m pio\u001b[39m.\u001b[39mrenderers\u001b[39m.\u001b[39mrender_on_display \u001b[39mand\u001b[39;00m pio\u001b[39m.\u001b[39mrenderers\u001b[39m.\u001b[39mdefault:\n\u001b[0;32m--> 834\u001b[0m     pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/plotly/io/_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         )\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "step=%{x}<br>train_loss=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810
         ],
         "xaxis": "x",
         "y": [
          0.72572,
          0.64681,
          0.51859,
          0.33327,
          0.22332,
          0.31847,
          0.31428,
          0.29801,
          0.53667,
          0.24963,
          0.42805,
          0.32082,
          0.52567,
          0.53961,
          0.39994,
          0.42182,
          0.13842,
          0.54742,
          0.14782,
          0.2264,
          0.32489,
          0.43328,
          0.13255,
          0.38074,
          0.27745,
          0.02666,
          0.01851,
          0.23417,
          0.19434,
          0.00821,
          0.31149,
          0.08018,
          0.14823,
          0.15808,
          0.12645,
          0.16169,
          0.73251,
          0.15836,
          0.00204,
          0.33968,
          0.24433,
          0.39227,
          0.23507,
          0.31601,
          0.44038,
          0.22873,
          0.09475,
          0.44477,
          0.11556,
          0.17515,
          0.11657,
          0.51878,
          0.10889,
          0.26412,
          0.09377,
          0.27306,
          0.02121,
          0.31447,
          0.14801,
          0.1048,
          0.14951,
          0.14852,
          0.10547,
          0.37961,
          0.15099,
          0.13194,
          0.10568,
          0.36865,
          0.11316,
          0.00361,
          0.18628,
          0.15801,
          0.00128,
          0.00091,
          0.00059,
          0.42935,
          0.1049,
          0.00021,
          0.24925,
          0.06764,
          0.34891,
          0.56571,
          0.00004,
          0.19485,
          0.36894,
          0.05213,
          0.23474,
          0.08363,
          0.33894,
          0.25574,
          0.1813,
          0.24367,
          0.26432,
          0.03805,
          0.21315,
          0.08583,
          0.06003,
          0.30824,
          0.13424,
          0.10239,
          0.04273,
          0.07864,
          0.16323,
          0.26789,
          0.09535,
          0.25062,
          0.24712,
          0.06798,
          0.21272,
          0.2087,
          0.03601,
          0.11721,
          0.00001,
          0.00001,
          0.24522,
          0.15994,
          0.16971,
          0.16979,
          0,
          0.25324,
          0.13967,
          0,
          0,
          0.13477,
          0.15214,
          0.25365,
          0.2838,
          0.10798,
          0.11947,
          0.31475,
          0.15045,
          0,
          0.24428,
          0.12422,
          0.05282,
          0.87958,
          0,
          0.23864,
          0.09584,
          0.16894,
          0.14841,
          0,
          0.20878,
          0.13322,
          0.24179,
          0.14331,
          0.18601,
          0.19853,
          0.07846,
          0.11215,
          0.08429,
          0.22899,
          0.20463,
          0.19667,
          0,
          0.13411,
          0.11162,
          0.1244,
          0.37424,
          0,
          0.06235,
          0.09743,
          0.14394,
          0.1792,
          0.04534,
          0.13422,
          0.06861,
          0.31251,
          0,
          0,
          0.49028,
          0.097,
          0.37459,
          0.15966,
          0.26275,
          0,
          0.19939,
          0.15286,
          0.05959,
          0.04694,
          0.2201,
          0.15229,
          0.17356,
          0.24064,
          0.16421,
          0.16042,
          0.15873,
          0.24432,
          0.07301,
          0.03465,
          0.08176,
          0.01222,
          0.21588,
          0.21376,
          0.11437,
          0.19463,
          0.10128,
          0.11527,
          0.2918,
          0.09663,
          0.07625,
          0.1121,
          0.1433,
          0.26683,
          0.11164,
          0.051,
          0.06075,
          0.12571,
          0.08224,
          0.09974,
          0.0862,
          0.06507,
          0.17288,
          0.08566,
          0.16667,
          0.07899,
          0.57646,
          0.07947,
          0.23446,
          0.42252,
          0.37948,
          0.34511,
          0.2987,
          0,
          0.10279,
          0.11274,
          0.14018,
          0.11217,
          0.15403,
          0.23727,
          0.32016,
          0.42266,
          0.5469,
          0.28656,
          0.10306,
          0.24608,
          0.07178,
          0.04232,
          0.08735,
          0.0924,
          0.06099,
          0.3798,
          0.18743,
          0.27302,
          0,
          0.0751,
          0.40946,
          0.22354,
          0.22597,
          0.25062,
          0.06605,
          0.31355,
          0.13554,
          0,
          0.36916,
          0.15123,
          0.59864,
          0.05159,
          0,
          0.05168,
          0.11605,
          0.04984,
          0.30301,
          0,
          0.07551,
          0.17524,
          0.31194,
          0.06674,
          0.09786,
          0,
          0,
          0.10412,
          0.07798,
          0.01348,
          0.11435,
          0.12025,
          0.05551,
          0.09846,
          0.04223,
          0.07075,
          0.20973,
          0.0363,
          0,
          0.07896,
          0,
          0.04896,
          0.29429,
          0,
          0.20838,
          0.11341,
          0.03855,
          0.08506,
          0.18995,
          0.20471,
          0.14521,
          0.03307,
          0.16761,
          0,
          0.02026,
          0.04578,
          0.12944,
          0,
          0.0804,
          0.10222,
          0.06295,
          0.09518,
          0.36067,
          0.02826,
          0.31967,
          0.01116,
          0.02442,
          0.08862,
          0.19801,
          0.30284,
          0.10565,
          0.04886,
          0.06328,
          0.11665,
          0.06386,
          0.14062,
          0.08739,
          0.10287,
          0.13005,
          0.10596,
          0.18503,
          0.0901,
          0.18369,
          0.06644,
          0.10879,
          0.15531,
          0.0219,
          0.02111,
          0.11016,
          0,
          0.13913,
          0.08925,
          0.31366,
          0.19608,
          0.20146,
          0.03164,
          0.36868,
          0.04121,
          0.04805,
          0.07426,
          0.06615,
          0.21556,
          0.29792,
          0.04899,
          0.0526,
          0.07038,
          0.20607,
          0,
          0.12009,
          0.09735,
          0.11804,
          0.06676,
          0.14179,
          0.5887,
          0.23036,
          0.07596,
          0.35109,
          0.22652,
          0,
          0.25483,
          0.11622,
          0.03181,
          0.05417,
          0.25909,
          0.09285,
          0.04305,
          0.25143,
          0.01421,
          0.06766,
          0.03314,
          0.06272,
          0.11312,
          0.07909,
          0.07654,
          0.05153,
          0.28084,
          0.08641,
          0.01915,
          0.10344,
          0.0427,
          0.02354,
          0,
          0,
          0.03652,
          0,
          0.15677,
          0.15391,
          0.02502,
          0.11869,
          0.31125,
          0.03538,
          0,
          0.0578,
          0.12969,
          0.18987,
          0.18222,
          0.28775,
          0.04654,
          0.16762,
          0.03418,
          0.15378,
          0.0385,
          0.07579,
          0.09672,
          0.18577,
          0.08886,
          0.01515,
          0.0342,
          0.01795,
          0.06991,
          0.03572,
          0.37336,
          0.05448,
          0.18541,
          0.1741,
          0.04537,
          0.00958,
          0.02148,
          0.07502,
          0.12154,
          0.08685,
          0.21517,
          0.2496,
          0.02152,
          0.06584,
          0.03681,
          0.01474,
          0.23222,
          0.05229,
          0.03668,
          0.04094,
          0.1718,
          0.15204,
          0.00553,
          0.12529,
          0.00557,
          0,
          0,
          0.2282,
          0,
          0,
          0.01222,
          0.03981,
          0,
          0.35811,
          0.11842,
          0.13851,
          0.19397,
          0,
          0.35416,
          0.08905,
          0,
          0,
          0.09507,
          0.0805,
          0.16255,
          0,
          0.01437,
          0.02696,
          0.15642,
          0.16247,
          0.0709,
          0.05386,
          0.02167,
          0.04969,
          0.0504,
          0,
          0.06877,
          0,
          0,
          0.18948,
          0.05928,
          0.17513,
          0.05912,
          0.17209,
          0.12615,
          0.01491,
          0.15696,
          0.0198,
          0.19095,
          0.00577,
          0.06186,
          0.28273,
          0.06206,
          0.02269,
          0.0792,
          0.04999,
          0.03564,
          0.14057,
          0.10487,
          0.16383,
          0.16446,
          0.02479,
          0.06044,
          0.00143,
          0.1091,
          0.26297,
          0.05343,
          0,
          0.03142,
          0.03718,
          0.02721,
          0,
          0.08159,
          0.21535,
          0.04947,
          0,
          0.07183,
          0.09646,
          0.13337,
          0.2584,
          0.08298,
          0.04678,
          0.05,
          0,
          0.05958,
          0.01195,
          0,
          0.00836,
          0.10891,
          0.08715,
          0,
          0,
          0.26526,
          0.04195,
          0.41691,
          0,
          0.47268,
          0.05704,
          0.02974,
          0.39913,
          0.03902,
          0,
          0.16245,
          0.20087,
          0.02696,
          0,
          0.02456,
          0.1048,
          0.05253,
          0.18317,
          0.0212,
          0.07486,
          0.23867,
          0.02249,
          0.24104,
          0.03781,
          0.01276,
          0.03734,
          0.02564,
          0.00349,
          0.07569,
          0.07014,
          0,
          0.03542,
          0.01356,
          0.00765,
          0.02479,
          0.18804,
          0.12378,
          0.08505,
          0.04329,
          0.06169,
          0.01088,
          0.02226,
          0.13973,
          0.00994,
          0.00503,
          0.05905,
          0.02944,
          0.23654,
          0.00985,
          0.04846,
          0.17688,
          0.01742,
          0.22231,
          0.03523,
          0.11208,
          0.16584,
          0.04189,
          0,
          0.0103,
          0.1655,
          0.09752,
          0.01958,
          0.01591,
          0.00519,
          0.06862,
          0.00742,
          0.03462,
          0.06727,
          0.15972,
          0,
          0.01148,
          0.10326,
          0.02239,
          0.11144,
          0.04909,
          0.02062,
          0,
          0.02,
          0.20735,
          0.0392,
          0,
          0.1034,
          0.01023,
          0.20471,
          0.00753,
          0.11793,
          0.13261,
          0.35558,
          0,
          0,
          0.24286,
          0.0126,
          0.02865,
          0.12503,
          0.00673,
          0.01355,
          0.00753,
          0.03158,
          0.1337,
          0.19127,
          0.09421,
          0.01906,
          0,
          0.17786,
          0.08322,
          0.01806,
          0.46975,
          0.11469,
          0.1551,
          0.00961,
          0.08794,
          0,
          0,
          0.00707,
          0.01327,
          0.00853,
          0.0599,
          0.00496,
          0,
          0.00433,
          0.00644,
          0.25275,
          0.03459,
          0.04653,
          0.12099,
          0.01065,
          0,
          0.00523,
          0.07743,
          0,
          0.08193,
          0.008,
          0.06423,
          0,
          0.17638,
          0.16756,
          0.00266,
          0.20702,
          0.10828,
          0.0482,
          0.08514,
          0.05527,
          0,
          0.00716,
          0.02401,
          0.03693,
          0.01562,
          0.21518,
          0.03298,
          0.0833,
          0.24044,
          0.00544,
          0.0087,
          0.12977,
          0.00642,
          0.00132,
          0.0159,
          0,
          0,
          0.00173,
          0.12196,
          0.18222,
          0.27381,
          0.12214,
          0.03887,
          0.01634,
          0.02714,
          0.07922,
          0.25444,
          0.03462,
          0.01636,
          0.0853,
          0.07808,
          0.00586,
          0.05628,
          0,
          0.04448,
          0.00084,
          0.07935,
          0.20278,
          0.03037,
          0.00329,
          0.00104,
          0,
          0.1307,
          0,
          0.04703,
          0.23266,
          0.00678,
          0.07679,
          0,
          0.0058,
          0.15984,
          0.05455,
          0.03058,
          0.06798,
          0.01246,
          0.13756,
          0.20708,
          0.14353,
          0.06249,
          0.03869,
          0,
          0.0182,
          0,
          0.19005,
          0.11267,
          0,
          0.0087,
          0,
          0.06746,
          0.11604,
          0.01199,
          0.02054,
          0.01879,
          0.05481,
          0.01411,
          0.08652,
          0.02692,
          0.00262,
          0.12442,
          0.0163,
          0.00761,
          0.13216,
          0.35523,
          0.00445,
          0.0797,
          0.00668,
          0,
          0.01468,
          0.07389,
          0,
          0.1187,
          0.05452,
          0.05611,
          0.04187,
          0.06108,
          0.01595,
          0.0344,
          0.00041,
          0.02981,
          0.23024,
          0.09353,
          0.00218,
          0,
          0,
          0.00322,
          0,
          0.09385,
          0.0655,
          0.39313,
          0,
          0.01893,
          0.01269,
          0.0029,
          0,
          0.0183,
          0.01792,
          0,
          0.01006,
          0.14185,
          0.02296,
          0.04414,
          0.03389,
          0.01912,
          0.08232,
          0.0045,
          0.00204,
          0.01516,
          0.02271,
          0.05087,
          0.12784,
          0.04752,
          0.04567,
          0.00082,
          0.08168,
          0.02532,
          0.06625,
          0.0457,
          0.00174,
          0.00246,
          0.02496,
          0.01769
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "step"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "train_loss"
         }
        }
       }
      },
      "text/html": [
       "<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.20.0.min.js\"></script>                <div id=\"793364fb-a48f-424d-99bb-56411ef5d7b5\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"793364fb-a48f-424d-99bb-56411ef5d7b5\")) {                    Plotly.newPlot(                        \"793364fb-a48f-424d-99bb-56411ef5d7b5\",                        [{\"hovertemplate\":\"step=%{x}<br>train_loss=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810],\"xaxis\":\"x\",\"y\":[0.72572,0.64681,0.51859,0.33327,0.22332,0.31847,0.31428,0.29801,0.53667,0.24963,0.42805,0.32082,0.52567,0.53961,0.39994,0.42182,0.13842,0.54742,0.14782,0.2264,0.32489,0.43328,0.13255,0.38074,0.27745,0.02666,0.01851,0.23417,0.19434,0.00821,0.31149,0.08018,0.14823,0.15808,0.12645,0.16169,0.73251,0.15836,0.00204,0.33968,0.24433,0.39227,0.23507,0.31601,0.44038,0.22873,0.09475,0.44477,0.11556,0.17515,0.11657,0.51878,0.10889,0.26412,0.09377,0.27306,0.02121,0.31447,0.14801,0.1048,0.14951,0.14852,0.10547,0.37961,0.15099,0.13194,0.10568,0.36865,0.11316,0.00361,0.18628,0.15801,0.00128,0.00091,0.00059,0.42935,0.1049,0.00021,0.24925,0.06764,0.34891,0.56571,0.00004,0.19485,0.36894,0.05213,0.23474,0.08363,0.33894,0.25574,0.1813,0.24367,0.26432,0.03805,0.21315,0.08583,0.06003,0.30824,0.13424,0.10239,0.04273,0.07864,0.16323,0.26789,0.09535,0.25062,0.24712,0.06798,0.21272,0.2087,0.03601,0.11721,0.00001,0.00001,0.24522,0.15994,0.16971,0.16979,0.0,0.25324,0.13967,0.0,0.0,0.13477,0.15214,0.25365,0.2838,0.10798,0.11947,0.31475,0.15045,0.0,0.24428,0.12422,0.05282,0.87958,0.0,0.23864,0.09584,0.16894,0.14841,0.0,0.20878,0.13322,0.24179,0.14331,0.18601,0.19853,0.07846,0.11215,0.08429,0.22899,0.20463,0.19667,0.0,0.13411,0.11162,0.1244,0.37424,0.0,0.06235,0.09743,0.14394,0.1792,0.04534,0.13422,0.06861,0.31251,0.0,0.0,0.49028,0.097,0.37459,0.15966,0.26275,0.0,0.19939,0.15286,0.05959,0.04694,0.2201,0.15229,0.17356,0.24064,0.16421,0.16042,0.15873,0.24432,0.07301,0.03465,0.08176,0.01222,0.21588,0.21376,0.11437,0.19463,0.10128,0.11527,0.2918,0.09663,0.07625,0.1121,0.1433,0.26683,0.11164,0.051,0.06075,0.12571,0.08224,0.09974,0.0862,0.06507,0.17288,0.08566,0.16667,0.07899,0.57646,0.07947,0.23446,0.42252,0.37948,0.34511,0.2987,0.0,0.10279,0.11274,0.14018,0.11217,0.15403,0.23727,0.32016,0.42266,0.5469,0.28656,0.10306,0.24608,0.07178,0.04232,0.08735,0.0924,0.06099,0.3798,0.18743,0.27302,0.0,0.0751,0.40946,0.22354,0.22597,0.25062,0.06605,0.31355,0.13554,0.0,0.36916,0.15123,0.59864,0.05159,0.0,0.05168,0.11605,0.04984,0.30301,0.0,0.07551,0.17524,0.31194,0.06674,0.09786,0.0,0.0,0.10412,0.07798,0.01348,0.11435,0.12025,0.05551,0.09846,0.04223,0.07075,0.20973,0.0363,0.0,0.07896,0.0,0.04896,0.29429,0.0,0.20838,0.11341,0.03855,0.08506,0.18995,0.20471,0.14521,0.03307,0.16761,0.0,0.02026,0.04578,0.12944,0.0,0.0804,0.10222,0.06295,0.09518,0.36067,0.02826,0.31967,0.01116,0.02442,0.08862,0.19801,0.30284,0.10565,0.04886,0.06328,0.11665,0.06386,0.14062,0.08739,0.10287,0.13005,0.10596,0.18503,0.0901,0.18369,0.06644,0.10879,0.15531,0.0219,0.02111,0.11016,0.0,0.13913,0.08925,0.31366,0.19608,0.20146,0.03164,0.36868,0.04121,0.04805,0.07426,0.06615,0.21556,0.29792,0.04899,0.0526,0.07038,0.20607,0.0,0.12009,0.09735,0.11804,0.06676,0.14179,0.5887,0.23036,0.07596,0.35109,0.22652,0.0,0.25483,0.11622,0.03181,0.05417,0.25909,0.09285,0.04305,0.25143,0.01421,0.06766,0.03314,0.06272,0.11312,0.07909,0.07654,0.05153,0.28084,0.08641,0.01915,0.10344,0.0427,0.02354,0.0,0.0,0.03652,0.0,0.15677,0.15391,0.02502,0.11869,0.31125,0.03538,0.0,0.0578,0.12969,0.18987,0.18222,0.28775,0.04654,0.16762,0.03418,0.15378,0.0385,0.07579,0.09672,0.18577,0.08886,0.01515,0.0342,0.01795,0.06991,0.03572,0.37336,0.05448,0.18541,0.1741,0.04537,0.00958,0.02148,0.07502,0.12154,0.08685,0.21517,0.2496,0.02152,0.06584,0.03681,0.01474,0.23222,0.05229,0.03668,0.04094,0.1718,0.15204,0.00553,0.12529,0.00557,0.0,0.0,0.2282,0.0,0.0,0.01222,0.03981,0.0,0.35811,0.11842,0.13851,0.19397,0.0,0.35416,0.08905,0.0,0.0,0.09507,0.0805,0.16255,0.0,0.01437,0.02696,0.15642,0.16247,0.0709,0.05386,0.02167,0.04969,0.0504,0.0,0.06877,0.0,0.0,0.18948,0.05928,0.17513,0.05912,0.17209,0.12615,0.01491,0.15696,0.0198,0.19095,0.00577,0.06186,0.28273,0.06206,0.02269,0.0792,0.04999,0.03564,0.14057,0.10487,0.16383,0.16446,0.02479,0.06044,0.00143,0.1091,0.26297,0.05343,0.0,0.03142,0.03718,0.02721,0.0,0.08159,0.21535,0.04947,0.0,0.07183,0.09646,0.13337,0.2584,0.08298,0.04678,0.05,0.0,0.05958,0.01195,0.0,0.00836,0.10891,0.08715,0.0,0.0,0.26526,0.04195,0.41691,0.0,0.47268,0.05704,0.02974,0.39913,0.03902,0.0,0.16245,0.20087,0.02696,0.0,0.02456,0.1048,0.05253,0.18317,0.0212,0.07486,0.23867,0.02249,0.24104,0.03781,0.01276,0.03734,0.02564,0.00349,0.07569,0.07014,0.0,0.03542,0.01356,0.00765,0.02479,0.18804,0.12378,0.08505,0.04329,0.06169,0.01088,0.02226,0.13973,0.00994,0.00503,0.05905,0.02944,0.23654,0.00985,0.04846,0.17688,0.01742,0.22231,0.03523,0.11208,0.16584,0.04189,0.0,0.0103,0.1655,0.09752,0.01958,0.01591,0.00519,0.06862,0.00742,0.03462,0.06727,0.15972,0.0,0.01148,0.10326,0.02239,0.11144,0.04909,0.02062,0.0,0.02,0.20735,0.0392,0.0,0.1034,0.01023,0.20471,0.00753,0.11793,0.13261,0.35558,0.0,0.0,0.24286,0.0126,0.02865,0.12503,0.00673,0.01355,0.00753,0.03158,0.1337,0.19127,0.09421,0.01906,0.0,0.17786,0.08322,0.01806,0.46975,0.11469,0.1551,0.00961,0.08794,0.0,0.0,0.00707,0.01327,0.00853,0.0599,0.00496,0.0,0.00433,0.00644,0.25275,0.03459,0.04653,0.12099,0.01065,0.0,0.00523,0.07743,0.0,0.08193,0.008,0.06423,0.0,0.17638,0.16756,0.00266,0.20702,0.10828,0.0482,0.08514,0.05527,0.0,0.00716,0.02401,0.03693,0.01562,0.21518,0.03298,0.0833,0.24044,0.00544,0.0087,0.12977,0.00642,0.00132,0.0159,0.0,0.0,0.00173,0.12196,0.18222,0.27381,0.12214,0.03887,0.01634,0.02714,0.07922,0.25444,0.03462,0.01636,0.0853,0.07808,0.00586,0.05628,0.0,0.04448,0.00084,0.07935,0.20278,0.03037,0.00329,0.00104,0.0,0.1307,0.0,0.04703,0.23266,0.00678,0.07679,0.0,0.0058,0.15984,0.05455,0.03058,0.06798,0.01246,0.13756,0.20708,0.14353,0.06249,0.03869,0.0,0.0182,0.0,0.19005,0.11267,0.0,0.0087,0.0,0.06746,0.11604,0.01199,0.02054,0.01879,0.05481,0.01411,0.08652,0.02692,0.00262,0.12442,0.0163,0.00761,0.13216,0.35523,0.00445,0.0797,0.00668,0.0,0.01468,0.07389,0.0,0.1187,0.05452,0.05611,0.04187,0.06108,0.01595,0.0344,0.00041,0.02981,0.23024,0.09353,0.00218,0.0,0.0,0.00322,0.0,0.09385,0.0655,0.39313,0.0,0.01893,0.01269,0.0029,0.0,0.0183,0.01792,0.0,0.01006,0.14185,0.02296,0.04414,0.03389,0.01912,0.08232,0.0045,0.00204,0.01516,0.02271,0.05087,0.12784,0.04752,0.04567,0.00082,0.08168,0.02532,0.06625,0.0457,0.00174,0.00246,0.02496,0.01769],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"step\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"train_loss\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    )                };                            </script>        </div>"
      ],
      "text/plain": [
       "Figure({\n",
       "    'data': [{'hovertemplate': 'step=%{x}<br>train_loss=%{y}<extra></extra>',\n",
       "              'legendgroup': '',\n",
       "              'line': {'color': '#636efa', 'dash': 'solid'},\n",
       "              'marker': {'symbol': 'circle'},\n",
       "              'mode': 'lines',\n",
       "              'name': '',\n",
       "              'orientation': 'v',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'x': array([  1,   2,   3, ..., 808, 809, 810]),\n",
       "              'xaxis': 'x',\n",
       "              'y': array([0.72572, 0.64681, 0.51859, ..., 0.00246, 0.02496, 0.01769]),\n",
       "              'yaxis': 'y'}],\n",
       "    'layout': {'legend': {'tracegroupgap': 0},\n",
       "               'margin': {'t': 60},\n",
       "               'template': '...',\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 1.0], 'title': {'text': 'step'}},\n",
       "               'yaxis': {'anchor': 'x', 'domain': [0.0, 1.0], 'title': {'text': 'train_loss'}}}\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_table\n",
    " .query(\"train_loss==train_loss\")\n",
    " .assign(step = lambda d: pd.to_numeric(d.step),\n",
    "         train_loss = lambda d: pd.to_numeric(d.train_loss))\n",
    " .plot.line(x=\"step\", y=\"train_loss\")\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's not take a look at loading a using the trained model from OpenAI, using LangChain on RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the finetuned (\"ft\") model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-0613:personal:012024-jc-sample:8jFZc3IB'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = client.fine_tuning.jobs.retrieve(model_finetune.dict()[\"id\"]).dict()[\"fine_tuned_model\"]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=model_name, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we loaded it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'ft:gpt-3.5-turbo-0613:personal:012024-jc-sample:8jFZc3IB',\n",
       " 'model': 'ft:gpt-3.5-turbo-0613:personal:012024-jc-sample:8jFZc3IB',\n",
       " 'stream': False,\n",
       " 'n': 1,\n",
       " 'temperature': 0.5,\n",
       " '_type': 'openai-chat'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the memory window. More on how ConversationBufferWindowMemory() works [here](https://github.com/jzamalloa1/langchain_learning/blob/main/Conversation_Memory.ipynb)\n",
    "\n",
    "**Note of 'memory_key'** The memory_key parameter specifies the key under which the conversation history will be stored and accessed in the memory. In this case, using \"messages\" as the key means that the conversation history (both the questions asked by the user and the responses given by the bot) will be stored and retrieved using this key. Essentially, it's a label for accessing the stored conversation data. [Source](https://medium.com/@vishalkalia.er/simplifying-langchain-memory-the-power-of-chatbuffermemory-in-chatbots-d44e3911fcb6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_window = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    k=5, #The conversation buffer window memory function sets the last number of interactions we want to keep. We are using 2 as an example below\n",
    "    return_messages=True,\n",
    "    output_key=\"output\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load an agent that the arxiv langchain tool. More context can be found [here](https://www.coditation.com/blog/introduction-to-langchain-agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv = ArxivQueryRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n",
      "A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "{'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(arxiv.name)\n",
    "print(arxiv.description)\n",
    "print(arxiv.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized a simple tool using the arxiv built-in tool in langchain. More on tools [here](https://python.langchain.com/docs/modules/agents/tools/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_tool = Tool(\n",
    "    name=arxiv.name,\n",
    "    func=arxiv.run,\n",
    "    description=\"useful when you need an answer from Arxiv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to initialize an agent with our model using **initialize_agent. Note that this is now (01/28/24) deprecated.** Refer to more up-to-date langchain agent resources [here](https://python.langchain.com/docs/modules/agents/quick_start). There are langchain classes that will replace this that are more specific to each use case (e.g. create_openai_functions_agent, create_react_agent). I believe [create_react_agent](https://api.python.langchain.com/en/stable/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent) applies in this case. Will explore later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = initialize_agent(\n",
    "    llm = llm,\n",
    "    tools=[arxiv_tool],\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method=\"generate\",\n",
    "    return_immediate_steps=True,\n",
    "    memory=memory_window\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"arxiv\",\n",
      "    \"action_input\": \"Llama 2: A Large Language Model Collection for Code and Instruction-Following Tasks\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-08-25\n",
      "Title: Code Llama: Open Foundation Models for Code\n",
      "Authors: Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve\n",
      "Summary: We release Code Llama, a family of large language models for code based on\n",
      "Llama 2 providing state-of-the-art performance among open models, infilling\n",
      "capabilities, support for large input contexts, and zero-shot instruction\n",
      "following ability for programming tasks. We provide multiple flavors to cover a\n",
      "wide range of applications: foundation models (Code Llama), Python\n",
      "specializations (Code Llama - Python), and instruction-following models (Code\n",
      "Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained\n",
      "on sequences of 16k tokens and show improvements on inputs with up to 100k\n",
      "tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support\n",
      "infilling based on surrounding content. Code Llama reaches state-of-the-art\n",
      "performance among open models on several code benchmarks, with scores of up to\n",
      "53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python\n",
      "7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform\n",
      "every other publicly available model on MultiPL-E. We release Code Llama under\n",
      "a permissive license that allows for both research and commercial use.\n",
      "\n",
      "Published: 2024-01-04\n",
      "Title: LLaMA Pro: Progressive LLaMA with Block Expansion\n",
      "Authors: Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan\n",
      "Summary: Humans generally acquire new skills without compromising the old; however,\n",
      "the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to\n",
      "CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with\n",
      "an expansion of Transformer blocks. We tune the expanded blocks using only new\n",
      "corpus, efficiently and effectively improving the model's knowledge without\n",
      "catastrophic forgetting. In this paper, we experiment on the corpus of code and\n",
      "math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from\n",
      "LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro\n",
      "and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced\n",
      "performance among various benchmarks, demonstrating superiority over existing\n",
      "open models in the LLaMA family and the immense potential of reasoning and\n",
      "addressing diverse tasks as an intelligent agent. Our findings provide valuable\n",
      "insights into integrating natural and programming languages, laying a solid\n",
      "foundation for developing advanced language agents that operate effectively in\n",
      "various environments.\n",
      "\n",
      "Published: 2023-08-03\n",
      "Title: Local Large Language Models for Complex Structured Medical Tasks\n",
      "Authors: V. K. Cody Bumgardner, Aaron Mullen, Sam Armstrong, Caylin Hickey, Jeff Talbert\n",
      "Summary: This paper introduces an approach that combines the language reasoning\n",
      "capabilities of large language models (LLMs) with the benefits of local\n",
      "training to tackle complex, domain-specific tasks. Specifically, the authors\n",
      "demonstrate their approach by extracting structured condition codes from\n",
      "pathology reports. The proposed approach utilizes local LLMs, which can be\n",
      "fine-tuned to respond to specific generative instructions and provide\n",
      "structured outputs. The authors collected a dataset of over 150k uncurated\n",
      "surgical pathology reports, containing gross descriptions, final diagnoses, and\n",
      "condition codes. They trained different model architectures, including LLaMA,\n",
      "BERT and LongFormer and evaluated their performance. The results show that the\n",
      "LLaMA-based models significantly outperform BERT-style models across all\n",
      "evaluated metrics, even with extremely reduced precision. The LLaMA models\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Llama 2 is a collection of large language models that Meta developed and released to the public. It has been fine-tuned for various purposes, such as code generation and instruction-following tasks. The models in Llama 2 show state-of-the-art performance among open models on code benchmarks and Python tasks. There are also specialized variants of Llama 2 for Python and instruction-following tasks, with different parameter sizes. Llama 2 is released under a permissive license that allows for both research and commercial use.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Llama 2',\n",
       " 'chat_history': [HumanMessage(content='Tell me about Llama 2'),\n",
       "  AIMessage(content='Llama 2 is a collection of large language models that Meta developed and released to the public. It has been fine-tuned for various purposes, such as code generation and instruction-following tasks. The models in Llama 2 show state-of-the-art performance among open models on code benchmarks and Python tasks. There are also specialized variants of Llama 2 for Python and instruction-following tasks, with different parameter sizes. Llama 2 is released under a permissive license that allows for both research and commercial use.')],\n",
       " 'output': 'Llama 2 is a collection of large language models that Meta developed and released to the public. It has been fine-tuned for various purposes, such as code generation and instruction-following tasks. The models in Llama 2 show state-of-the-art performance among open models on code benchmarks and Python tasks. There are also specialized variants of Llama 2 for Python and instruction-following tasks, with different parameter sizes. Llama 2 is released under a permissive license that allows for both research and commercial use.'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent(\"Tell me about Llama 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the agent was able to run. **Notice** how the output is **similar in format to the training data** that is, the correct JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Vector Search Tool: This tool allows you to get research information about LLMs.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\ The action to take. Must be one of Vector Search Tool\\n    \"action_input\": string \\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWhat is the main focus of the technical report IDSIA-01-11?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '```json\\n{\\n    \"action\": \"Vector Search Tool\",\\n    \"action_input\": \"Main focus of technical report IDSIA-01-11\"\\n}\\n```'},\n",
       "  {'role': 'user',\n",
       "   'content': '[High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but]'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": \"The main focus of the technical report IDSIA-01-11 is the presentation of a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants for visual object classification.\"\\n}\\n```'}]}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with another question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"arxiv\",\n",
      "    \"action_input\": \"Difference between Llama 2 and Bard\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-12-16\n",
      "Title: A Comparative Analysis of Large Language Models for Code Documentation Generation\n",
      "Authors: Shubhang Shekhar Dvivedi, Vyshnav Vijay, Sai Leela Rahul Pujari, Shoumik Lodh, Dhruv Kumar\n",
      "Summary: This paper presents a comprehensive comparative analysis of Large Language\n",
      "Models (LLMs) for generation of code documentation. Code documentation is an\n",
      "essential part of the software writing process. The paper evaluates models such\n",
      "as GPT-3.5, GPT-4, Bard, Llama2, and Starchat on various parameters like\n",
      "Accuracy, Completeness, Relevance, Understandability, Readability and Time\n",
      "Taken for different levels of code documentation. Our evaluation employs a\n",
      "checklist-based system to minimize subjectivity, providing a more objective\n",
      "assessment. We find that, barring Starchat, all LLMs consistently outperform\n",
      "the original documentation. Notably, closed-source models GPT-3.5, GPT-4, and\n",
      "Bard exhibit superior performance across various parameters compared to\n",
      "open-source/source-available LLMs, namely LLama 2 and StarChat. Considering the\n",
      "time taken for generation, GPT-4 demonstrated the longest duration, followed by\n",
      "Llama2, Bard, with ChatGPT and Starchat having comparable generation times.\n",
      "Additionally, file level documentation had a considerably worse performance\n",
      "across all parameters (except for time taken) as compared to inline and\n",
      "function level documentation.\n",
      "\n",
      "Published: 2023-10-08\n",
      "Title: Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT\n",
      "Authors: Akshaj Kumar Veldanda, Fabian Grob, Shailja Thakur, Hammond Pearce, Benjamin Tan, Ramesh Karri, Siddharth Garg\n",
      "Summary: Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit\n",
      "applicability across numerous tasks. One domain of interest is their use in\n",
      "algorithmic hiring, specifically in matching resumes with job categories. Yet,\n",
      "this introduces issues of bias on protected attributes like gender, race and\n",
      "maternity status. The seminal work of Bertrand & Mullainathan (2003) set the\n",
      "gold-standard for identifying hiring bias via field experiments where the\n",
      "response rate for identical resumes that differ only in protected attributes,\n",
      "e.g., racially suggestive names such as Emily or Lakisha, is compared. We\n",
      "replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and\n",
      "Llama) to evaluate bias (or lack thereof) on gender, race, maternity status,\n",
      "pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)\n",
      "matching resumes to job categories; and (2) summarizing resumes with employment\n",
      "relevant information. Overall, LLMs are robust across race and gender. They\n",
      "differ in their performance on pregnancy status and political affiliation. We\n",
      "use contrastive input decoding on open-source LLMs to uncover potential sources\n",
      "of bias.\n",
      "\n",
      "Published: 2023-10-16\n",
      "Title: Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison\n",
      "Authors: Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, Jian Su\n",
      "Summary: The success of ChatGPT has ignited an AI race, with researchers striving to\n",
      "develop new large language models (LLMs) that can match or surpass the language\n",
      "understanding and generation abilities of commercial ones. In recent times, a\n",
      "number of models have emerged, claiming performance near that of GPT-3.5 or\n",
      "GPT-4 through various instruction-tuning methods. As practitioners of\n",
      "Text-to-SQL parsing, we are grateful for their valuable contributions to\n",
      "open-source research. However, it is important to approach these claims with a\n",
      "sense of scrutiny and ascertain the actual effectiveness of these models.\n",
      "Therefore, we pit six popular large language models against each other,\n",
      "systematically evaluating their Text-to-SQL parsing capability on nine\n",
      "benchmark datasets with five different prompting strategies, covering both\n",
      "zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell\n",
      "significan\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Llama 2 is reported to exhibit inferior performance compared to closed-source models GPT-3.5, GPT-4, and Bard across various parameters like Accuracy, Completeness, Relevance, Understandability, and Readability. It also has a longer generation time compared to Bard. However, the specific differences between Llama 2 and Bard are not mentioned in the available tool responses.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How is Llama 2 different than Bard?',\n",
       " 'chat_history': [HumanMessage(content='Tell me about Llama 2'),\n",
       "  AIMessage(content='Llama 2 is a collection of large language models that Meta developed and released to the public. It has been fine-tuned for various purposes, such as code generation and instruction-following tasks. The models in Llama 2 show state-of-the-art performance among open models on code benchmarks and Python tasks. There are also specialized variants of Llama 2 for Python and instruction-following tasks, with different parameter sizes. Llama 2 is released under a permissive license that allows for both research and commercial use.'),\n",
       "  HumanMessage(content='Tell me about Llama 2'),\n",
       "  AIMessage(content='Llama 2 is a collection of large language models that Meta developed and released to the public. It has been fine-tuned for various purposes, such as code generation and instruction-following tasks. The models in Llama 2 show state-of-the-art performance among open models on code benchmarks and Python tasks. There are also specialized variants of Llama 2 for Python and instruction-following tasks, with different parameter sizes. Llama 2 is released under a permissive license that allows for both research and commercial use.')],\n",
       " 'output': 'Llama 2 is reported to exhibit inferior performance compared to closed-source models GPT-3.5, GPT-4, and Bard across various parameters like Accuracy, Completeness, Relevance, Understandability, and Readability. It also has a longer generation time compared to Bard. However, the specific differences between Llama 2 and Bard are not mentioned in the available tool responses.'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent(\"How is Llama 2 different than Bard?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We were able to train our agent. We'll try to learn how to create our data for training in the future (and update the format of agent use from Langchain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
