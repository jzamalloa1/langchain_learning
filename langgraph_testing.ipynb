{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Testing\n",
    "\n",
    "Inspiration from [Langchain Videos](https://www.youtube.com/watch?v=E2shqsYwxck&t=394s)\n",
    "\n",
    "<p>\n",
    "<img src=\"ILLUSTRATIONS/langgraph_example.png\" \n",
    "      width=\"65%\" height=\"auto\"\n",
    "      style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Illustration [reference](https://github.com/langchain-ai/langgraph/tree/main/examples/rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, os, pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import random\n",
    "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union, Dict\n",
    "import operator\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain import hub\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample website docs, split, embed and place into VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 33 chunks\n"
     ]
    }
   ],
   "source": [
    "# Load docs from website (it can be any site)\n",
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load() # Loads as single doc\n",
    "\n",
    "# Split doc into chunks\n",
    "text_splitter_class = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = text_splitter_class.split_documents(docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "# Embed into Vector Store\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = \"chroma_testing_langgraph\"\n",
    ")\n",
    "\n",
    "# Build retriever\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\", metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what is FAISS?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load llm with json output to parse decision in graph (grader). Built chain with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_json_output = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.1, \n",
    "                             model_kwargs={\"response_format\":{\"type\":\"json_object\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "Here is the user question: {question} \\n\n",
    "If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm_json_output | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually feed in retrieved documents which may or may not have relevant documents to test chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the different types of agent memory?\"\n",
    "query_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "chain_scores = chain.invoke(\n",
    "    {\n",
    "        \"question\":query, \n",
    "        \"document\":query_docs[0].page_content\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for illustration without `JsonOutputParser()` in chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n\"score\": \"yes\"\\n}', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 544, 'total_tokens': 552}, 'model_name': 'gpt-4-turbo-preview', 'system_fingerprint': 'fp_122114e45f', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Graph\n",
    "\n",
    "We'll develop a graph based on multiple vector inputs. We'll start with building multiple vector stores\n",
    "\n",
    "Referenced from [here](https://www.youtube.com/watch?v=pbAd8O1Lvm4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    \n",
    "]\n",
    "\n",
    "plotly_yt_urls = [\n",
    "    \"https://www.youtube.com/watch?v=Qx5eFVUdDxk&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=1\",\n",
    "    \"https://www.youtube.com/watch?v=Z9YUejzkFa0&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=2\",\n",
    "    \"https://www.youtube.com/watch?v=4bP66rRxVBw&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=3\",\n",
    "    \"https://www.youtube.com/watch?v=a1qzu5GKIf0&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=4\",\n",
    "    \"https://www.youtube.com/watch?v=Fm7DC-Z5R7A&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=5\",\n",
    "    \"https://www.youtube.com/watch?v=4jcWJ30HqSY&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=6\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vector store per ulrs and video inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 194 chunks\n"
     ]
    }
   ],
   "source": [
    "# Vector store for llm urls\n",
    "\n",
    "llm_docs = [WebBaseLoader(url).load() for url in llm_urls] # Each loader produced a list of one element\n",
    "llm_docs = [i for j in llm_docs for i in j] # Decoupling lists of one element\n",
    "\n",
    "text_splitter_class = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = text_splitter_class.split_documents(llm_docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "llm_llw_vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = \"chroma_llm_llw\"\n",
    ")\n",
    "\n",
    "llm_llw_retriever = llm_llw_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 109 chunks\n"
     ]
    }
   ],
   "source": [
    "# Vector store for plotly youtube urls\n",
    "\n",
    "yt_docs = [YoutubeLoader.from_youtube_url(url, add_video_info=False).load() for url in plotly_yt_urls] # Each loader produced a list of one element\n",
    "yt_docs = [i for j in yt_docs for i in j] # Decoupling lists of one element\n",
    "\n",
    "text_splitter_class = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = text_splitter_class.split_documents(yt_docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "plotly_yt_vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = \"chroma_plotly_yt\"\n",
    ")\n",
    "\n",
    "plotly_yt_retriever = plotly_yt_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Graph State - ie. a dict where we keep state of flow elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Dict\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \n",
    "    question: str\n",
    "    generation: str\n",
    "    docs: List[str]\n",
    "    run_web_search: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define **functions for every node (changes state) in graph** as well as **Edges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "##### NODES #####\n",
    "\n",
    "# RETRIEVER NODE\n",
    "def retrieve(state):\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "\n",
    "    # Get a question from the state dictionary\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Execute retriever to get documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Return to write back (update) state\n",
    "    return {\"docs\":docs, \"question\":question}\n",
    "\n",
    "# GENERATE NODE\n",
    "def generate(state):\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "\n",
    "    # Get question from dictionary\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Get documents stored in update state (theoretically retrieved using retriever)\n",
    "    docs = state[\"docs\"]\n",
    "\n",
    "    # Build LLM chain to generate answer based on question and retrieved docs\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.1, streaming=True)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Generate\n",
    "    generation = chain.invoke({\"context\":format_docs(docs), \"question\":question})\n",
    "\n",
    "    # Return generation to update state\n",
    "    return {\"docs\":docs, \"question\":question, \"generation\":generation}\n",
    "\n",
    "# GRADING DOCUMENTS NODE\n",
    "def grading_docs(state):\n",
    "\n",
    "    print(\"---GRADE---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"docs\"]\n",
    "\n",
    "    # Define pydantic object to defined desired structure in llm\n",
    "    class grader_output(BaseModel):\n",
    "        binary_score: str = Field(description=\"Relevance score by grader of yes or no\")\n",
    "\n",
    "    # Import llm and incorporate structured function call\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.1, streaming=True)    \n",
    "    structured_llm = llm.with_structured_output(grader_output)\n",
    "\n",
    "    # Define prompt for grader\n",
    "    system = \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    \"\"\"\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define grader chain - THIS WILL GRADE ONE DOCUMENT AT A TIME\n",
    "    grader_chain = grade_prompt | structured_llm\n",
    "\n",
    "    # Grade retrieved docs with grader_chain AND add `web_search` if at least one document irrelevant\n",
    "    filtered_docs = []\n",
    "    run_web_search = \"no\" # Start variable as \"no\" and change only if we get a single irrelevant document\n",
    "    for i in docs:\n",
    "        score = grader_chain.invoke({\"question\":question, \"document\":i.page_content})\n",
    "        grade = score.binary_score\n",
    "\n",
    "        if grade==\"yes\":\n",
    "            print(\"---Doc is relevant---\")\n",
    "            filtered_docs.append(i)\n",
    "        else:\n",
    "            print(\"---Doc is not relevant\")\n",
    "            run_web_search = \"yes\" # WILL PERFORM WEB SEARCH if we have and irrelevant document\n",
    "            continue\n",
    "    \n",
    "    # Update state with filtered docs\n",
    "    return {\"docs\": filtered_docs, \"question\":question, \"run_web_search\":run_web_search}\n",
    "\n",
    "def improve_query(state):\n",
    "    \"\"\"\n",
    "    Improve query being asked by re-generating it\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---IMPROVE QUERY---\")\n",
    "\n",
    "    question  = state[\"question\"]\n",
    "    docs = state[\"docs\"]\n",
    "\n",
    "    # Build chain that takes in a query and outputs an improves query\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "    system = \"\"\"\n",
    "    You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "    for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning and re-write it.\n",
    "    \"\"\"\n",
    "\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    re_write_chain = re_write_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Improve query\n",
    "    improved_question = re_write_chain.invoke({\"question\":question})\n",
    "\n",
    "    # Update state with improved question\n",
    "    return {\"docs\": docs, \"question\":improved_question}\n",
    "\n",
    "def web_search(state):\n",
    "\n",
    "    \"\"\"\n",
    "    Web search using query to retrieve relevant docs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH FOR DOCS---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"docs\"]\n",
    "\n",
    "    # Retrieve relevant web documents\n",
    "    web_tool = TavilySearchAPIRetriever(k=5)\n",
    "    web_results = web_tool.invoke(question)\n",
    "    web_docs = [Document(page_content = i.page_content) for i in web_results]\n",
    "\n",
    "    # Add to running document list\n",
    "    docs.extend(web_docs)\n",
    "\n",
    "    # Return updated document list into state\n",
    "    return {\"docs\":docs, \"question\":question}\n",
    "\n",
    "###### EDGES ######\n",
    "\n",
    "def retrieved_decission_maker(state):\n",
    "\n",
    "    \"\"\"\n",
    "    Will decide to re-formulate query if no relevant documents are found, \n",
    "    otherwise will generate answer based on retrieved documents\n",
    "    \"\"\"\n",
    "\n",
    "    print (\"---GT:1 - ASSESS IF NEEDS TO RE-GENERATE QUERY---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    filtered_docs = state[\"docs\"]\n",
    "\n",
    "    if not filtered_docs:\n",
    "        print(\"---ALL DOCS ARE NOT RELEVANT: WILL TRANSFORM AND IMPROVE QUERY AND TRY RETRIEVING AGAIN---\")\n",
    "        return \"improve_query\"\n",
    "    else:\n",
    "        print(\"---FOUND RELEVANT DOCUMENTS: WILL PROCEED TO GENERATE ANSWER---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "def retrieved_decission_maker_with_web_search(state):\n",
    "    \"\"\"\n",
    "    Will decide to re-formulate query + pass that to web search if at least\n",
    "    one retrieved document was found to be non-relevant\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GT:2 - ASSESS IF NEEDS TO RE-GENERATE QUERY AND ADD WEB SEARCH RESULTS---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    filtered_docs = state[\"docs\"]\n",
    "    run_web_search = state[\"run_web_search\"]\n",
    "\n",
    "    if run_web_search==\"yes\":\n",
    "        print(\"---FOUND AT LEAST ONE DOC IRRELEVANT: WILL IMPROVE QUERY AND RUN WEB SEARCH TO RETRIEVE ADDITIONAL RELEVANT DOCS\")\n",
    "        return \"improve_query\"\n",
    "    else:\n",
    "        print(\"---ALL DOCS FOUNDS TO BE RELEVANT: NO NEED TO RUN WEB SEARCH, WILL GO AHEAD AND GENERATE ANSWER---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "web_api_tool = TavilySearchAPIRetriever(k=5)\n",
    "\n",
    "web_api_results = web_api_tool.invoke(\"what is the One Piece?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='According to one estimate, the series boasts over 516.6 million in sales and broke the world record for “the most copies published for the same comic book series by a single author” in 2015 and again in 2022.\\n How long are the One Piece manga and anime?\\nAt the time of publication, the streaming service Crunchyroll has 1,073 episodes of the One Piece anime. The entertainment analytics firm Parrot Analytics finds that audience demand for One Piece is “40.7 times the demand of the average TV series in the United States in the last 30 days.” At the beginning of the manga and anime, Pirate King Gol D. Roger confirms the existence of the mythological treasure known as the One Piece. The anime series takes certain liberties to adapt the story of the manga — like extending fight scenes or adding characters — but it generally follows the same story beats of the manga.'),\n",
       " Document(page_content='The reading of pirate biographies influenced Oda to incorporate the characteristics of real-life pirates into many of the characters in One Piece; for example, the character Marshall D. Teach is based on and named after the historical pirate Edward \"Blackbeard\" Teach.[3] Apart from the history of piracy, Oda\\'s biggest influence is Akira Toriyama and his series Dragon Ball, which is one of his favorite manga.[4] He was also inspired by The Wizard of Oz, claiming not to endure stories where the reward of adventure is the adventure itself, opting for a story where travel is important, but even more important is the goal.[5]\\nWhile working as an assistant to Nobuhiro Watsuki, Oda began writing One Piece in 1996.[6] Funimation has licensed the eighth, tenth, and twelfth films for release in North America, and these films have received in-house dubs by the company.[62][63]\\nLive-action series\\nOn July 21, 2017, Weekly Shōnen Jump editor-in-chief Hiroyuki Nakano announced that Tomorrow Studios (a partnership between Marty Adelstein and ITV Studios) and Shueisha would commence production of an American live-action television adaptation of Eiichiro Oda\\'s One Piece manga series as part of the series\\' 20th anniversary celebrations.[64][65] Eiichiro Oda served as executive producer for the series alongside Tomorrow Studios CEO Adelstein and Becky Clements.[65]\\nThe series would reportedly begin with the East Blue arc.[66]\\n In 2021, TV Asahi announced the results of its \"Manga General Election\" poll in which 150,000 people voted for their \"Most Favorite Manga\", One Piece ranked first on the list.[226][227]\\nIn 2014, the \"One Piece Premiere Summer\" event received the \"Best Overall Production\" award from the International Association of Amusement Parks and Attractions.[228]\\nCultural impact\\nAs part of an effort to help Kumamoto Prefecture recover from the 2016 earthquakes, Oda helped set up 10 statues of the Straw Hat Pirates around the prefecture. He also notes that the influence of Akira Toriyama (Dragon Ball) shines through in Oda\\'s style of writing with its \"huge epic battles punctuated by a lot of humor\" and that, in One Piece, he \"manages to share a rich tale without getting bogged down by overly complicated plots\".[177] Rebecca Silverman of the same site stated that one of the series\\' strengths is to \"blend action, humor, and heavy fare together\" and praised the art, but stated that the panels could get too crowded for easy reading.[178] Oda\\'s work program includes the first three days of the week dedicated to the writing of the storyboard and the remaining time for the definitive inking of the boards and for the possible coloring.[14] When a reader asked who Nami was in love with, Oda replied that there would hardly be any love affairs within Luffy\\'s crew.'),\n",
       " Document(page_content='In Japanese, some words can be written with a double reading, a phonetic and a written one. For One Piece, the phonetic reading is the English words \"One Piece\", but the written reading is \"hitotsunagi no daihihou\" (ひとつなぎの大秘宝). \"Daihihou\" means \"great hidden treasure\", while \"hitotsunagi\" traditionally ...'),\n",
       " Document(page_content='-More than just anime-\\nRelated Anime\\nCharacters & Voice Actors\\nMonkey D., Luffy\\nRoronoa, Zoro\\nSanji\\nNico, Robin\\nTony Tony, Chopper\\nNami\\nUsopp\\nBrook\\nFranky\\nJinbe\\nStaff\\nOpening Theme\\nEnding Theme\\nReviews\\nInterest Stacks\\nRecommendations\\nRecent News\\nNorth American Anime & Manga Releases for March\\nHere are the North American anime, manga, and light novel releases for March. by MAL_editing_team\\n23,758 views\\nMoreTop Anime\\nMoreTop Airing Anime\\nMoreMost Popular Characters\\nHome\\nAbout\\nPress Room\\nSupport\\nAdvertising\\nFAQ\\nTerms\\nPrivacy\\nPrivacy Settings\\nDo Not Sell My Personal Information\\nCookie\\nNotice at Collection\\nSitemap\\nLogin\\nSign Up One Piece\\nAlternative Titles\\nInformation\\nStatistics\\nAvailable At\\nResources\\nStreaming Platforms\\nSynopsis\\nBarely surviving in a barrel after passing through a terrible whirlpool at sea, carefree Monkey D. Luffy ends up aboard a ship under attack by fearsome pirates. Recent Forum Discussion\\nRecent Featured Articles\\nThe Disappearing Art of Printing Manga, and Its Rebirth as Manga Art\\nby MAL_editing_team\\n39,974 views\\nShueisha to Market Manga Art of One Piece and Other Series Worldwide!\\n Week 1: March 5 - 11 Anime Releases Kaifuku Jutsushi no Yarinaoshi (Redo of Healer) Com...read more\\nYesterday, 9:00 AM by Aiimee | Discuss (0 comments)\\nNorth American Anime & Manga Releases for February\\nHere are the North American anime, manga, and light novel releases for February.'),\n",
       " Document(page_content='Here, he left behind a treasure of unimaginable value.[3] Stories of this treasure on the final island piqued the interest of Gol D. Roger, and he took the World Government forbidding exploration of the island as evidence of it being real.[4]\\nOnly the members of the Roger Pirates crew that journeyed to the island learned what exactly the treasure consists of. Roger described it as a \"tale full of laughs\", which gave him the idea to name the final island \"Laugh Tale\".[3] Sometime after the Roger Pirates\\' discovery, the world at large would begin to refer to Roger\\'s treasures as the \"One Piece\".[2]\\nBefore Roger was executed, he announced to the world that this great treasure could be claimed by anyone who could reach it, thereby starting the Great Age of Pirates.\\n He mentioned that \"a grand battle will engulf the entire world\" and \"the world will be shaken to its core\" when the One Piece is found.[6]\\nDuring the Wano Country Arc, Big Mom had an internal monologue expressing the idea that \"some of\" the One Piece might be located in Wano Country rather than Laugh Tale. This has yet to be elaborated on further.[7]\\nTranslation and Dub Issues[]\\nIn the original manga, the term \"One Piece\" (ワンピース, Wan Pīsu?) is often accompanied by an additional string of text translating roughly to \"the great hito-tsunagi treasure\" (ひとつなぎの大秘宝, Hito-tsunagi no Dai-hihō?), initially as a separate descriptor[1] and later as a base-text directly glossed with the \"One Piece\" katakana.[8]\\n However, Luffy stopped him on the grounds that learning about it from someone else would defeat the purpose of their adventures and that becoming the Pirate King would have little merit if he already knew anything about the One Piece.[5]\\nAfter decades of speculations and doubts, the treasure\\'s existence was confirmed by Whitebeard with his last breath.')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Document(page_content=i.page_content) for i in web_api_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incoporate nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grading_docs\", grading_docs)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"improve_query\", improve_query)\n",
    "workflow.add_node(\"web_search\", web_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grading_docs\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grading_docs\", # the entry node where the edge is coming from\n",
    "    retrieved_decission_maker_with_web_search, # the function that will provide a decision based on the entry node's output\n",
    "    {\n",
    "        \"improve_query\":\"improve_query\", #pair: first is the decision of the deciding function, second is the node to follow if that decision happens\n",
    "        \"generate\":\"generate\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"improve_query\", \"web_search\") # Will improve query and apply to web search to retrieve more relevant documents and incorporate to doc corpus\n",
    "workflow.add_edge(\"web_search\", \"generate\") # With both, internal Vector DB relevant docs and web docs, it will generate an answer\n",
    "workflow.add_edge(\"generate\", END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LangGraph-based Multi-Agent App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---GRADE---\n",
      "---Doc is relevant---\n",
      "---Doc is relevant---\n",
      "---Doc is relevant---\n",
      "---Doc is relevant---\n",
      "\"Node 'grading_docs':\"\n",
      "'\\n---\\n'\n",
      "---GT:2 - ASSESS IF NEEDS TO RE-GENERATE QUERY AND ADD WEB SEARCH RESULTS---\n",
      "---ALL DOCS FOUNDS TO BE RELEVANT: NO NEED TO RUN WEB SEARCH, WILL GO AHEAD AND GENERATE ANSWER---\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "\"Node '__end__':\"\n",
      "'\\n---\\n'\n",
      "('Different types of agent memory work as follows: Short-term memory in agents '\n",
      " \"is akin to in-context learning, utilizing the model's immediate context to \"\n",
      " \"learn and adapt, but it is limited by the model's finite context window. \"\n",
      " 'Long-term memory allows agents to retain and recall information over '\n",
      " 'extended periods, often through an external vector store that supports fast '\n",
      " 'retrieval, enabling the agent to access a vast amount of information beyond '\n",
      " 'its immediate context. Sensory memory in agents could be compared to '\n",
      " 'learning embedding representations for raw inputs, allowing for the initial '\n",
      " 'processing of sensory information in various modalities before further '\n",
      " 'cognitive processing.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
    "for output in multi_agent_app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"]\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Succesfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
