{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground 3\n",
    "\n",
    "Workshopping LLM Output Code Execution and Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, os, pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import random\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import JsonOutputToolsParser, JsonOutputKeyToolsParser\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent, create_react_agent, Tool\n",
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain import hub\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Python execution tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[0.06119131 0.76473527]\\n [0.82000326 0.26274933]\\n [0.85977688 0.88628719]]\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_repl = PythonREPL()\n",
    "\n",
    "python_repl.run(\"import numpy as np; x=np.random.rand(3,2); print(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()\n",
    "\n",
    "exec_python_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.1, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([exec_python_tool])\n",
    "\n",
    "python_create_chain = llm_with_tools | JsonOutputToolsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'python_repl',\n",
       "  'args': {'__arg1': 'import numpy as np\\n\\n# Generate a random vector of size 3x2\\nrandom_vector = np.random.rand(3, 2)\\n\\nprint(random_vector)'}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = python_create_chain.invoke(\"can you generate a random vector in python of size 3 by 2?\")\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\n\\n# Generate a random vector of size 3x2\\nrandom_vector = np.random.rand(3, 2)\\n\\nprint(random_vector)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][\"args\"][\"__arg1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use with a more specific template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is in markdown we need to clean the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_output(text: str):\n",
    "    _, after = text.split(\"```python\")\n",
    "    return after.split(\"```\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[0.43141703 0.19956448]\\n [0.26696694 0.17928585]\\n [0.59987539 0.47697729]]\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_create_chain_2 =  prompt | llm | StrOutputParser() | _sanitize_output | PythonREPL().run\n",
    "\n",
    "python_create_chain_2.invoke(\n",
    "    {\n",
    "        \"input\":\"can you generate a random vector in python of size 3 by 2?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's great, but we probably need a way to ask the user to look at the code an confirm it before running for more complex uses cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [PythonREPLTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You should always show the user the code you and get a confirmation before executing the code. Only\n",
    "execute it after the user has provided a confirmation that the code is OK. If not, iterate with the user\n",
    "until the user confirms the code is good to run. Then and only then, execute the code.\n",
    "\n",
    "To execute it you have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Python_REPL` with `from datetime import datetime\n",
      "print(datetime.strptime('2019-12-01', '%Y-%m-%d').strftime('%B %d, %Y'))`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mDecember 01, 2019\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThe first Covid-19 case was reported on December 1, 2019.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'When was the first Covid-19 case reported?',\n",
       " 'output': 'The first Covid-19 case was reported on December 1, 2019.'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "        {\n",
    "        \"input\": \"When was the first Covid-19 case reported?\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Python_REPL` with `import numpy as np\n",
      "\n",
      "# Generating a random vector of size 3x2\n",
      "random_vector = np.random.rand(3, 2)\n",
      "print(random_vector)`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[[0.37109486 0.2972693 ]\n",
      " [0.92238279 0.498591  ]\n",
      " [0.40822424 0.96920843]]\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mI have generated a random vector of size 3 by 2. Here are the values:\n",
      "\n",
      "```\n",
      "[[0.37109486 0.2972693 ]\n",
      " [0.92238279 0.498591  ]\n",
      " [0.40822424 0.96920843]]\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you generate a random vector in python of size 3 by 2?',\n",
       " 'output': 'I have generated a random vector of size 3 by 2. Here are the values:\\n\\n```\\n[[0.37109486 0.2972693 ]\\n [0.92238279 0.498591  ]\\n [0.40822424 0.96920843]]\\n```'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "        {\n",
    "        \"input\": \"can you generate a random vector in python of size 3 by 2?\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCertainly! To import the Titanic dataset in Python, you can use the `pandas` library, which is a powerful tool for data manipulation and analysis. If you don't already have `pandas` installed, you can install it using pip by running `!pip install pandas` in your Python environment.\n",
      "\n",
      "Here's a code snippet to import the Titanic dataset assuming you have the dataset in a CSV file named `titanic.csv`:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load the Titanic dataset\n",
      "titanic_data = pd.read_csv('titanic.csv')\n",
      "\n",
      "# Display the first few rows of the dataset\n",
      "print(titanic_data.head())\n",
      "```\n",
      "\n",
      "If you're looking to import the Titanic dataset from a specific source online, such as from a URL, you can modify the `pd.read_csv()` function to include the URL directly. For example, if the dataset is hosted on a website, you can do the following:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# URL of the Titanic dataset\n",
      "url = 'http://example.com/titanic.csv'\n",
      "\n",
      "# Load the Titanic dataset from the URL\n",
      "titanic_data = pd.read_csv(url)\n",
      "\n",
      "# Display the first few rows of the dataset\n",
      "print(titanic_data.head())\n",
      "```\n",
      "\n",
      "Replace `'http://example.com/titanic.csv'` with the actual URL where the dataset is hosted.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you generage code to import the titanic data in python?',\n",
       " 'output': \"Certainly! To import the Titanic dataset in Python, you can use the `pandas` library, which is a powerful tool for data manipulation and analysis. If you don't already have `pandas` installed, you can install it using pip by running `!pip install pandas` in your Python environment.\\n\\nHere's a code snippet to import the Titanic dataset assuming you have the dataset in a CSV file named `titanic.csv`:\\n\\n```python\\nimport pandas as pd\\n\\n# Load the Titanic dataset\\ntitanic_data = pd.read_csv('titanic.csv')\\n\\n# Display the first few rows of the dataset\\nprint(titanic_data.head())\\n```\\n\\nIf you're looking to import the Titanic dataset from a specific source online, such as from a URL, you can modify the `pd.read_csv()` function to include the URL directly. For example, if the dataset is hosted on a website, you can do the following:\\n\\n```python\\nimport pandas as pd\\n\\n# URL of the Titanic dataset\\nurl = 'http://example.com/titanic.csv'\\n\\n# Load the Titanic dataset from the URL\\ntitanic_data = pd.read_csv(url)\\n\\n# Display the first few rows of the dataset\\nprint(titanic_data.head())\\n```\\n\\nReplace `'http://example.com/titanic.csv'` with the actual URL where the dataset is hosted.\"}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "        {\n",
    "        \"input\": \"can you generage code to import the titanic data in python?\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So probably asking the agent to first check with us on the answer, and then execute the code may not work with a single agent. We need a multi-agent workflow. LangGraph is being explored below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, MessageGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize graph with a single node called `oracle` that executes chat model and returns result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = MessageGraph()\n",
    "\n",
    "graph.add_node(\"oracle\", llm)\n",
    "graph.add_edge(\"oracle\", END) # Means execution will end after current node\n",
    "\n",
    "graph.set_entry_point(\"oracle\") # Setting \"oracle\" as entry point to graph\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Annotated[list[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], <function add_messages at 0x16c44b820>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(nodes={'__start__': Node(id='__start__', data=<class 'pydantic.v1.main.LangGraphInput'>), '__end__': Node(id='__end__', data=<class 'pydantic.v1.main.LangGraphOutput'>), 'oracle': Node(id='oracle', data=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x16a2421f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x103bd6790>, model_name='gpt-4-turbo-preview', temperature=0.1, openai_api_key=SecretStr('**********'), openai_organization='org-HCpyLwy1gf2wzE49sCGQRU79', openai_proxy='', streaming=True))}, edges=[Edge(source='oracle', target='__end__', data=None), Edge(source='__start__', target='oracle', data=None)])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.get_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='When did Covid-19 start?', id='11d82691-04fe-4c5d-8a2c-b09a61b8694e'),\n",
       " AIMessage(content='The COVID-19 pandemic, caused by the novel coronavirus SARS-CoV-2, is believed to have originated in December 2019. The first cases were reported in the city of Wuhan, Hubei province, China. The World Health Organization (WHO) was alerted to several cases of pneumonia in Wuhan on December 31, 2019. The virus did not match any other known virus, leading to an urgent investigation.', response_metadata={'finish_reason': 'stop'}, id='05f2e7b6-b45f-4e6d-8217-ce06e6464505')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(\n",
    "    HumanMessage(content=\"When did Covid-19 start?\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(first_number:int, second_number:int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"\\nYou are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\n\\nQuestion: {question} \\n\\nContext: {context} \\n\\nAnswer:\\n\"))])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\\n\n",
    "\n",
    "Question: {question} \\n\n",
    "Context: {context} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(human_template)\n",
    "    ]    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"\\nYou are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\n\\nQuestion: {question} \\n\\nContext: {context} \\n\\nAnswer:\\n\"))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", human_template)\n",
    "    ]    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
