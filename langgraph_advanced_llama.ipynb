{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Advanced with Llama 3\n",
    "\n",
    "Will apply [Llama 3 (8B Model)](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3) through [Ollama](https://python.langchain.com/docs/integrations/chat/ollama/) to build [LangGraph](https://python.langchain.com/docs/langgraph/) multi-agent RAG Sytems\n",
    "\n",
    "Ensure that you have `Ollama` running and have pulled `Llama3:8B` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Dict, List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.docstore.document import Document\n",
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration from [Langchain Videos](https://www.youtube.com/watch?v=-ROS6gfYIts) and [previous notebook](https://github.com/jzamalloa1/langchain_learning/blob/main/langgraph_testing.ipynb)\n",
    "\n",
    "<p>\n",
    "<img src=\"ILLUSTRATIONS/langgraph_advanced_flow.png\" \n",
    "      width=\"65%\" height=\"auto\"\n",
    "      style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Illustration [reference](https://github.com/jzamalloa1/langchain_learning/blob/main/langgraph_testing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key solely to use embedding model\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = \"llama3:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source for Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    \n",
    "]\n",
    "\n",
    "plotly_yt_urls = [\n",
    "    \"https://www.youtube.com/watch?v=Qx5eFVUdDxk&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=1\",\n",
    "    \"https://www.youtube.com/watch?v=Z9YUejzkFa0&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=2\",\n",
    "    \"https://www.youtube.com/watch?v=4bP66rRxVBw&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=3\",\n",
    "    \"https://www.youtube.com/watch?v=a1qzu5GKIf0&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=4\",\n",
    "    \"https://www.youtube.com/watch?v=Fm7DC-Z5R7A&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=5\",\n",
    "    \"https://www.youtube.com/watch?v=4jcWJ30HqSY&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=6\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Retrievers - One for LLM docs and one for Plotly docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 194 chunks\n",
      "Split into 109 chunks\n"
     ]
    }
   ],
   "source": [
    "#### LLM DOCS VECTOR STORE #####\n",
    "llm_docs = [WebBaseLoader(url).load() for url in llm_urls] # Each loader produced a list of one element\n",
    "llm_docs = [i for j in llm_docs for i in j] # Decoupling lists of one element\n",
    "\n",
    "text_splitter_class = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = text_splitter_class.split_documents(llm_docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "llm_llw_vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = \"chroma_llm_llw\"\n",
    ")\n",
    "\n",
    "llm_llw_retriever = llm_llw_vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "#### YOUTUBE PLOTLY VECTOR STORE #####\n",
    "yt_docs = [YoutubeLoader.from_youtube_url(url, add_video_info=False).load() for url in plotly_yt_urls] # Each loader produced a list of one element\n",
    "yt_docs = [i for j in yt_docs for i in j] # Decoupling lists of one element\n",
    "\n",
    "text_splitter_class = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = text_splitter_class.split_documents(yt_docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "plotly_yt_vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = \"chroma_plotly_yt\"\n",
    ")\n",
    "\n",
    "plotly_yt_retriever = plotly_yt_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n",
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm_llw_retriever))\n",
    "print(type(plotly_yt_retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Routers function for Conditional Graph node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ollama** has `format=\"json\"`. This ensures output from llm is a JSON. Recall that in the [ChatOpenAI implementation](https://github.com/jzamalloa1/langchain_learning/blob/main/langgraph_testing.ipynb) we tested before we had to create a pydantic object and bind it to our ChatOpenAI llm to ensure structured output. We don't have to do that here.\n",
    "\n",
    "Note: One way that we could have addressed that could have been by adding `model_kwargs={\"response_format\":{\"type\":\"json_object\"}}` to **ChatOpenAI**, but not sure if this works 100% (or perhaps does work as well as Ollama's json mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"datasource\": \"vectorstore\"} \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'datasource': 'vectorstore'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST ROUTER TO DECIDE IF WE SHOULD USE WEB SEARCH OR INTERNAL VECTOR STORES #####\n",
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are an expert at routing a user question to a vectorstore or web search. \n",
    "    Use the vectorstore for questions on LLM  agents, \n",
    "    prompt engineering, adversarial attacks and developing charts with Plotly. \n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no preamble or explanation. \n",
    "    Question to route: {question} <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "initial_router_chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "initial_router_chain.invoke({\"question\":\"How can I build a histogram using plotly?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"datasource\": \"llm_agent\"} \n",
      "\n",
      "  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   "
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'datasource': 'llm_agent'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### SECOND ROUTER (IF VECTORSTORE IS CHOSEN) TO DECIDE IF WE SHOULD USE THE LLM OR THE PLOTLY RETRIEVER #####\n",
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are an expert at routing a user question to an LLM documentation vectorstore or a Plotly vectorstore. \n",
    "    Use the LLM vectorstore for questions on LLM  agents, \n",
    "    prompt engineering and adversarial attacks. \n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Otherwise, use the plotly vectorstore for things like Plotly charts. Give a binary choice 'llm_agent' \n",
    "    or 'plotly' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no preamble or explanation. \n",
    "    Question to route: {question} <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "vector_router_chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "vector_router_chain.invoke({\"question\":\"what are adversarial agents?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": \"yes\"} \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval Grader\n",
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"langchain memory use\"\n",
    "docs = llm_llw_retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "grader_output = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(grader_output))\n",
    "grader_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Retrieval Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM (Without json mode since we want inference text)\n",
    "llm = ChatOllama(model=llama_model, temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Keep the answer concise and to the point. <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    \n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a lineplot, you can use Plotly Express in Dash. Here's an example:\n",
      "\n",
      "```\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "\n",
      "# Create a sample dataset\n",
      "df = pd.DataFrame({\n",
      "    'Year': [2015, 2016, 2017, 2018, 2019],\n",
      "    'Value': [10, 20, 30, 40, 50]\n",
      "})\n",
      "\n",
      "# Create the lineplot\n",
      "fig = px.line(df, x='Year', y='Value')\n",
      "\n",
      "# Update the figure reference\n",
      "fig.update_layout(title='Lineplot Example')\n",
      "```\n",
      "\n",
      "In this example, we create a sample dataset using Pandas, and then use Plotly Express to create a lineplot. The `px.line()` function takes in the dataset and specifies the x-axis (`'Year'`) and y-axis (`'Value'`) columns.\n",
      "\n",
      "You can customize your lineplot by adding more attributes, such as colors, markers, and ranges. For example, you can add a color attribute to differentiate between different lines:\n",
      "\n",
      "```\n",
      "fig = px.line(df, x='Year', y='Value', color='Value')\n",
      "```\n",
      "\n",
      "This will create a lineplot with different colors for each year's data point.\n",
      "\n",
      "You can also use the `range` attribute to limit the range of your x-axis. For example:\n",
      "\n",
      "```\n",
      "fig = px.line(df, x='Year', y='Value', range_x=[2015, 2019])\n",
      "```\n",
      "\n",
      "This will create a lineplot that only shows data points from 2015 to 2019.\n",
      "\n",
      "Remember to review the Plotly Express documentation page for more information on how to customize your graphs."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To create a lineplot, you can use Plotly Express in Dash. Here's an example:\\n\\n```\\nimport plotly.express as px\\nimport pandas as pd\\n\\n# Create a sample dataset\\ndf = pd.DataFrame({\\n    'Year': [2015, 2016, 2017, 2018, 2019],\\n    'Value': [10, 20, 30, 40, 50]\\n})\\n\\n# Create the lineplot\\nfig = px.line(df, x='Year', y='Value')\\n\\n# Update the figure reference\\nfig.update_layout(title='Lineplot Example')\\n```\\n\\nIn this example, we create a sample dataset using Pandas, and then use Plotly Express to create a lineplot. The `px.line()` function takes in the dataset and specifies the x-axis (`'Year'`) and y-axis (`'Value'`) columns.\\n\\nYou can customize your lineplot by adding more attributes, such as colors, markers, and ranges. For example, you can add a color attribute to differentiate between different lines:\\n\\n```\\nfig = px.line(df, x='Year', y='Value', color='Value')\\n```\\n\\nThis will create a lineplot with different colors for each year's data point.\\n\\nYou can also use the `range` attribute to limit the range of your x-axis. For example:\\n\\n```\\nfig = px.line(df, x='Year', y='Value', range_x=[2015, 2019])\\n```\\n\\nThis will create a lineplot that only shows data points from 2015 to 2019.\\n\\nRemember to review the Plotly Express documentation page for more information on how to customize your graphs.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example run\n",
    "question = \"how can I create a lineplot?\"\n",
    "docs = plotly_yt_retriever.invoke(question)\n",
    "rag_chain.invoke({\"context\":format_docs(docs), \"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the code actually work?? - A simple plot does work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Year=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          2015,
          2016,
          2017,
          2018,
          2019
         ],
         "xaxis": "x",
         "y": [
          10,
          20,
          30,
          40,
          50
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Lineplot Example"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'Year': [2015, 2016, 2017, 2018, 2019],\n",
    "    'Value': [10, 20, 30, 40, 50]\n",
    "})\n",
    "\n",
    "# Create the lineplot\n",
    "fig = px.line(df, x='Year', y='Value')\n",
    "# fig = px.line(df, x='Year', y='Value', color='Value')\n",
    "\n",
    "# Update the figure reference\n",
    "fig.update_layout(title='Lineplot Example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what the One Piece is. The context provided appears to be about a course on Plotly and Dash, but it doesn't mention anything related to \"One Piece\"."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I don\\'t know what the One Piece is. The context provided appears to be about a course on Plotly and Dash, but it doesn\\'t mention anything related to \"One Piece\".'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example run with unrelated question, info not found in vector store\n",
    "question = \"what is the One Piece?\"\n",
    "docs = plotly_yt_retriever.invoke(question)\n",
    "rag_chain.invoke({\"context\":format_docs(docs), \"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Websearch Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_tool = TavilySearchAPIRetriever(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='El conjunto de Andahuaylas cuenta hasta el momento con el 50% de los puntos que se pusieron en juego. En casa derrotó a Unión Comercio, ADT y en la pasada fecha a Sport Boys (2-0).', metadata={'title': 'Goles, Mannucci vs. Los Chankas: ver 2-1, resumen y VIDEO del partido ...', 'source': 'https://rpp.pe/futbol/descentralizado/mannucci-vs-los-chankas-en-vivo-golperu-ver-liga-1-2024-online-link-gratis-apertura-fecha-7-trujillo-noticia-1539796', 'score': 0.97491, 'images': None}),\n",
       " Document(page_content='Visita ESPN DEPORTES y disfruta de resultados en vivo, highlights y las últimas noticias de Los Chankas. Conoce la tabla de posiciones y el calendario completo de la temporada 2024.', metadata={'title': 'Los Chankas Resultados, estadísticas y highlights - ESPN Deportes', 'source': 'https://espndeportes.espn.com/futbol/equipo/_/id/22168', 'score': 0.95997, 'images': None}),\n",
       " Document(page_content='Los Chankas derrotaron 2-0 a Santos FC en el cierre del Torneo Apertura de la Liga 2. Con este resultado, el equipo dirigido por Gustavo Cisneros terminó ubicado en el tercer lugar de las clasificaciones con 20 puntos; mientras que los nasqueños finalizaron en el sexto casillero con 16 unidades. ... que no tuvo acción el pasado fin de semana ...', metadata={'title': 'Los Chankas derrotaron 2-0 a Santos FC en el cierre del Torneo Apertura ...', 'source': 'https://www.futbolperuano.com/liga-2/noticias/los-chankas-vs-santos-fc-en-vivo-online-por-la-fecha-13-del-torneo-apertura-de-la-liga-2-358470', 'score': 0.94727, 'images': None}),\n",
       " Document(page_content='Visita ESPN (CO) y disfruta de resultados en vivo, highlights y las últimas noticias de Los Chankas. Conoce la tabla de posiciones y el calendario completo de la temporada 2024. Salta al contenido principal Ir a la navegación. ESPN. Fútbol ... El equipo de Enderson Moreira volvió al triunfo y llegó a los 10 puntos en la tabla del Apertura. 2M;', metadata={'title': 'Los Chankas Resultados, estadísticas y highlights - ESPN (CO)', 'source': 'https://www.espn.com.co/futbol/equipo/_/id/22168', 'score': 0.94264, 'images': None}),\n",
       " Document(page_content='Con un Martín Cauteruccio intratable, Sporting Cristal goleó 4-1 a Los Chankas por la fecha 4 del Torneo Apertura de la Liga 1 Te Apuesto 2024.El uruguayo anotó tres goles en 20 minutos y es ...', metadata={'title': 'VER RESUMEN y GOLES Sporting Cristal vs. Los Chankas: VIDEO 4-1 por la ...', 'source': 'https://rpp.pe/futbol/descentralizado/sporting-cristal-vs-los-chankas-via-liga-1-max-en-vivo-ver-online-en-directo-partido-en-estadio-nacional-por-fecha-4-del-torneo-apertura-2024-noticia-1534477', 'score': 0.91005, 'images': None})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_tool.invoke(\"A que equipo derrotaron Los Chankas el fin de semana pasado?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|>\n",
    "     \n",
    "     <|start_header_id|>user<|end_header_id|> \n",
    "     Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: \n",
    "    {question} \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": \"no\"} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test answer grader\n",
    "answer_grader.invoke({\"question\": \"How do we build graphs in plotly?\",\n",
    "                      \"generation\": \"The One Piece is the best manga in the world\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUILD QUERY IMPROVE (RE-GENERATE QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I effectively utilize Plotly to create well-structured and informative line charts, considering various visualization options and customization possibilities?"
     ]
    }
   ],
   "source": [
    "# LLM (Without json mode since we want inference text)\n",
    "llm = ChatOllama(model=llama_model, temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are question re-writer that converts an input question to a better \n",
    "    version that is optimized for vectorstore retrieval.\n",
    "    Look at the input and try to reason about the underlying \n",
    "    semantic intent / meaning and re-write it.\n",
    "    Don't provide a preamble.\n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question}\n",
    "    \\n ------- \\n\n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    \n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "re_write_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Improve query example\n",
    "question = \"what is the write way to use plotly to define line charts?\"\n",
    "improved_question = re_write_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph - Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \n",
    "    question: str\n",
    "    generation: str\n",
    "    docs: List[str]\n",
    "    run_web_search: str\n",
    "    retriever: VectorStoreRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph - Define Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NODES #####\n",
    "\n",
    "# RETRIEVER SELECTOR NODE - WILL RETURN APPROPRIATE RETRIEVER BASED ON VECTOR_ROUTER OUTPUT\n",
    "def retriever_selector(state): \n",
    "\n",
    "    \"\"\"\n",
    "    Selects retriever to be used from either plotly or llm agent vector store retrievers based on vector_router llm output\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---VECTOR STORE SOURCE ROUTER---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    vector_source_response = vector_router_chain.invoke({\"question\":question})\n",
    "\n",
    "    if vector_source_response[\"datasource\"] == \"llm_agent\":\n",
    "        print(\"---VECTOR ROUTING TO: LLM AGENT DOCS---\")\n",
    "        # return \"llm_agent_vdb\"\n",
    "        db_retriever =llm_llw_retriever\n",
    "\n",
    "    elif vector_source_response[\"datasource\"] == \"plotly\":\n",
    "        print(\"---INITIAL ROUTE TO: PLOTLY DOCS---\")\n",
    "        db_retriever =plotly_yt_retriever\n",
    "    \n",
    "    # Return updated state with selected retriever\n",
    "    return {\"question\":question, \"retriever\":db_retriever}\n",
    "    \n",
    "# RETRIEVER NODE\n",
    "def retrieve(state):\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "\n",
    "    # Get a question from the state dictionary\n",
    "    question = state[\"question\"]\n",
    "    retriever = state[\"retriever\"] # Of class VectorStoreRetriever (either llm_llw_retriever or plotly_yt_retriever in our case)\n",
    "\n",
    "    # Execute retriever to get documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Return to write back (update) state\n",
    "    return {\"docs\":docs, \"question\":question}\n",
    "\n",
    "# GENERATE NODE\n",
    "def generate(state):\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "\n",
    "    # Get question from dictionary\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Get documents stored in update state (theoretically retrieved using retriever)\n",
    "    docs = state[\"docs\"]\n",
    "\n",
    "    # Generate\n",
    "    generation = rag_chain.invoke({\"context\":format_docs(docs), \"question\":question})\n",
    "\n",
    "    # Return generation to update state\n",
    "    return {\"docs\":docs, \"question\":question, \"generation\":generation}\n",
    "\n",
    "# GRADING DOCUMENTS NODE\n",
    "def grading_docs(state):\n",
    "\n",
    "    print(\"---GRADE DOCS WITH REGARDS TO QUESTION---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"docs\"]\n",
    "\n",
    "    # Grade retrieved docs with grader_chain AND add `web_search` if at least one document irrelevant\n",
    "    filtered_docs = []\n",
    "    run_web_search = \"no\" # Start variable as \"no\" and change only if we get a single irrelevant document\n",
    "    for i in docs:\n",
    "        score = retrieval_grader.invoke({\"question\":question, \"document\":i.page_content})\n",
    "                \n",
    "        grade = score.score\n",
    "\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---Doc is relevant---\")\n",
    "            filtered_docs.append(i)\n",
    "        else:\n",
    "            print(\"---Doc is not relevant\")\n",
    "            run_web_search = \"yes\" # WILL PERFORM WEB SEARCH if we have an irrelevant document\n",
    "            continue\n",
    "    \n",
    "    # Update state with filtered docs\n",
    "    return {\"docs\": filtered_docs, \"question\":question, \"run_web_search\":run_web_search}\n",
    "\n",
    "# IMPROVE QUERY NODE\n",
    "def improve_query(state):\n",
    "    \"\"\"\n",
    "    Improve query being asked by re-generating it with an LLM\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---IMPROVE QUERY---\")\n",
    "\n",
    "    question  = state[\"question\"]\n",
    "    docs = state[\"docs\"]\n",
    "\n",
    "    # Improve query\n",
    "    improved_question = re_write_chain.invoke({\"question\":question})\n",
    "\n",
    "    # Update state with improved question\n",
    "    return {\"docs\": docs, \"question\":improved_question}\n",
    "\n",
    "\n",
    "# WEB SEARCH NODE - REVISE!!! EITHER IF WEB SEARCH DUE TO INITIAL ROUTING OR WEB SEARCH BECAUSE OF YES IN GRADER. THIS AFFECT DOCS FED AND EXTEND!\n",
    "def web_search(state):\n",
    "\n",
    "    \"\"\"\n",
    "    Web search using query to retrieve relevant docs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH FOR DOCS---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"docs\"] # MAY NOT HAVE DOCS AT FIRST, NEED TO PROBABLY INITIALY WITH NONE!!!\n",
    "\n",
    "    # Retrieve relevant web documents\n",
    "    web_results = web_tool.invoke(question)\n",
    "    web_docs = [Document(page_content = i.page_content) for i in web_results]\n",
    "\n",
    "    if docs is not None: # DOES NOT HAVE DOCS ON INITIAL ROUTER!!\n",
    "        # Add to running document list\n",
    "        docs.extend(web_docs)\n",
    "\n",
    "    else:\n",
    "        docs = web_docs\n",
    "\n",
    "    # Return updated document list into state\n",
    "    return {\"docs\":docs, \"question\":question}\n",
    "\n",
    "\n",
    "###### CONDITIONAL EDGES #######\n",
    "\n",
    "# INITIAL ROUTING EDGE\n",
    "\n",
    "def initial_router(state):\n",
    "\n",
    "    \"\"\"\n",
    "    Routes query to web search or vector database\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---INITIAL ROUTER----\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    source_choice_response = initial_router_chain.invoke({\"question\":question})\n",
    "\n",
    "    if source_choice_response[\"datasource\"] == \"web_search\":\n",
    "        print(\"---INITIAL ROUTE TO: WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "\n",
    "    elif source_choice_response[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---INITIAL ROUTE TO: VECTOR STORE---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def retrieved_decission_maker_with_web_search(state):\n",
    "    \"\"\"\n",
    "    Will decide to re-formulate query + pass that to web search if at least\n",
    "    one retrieved document was found to be non-relevant\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---COND EDGE: ASSESS IF NEEDS TO RE-GENERATE QUERY AND ADD WEB SEARCH RESULTS OR NO WEB SEARCH NEEDED---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    filtered_docs = state[\"docs\"]\n",
    "    run_web_search = state[\"run_web_search\"]\n",
    "\n",
    "    if run_web_search==\"yes\":\n",
    "        print(\"---FOUND AT LEAST ONE DOC IRRELEVANT: WILL IMPROVE QUERY AND RUN WEB SEARCH TO RETRIEVE ADDITIONAL RELEVANT DOCS\")\n",
    "        return \"improve_query\"\n",
    "    else:\n",
    "        print(\"---ALL DOCS FOUNDS TO BE RELEVANT: NO NEED TO RUN WEB SEARCH, WILL GO AHEAD AND GENERATE ANSWER---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph - Define Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we use `set_conditional_entry_point` instead of `set_entry_point` since we are using a conditional edge on entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retriever_selector\", retriever_selector)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"grading_docs\", grading_docs)\n",
    "workflow.add_node(\"improve_query\", improve_query)\n",
    "workflow.add_node(\"websearch\", web_search) # Notice key_name versus function being called\n",
    "\n",
    "workflow.set_conditional_entry_point( \n",
    "    initial_router,\n",
    "    {\n",
    "        \"websearch\":\"websearch\", # Notice key_name used as decision output and second argument is the node to follow\n",
    "        \"vectorstore\":\"retriever_selector\" # More visible here, first argument is output of \"initial_router\", and second argument is decision node/edge\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"retriever_selector\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grading_docs\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grading_docs\", # Source node\n",
    "    retrieved_decission_maker_with_web_search, # End node, notice how end node in conditional edges are the actual function variables\n",
    "    {\n",
    "        \"improve_query\":\"improve_query\", # If conditional edge output is \"improve_query\", then \"improve_query\" node will be called\n",
    "        \"generate\":\"generate\" # If conditional edge output is \"generate\", then \"generate\" node will be called\n",
    "    }\n",
    "    \n",
    ")\n",
    "workflow.add_edge(\"improve_query\", \"websearch\")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "\n",
    "workflow.add_edge(\"websearch\", \"....\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
