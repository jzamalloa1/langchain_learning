{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Advanced with Llama 3\n",
    "\n",
    "Will apply [Llama 3 (8B Model)](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3) through [Ollama](https://python.langchain.com/docs/integrations/chat/ollama/) to build [LangGraph](https://python.langchain.com/docs/langgraph/) multi-agent RAG Sytems\n",
    "\n",
    "Ensure that you have `Ollama` running and have pulled `Llama3:8B` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration from [Langchain Videos](https://www.youtube.com/watch?v=-ROS6gfYIts) and [previous notebook](https://github.com/jzamalloa1/langchain_learning/blob/main/langgraph_testing.ipynb)\n",
    "\n",
    "<p>\n",
    "<img src=\"ILLUSTRATIONS/langgraph_advanced_flow.png\" \n",
    "      width=\"65%\" height=\"auto\"\n",
    "      style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Illustration [reference](https://github.com/jzamalloa1/langchain_learning/blob/main/langgraph_testing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key solely to use embedding model\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = \"llama3:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source for Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    \n",
    "]\n",
    "\n",
    "plotly_yt_urls = [\n",
    "    \"https://www.youtube.com/watch?v=Qx5eFVUdDxk&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=1\",\n",
    "    \"https://www.youtube.com/watch?v=Z9YUejzkFa0&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=2\",\n",
    "    \"https://www.youtube.com/watch?v=4bP66rRxVBw&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=3\",\n",
    "    \"https://www.youtube.com/watch?v=a1qzu5GKIf0&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=4\",\n",
    "    \"https://www.youtube.com/watch?v=Fm7DC-Z5R7A&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=5\",\n",
    "    \"https://www.youtube.com/watch?v=4jcWJ30HqSY&list=PLYD54mj9I2JevdabetHsJ3RLCeMyBNKYV&index=6\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Retrievers - One for LLM docs and one for Plotly docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 194 chunks\n",
      "Split into 109 chunks\n"
     ]
    }
   ],
   "source": [
    "#### LLM DOCS VECTOR STORE #####\n",
    "llm_docs = [WebBaseLoader(url).load() for url in llm_urls] # Each loader produced a list of one element\n",
    "llm_docs = [i for j in llm_docs for i in j] # Decoupling lists of one element\n",
    "\n",
    "text_splitter_class = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = text_splitter_class.split_documents(llm_docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "llm_llw_vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = \"chroma_llm_llw\"\n",
    ")\n",
    "\n",
    "llm_llw_retriever = llm_llw_vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "#### YOUTUBE PLOTLY VECTOR STORE #####\n",
    "yt_docs = [YoutubeLoader.from_youtube_url(url, add_video_info=False).load() for url in plotly_yt_urls] # Each loader produced a list of one element\n",
    "yt_docs = [i for j in yt_docs for i in j] # Decoupling lists of one element\n",
    "\n",
    "text_splitter_class = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = text_splitter_class.split_documents(yt_docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "plotly_yt_vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embedding_model,\n",
    "    collection_name = \"chroma_plotly_yt\"\n",
    ")\n",
    "\n",
    "plotly_yt_retriever = plotly_yt_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Routers function for Conditional Graph node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ollama** has `format=\"json\"`. This ensures output from llm is a JSON. Recall that in the [ChatOpenAI implementation](https://github.com/jzamalloa1/langchain_learning/blob/main/langgraph_testing.ipynb) we tested before we had to create a pydantic object and bind it to our ChatOpenAI llm to ensure structured output. We don't have to do that here.\n",
    "\n",
    "Note: One way that we could have addressed that could have been by adding `model_kwargs={\"response_format\":{\"type\":\"json_object\"}}` to **ChatOpenAI**, but not sure if this works 100% (or perhaps does work as well as Ollama's json mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"datasource\": \"vectorstore\"} \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'datasource': 'vectorstore'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST ROUTER TO DECIDE IF WE SHOULD USE WEB SEARCH OR INTERNAL VECTOR STORES #####\n",
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are an expert at routing a user question to a vectorstore or web search. \n",
    "    Use the vectorstore for questions on LLM  agents, \n",
    "    prompt engineering, adversarial attacks and developing charts with Plotly. \n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explaination. \n",
    "    Question to route: {question} <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "initial_router_chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "initial_router_chain.invoke({\"question\":\"How can I build a histogram using plotly?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"datasource\": \"llm_agent\"} \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'datasource': 'llm_agent'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### SECOND ROUTER (IF VECTORSTORE IS CHOSEN) TO DECIDE IF WE SHOULD USE THE LLM OR THE PLOTLY RETRIEVER #####\n",
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are an expert at routing a user question to an LLM documentation vectorstore or a Plotly vectorstore. \n",
    "    Use the LLM vectorstore for questions on LLM  agents, \n",
    "    prompt engineering and adversarial attacks. \n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Otherwise, use the plotly vectorstore for things like Plotly charts. Give a binary choice 'llm_agent' \n",
    "    or 'plotly' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explaination. \n",
    "    Question to route: {question} <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "initial_router_chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "initial_router_chain.invoke({\"question\":\"what are adversarial agents?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": \"yes\"} \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval Grader\n",
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"langchain memory use\"\n",
    "docs = llm_llw_retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "grader_output = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(grader_output))\n",
    "grader_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Retrieval Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM (Without json mode since we want inference text)\n",
    "llm = ChatOllama(model=llama_model, temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Keep the answer concise and to the point. <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    \n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a lineplot, you can use Plotly Express in Dash. You would do this by using the `px.line` function and specifying the x-axis and y-axis data. For example:\n",
      "\n",
      "```\n",
      "import dash\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "from dash.dependencies import Input, Output\n",
      "\n",
      "app = dash.Dash()\n",
      "\n",
      "app.layout = html.Div([\n",
      "    dcc.Graph(id='line-plot'),\n",
      "])\n",
      "\n",
      "@app.callback(Output('line-plot', 'figure'), [Input('dropdown', 'value')])\n",
      "def update_line_plot(value):\n",
      "    fig = px.line(x=value['x'], y=value['y'])\n",
      "    return fig\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run_server(debug=True)\n",
      "```\n",
      "\n",
      "This code creates a simple line plot with the x-axis and y-axis data specified in the `update_line_plot` function."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To create a lineplot, you can use Plotly Express in Dash. You would do this by using the `px.line` function and specifying the x-axis and y-axis data. For example:\\n\\n```\\nimport dash\\nimport dash_core_components as dcc\\nimport dash_html_components as html\\nfrom dash.dependencies import Input, Output\\n\\napp = dash.Dash()\\n\\napp.layout = html.Div([\\n    dcc.Graph(id='line-plot'),\\n])\\n\\n@app.callback(Output('line-plot', 'figure'), [Input('dropdown', 'value')])\\ndef update_line_plot(value):\\n    fig = px.line(x=value['x'], y=value['y'])\\n    return fig\\n\\nif __name__ == '__main__':\\n    app.run_server(debug=True)\\n```\\n\\nThis code creates a simple line plot with the x-axis and y-axis data specified in the `update_line_plot` function.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example run\n",
    "question = \"how can I create a lineplot?\"\n",
    "docs = plotly_yt_retriever.invoke(question)\n",
    "rag_chain.invoke({\"context\":format_docs(docs), \"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what the One Piece is. The context provided seems to be about Dash, a Python framework for building web applications, and Plotly Express, a library for creating interactive visualizations. It doesn't appear to mention anything related to \"One Piece\"."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I don\\'t know what the One Piece is. The context provided seems to be about Dash, a Python framework for building web applications, and Plotly Express, a library for creating interactive visualizations. It doesn\\'t appear to mention anything related to \"One Piece\".'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example run with unrelated question, info not found in vector store\n",
    "question = \"what is the One Piece?\"\n",
    "docs = plotly_yt_retriever.invoke(question)\n",
    "rag_chain.invoke({\"context\":format_docs(docs), \"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Websearch Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_tool = TavilySearchAPIRetriever(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Los Chankas derrotaron 2-0 a Santos FC en el cierre del Torneo Apertura de la Liga 2. Con este resultado, el equipo dirigido por Gustavo Cisneros terminó ubicado en el tercer lugar de las clasificaciones con 20 puntos; mientras que los nasqueños finalizaron en el sexto casillero con 16 unidades. ... que no tuvo acción el pasado fin de semana ...', metadata={'title': 'Los Chankas derrotaron 2-0 a Santos FC en el cierre del Torneo Apertura ...', 'source': 'https://www.futbolperuano.com/liga-2/noticias/los-chankas-vs-santos-fc-en-vivo-online-por-la-fecha-13-del-torneo-apertura-de-la-liga-2-358470', 'score': 0.97505, 'images': None}),\n",
       " Document(page_content='Los Chankas derrotó 2-0 a Sport Boys en Andahuaylas por la fecha 6 del Torneo Apertura 2024. Los goles del equipo local fueron obra de Carlos López y Ángel Ledesma.', metadata={'title': 'En Andahuaylas: Los Chankas venció 2-0 a Sport Boys por el Torneo Apertura', 'source': 'https://depor.com/futbol-peruano/descentralizado/los-chankas-vs-sport-boys-en-vivo-en-directo-via-liga-1-max-directv-claro-y-futbol-libre-a-que-hora-juegan-link-y-donde-ver-gratis-online-el-torneo-apertura-2024-alineaciones-deportes-noticia/', 'score': 0.97491, 'images': None}),\n",
       " Document(page_content='Los Chankas vs. Sport Boys se enfrentarán EN VIVO y EN DIRECTO este fin de semana en el marco de la fecha 6 del Torneo Apertura 2024 de la Liga 1 Te Apuesto. El partido de Los Chankas contra el ...', metadata={'title': 'Los Chankas ganaron 2-0 a Sport Boys por el Torneo Apertura 2024', 'source': 'https://rpp.pe/futbol/descentralizado/los-chankas-vs-sport-boys-via-liga-1-max-en-vivo-a-que-hora-juegan-en-directo-y-donde-ver-partido-por-torneo-apertura-2024-noticia-1538298', 'score': 0.9486, 'images': None}),\n",
       " Document(page_content='El conjunto de Andahuaylas cuenta hasta el momento con el 50% de los puntos que se pusieron en juego. En casa derrotó a Unión Comercio, ADT y en la pasada fecha a Sport Boys (2-0).', metadata={'title': 'Goles, Mannucci vs. Los Chankas: ver 2-1, resumen y VIDEO del partido ...', 'source': 'https://rpp.pe/futbol/descentralizado/mannucci-vs-los-chankas-en-vivo-golperu-ver-liga-1-2024-online-link-gratis-apertura-fecha-7-trujillo-noticia-1539796', 'score': 0.91961, 'images': None}),\n",
       " Document(page_content='Visita ESPN (CO) y disfruta de resultados en vivo, highlights y las últimas noticias de Los Chankas. Conoce la tabla de posiciones y el calendario completo de la temporada 2024. Salta al contenido principal Ir a la navegación. ESPN. Fútbol ... El equipo de Enderson Moreira volvió al triunfo y llegó a los 10 puntos en la tabla del Apertura. 2M;', metadata={'title': 'Los Chankas Resultados, estadísticas y highlights - ESPN (CO)', 'source': 'https://www.espn.com.co/futbol/equipo/_/id/22168', 'score': 0.91259, 'images': None})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_tool.invoke(\"A que equipo derrotaron Los Chankas el fin de semana pasado?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=llama_model, format=\"json\", temperature=0,\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|>\n",
    "     \n",
    "     <|start_header_id|>user<|end_header_id|> \n",
    "     Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: \n",
    "    {question} \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": \"no\"} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test answer grader\n",
    "answer_grader.invoke({\"question\": \"How do we build graphs in plotly?\",\n",
    "                      \"generation\": \"The One Piece is the best manga in the world\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
