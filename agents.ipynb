{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "import openai\n",
    "import json\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have setup your environmental variables beforehand\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_ORGANIZATION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: What is Doge\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# Example of what \"PromptTemplate\" is doing\n",
    "print(prompt_template.format(query=\"What is Doge\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass prompt to model with a query not part of the 'Context.' The model should not know anything about it because we indicated so in the \"Instructions\" (first paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_openai = OpenAI(model_name=\"gpt-4-0613\", temperature=0, max_tokens=512)\n",
    "\n",
    "llm_openai(prompt_template.format(query=\"What is Doge\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass a query asking for something found in the context section. The model should be able to provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library offer LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai(prompt_template.format(query=\"Which libraries and model providers offer LLMs?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare that to simply calling the LLM without the instructions and the context. The LLM should use its own training wealth of information to provide a thorough answer. This can be useful in various occasions, but for bespoke purposes we may want to restrict, like the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Libraries:\n",
      "\n",
      "   - Hugging Face: This library provides a wide range of pre-trained models for various NLP tasks, including language modeling. It supports several transformer-based models such as BERT, GPT-2, RoBERTa, and T5.\n",
      "\n",
      "   - TensorFlow and Keras: These libraries provide functionalities to build and train language models from scratch. They also support pre-trained models.\n",
      "\n",
      "   - PyTorch: Similar to TensorFlow, PyTorch also provides functionalities to build and train language models. It also supports pre-trained models.\n",
      "\n",
      "   - NLTK: The Natural Language Toolkit (NLTK) is a Python library for symbolic and statistical natural language processing. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\n",
      "\n",
      "2. Model Providers:\n",
      "\n",
      "   - OpenAI: They provide models like GPT-2 and GPT-3 which are state-of-the-art language models.\n",
      "\n",
      "   - Google AI: They provide models like BERT and T5 which are widely used for various NLP tasks.\n",
      "\n",
      "   - Facebook AI: They provide models like RoBERTa and BART which are transformer-based models used for various NLP tasks.\n",
      "\n",
      "   - Hugging Face Model Hub: It's a repository of thousands of pre-trained models in 100+ languages.\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai(\"Which libraries and model providers offer LLMs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at few shot examples. Few shot prompt templates is useful for source knowledge, that is, knowledge provided to the model at inference time. Few shot learning will train the model to understand what is it we are trying to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, we could dive into deep philosophy and talk about existentialism and the human condition. Or we could go the route of the Hitchhiker's Guide to the Galaxy and say it's '42', or you know, we could say it's 'Pizza'. All seem equally valid!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "llm_openai.model_kwargs[\"temperature\"] = 1.0  # increase creativity/randomness of output\n",
    "\n",
    "print(llm_openai(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>FewShotPromptTemplate</b> We set the temperature to 1 which should have increased its creativity and wittiness, but we didn'g really get that. We'll train it so that it can achieve that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is like a box of chocolates. You never know what you're going to get.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "print(llm_openai(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It understood the assignment, literally. Now let's use langchain's formalized way of doing this FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically depressed and in a bad mood, producing\n",
      "distant  and sad responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: Who knows, I don't have energy to do most things, nothing interests me and I have trouble sleeping\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's probably time to regret the past\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "# create examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\":\"How are you?\",\n",
    "        \"answer\":\"Who knows, I don't have energy to do most things, nothing interests me and I have trouble sleeping\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"What time is it?\",\n",
    "        \"answer\":\"It's probably time to regret the past\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# define template to use examples\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create prompt to take in template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now create prefix and suffix that will wrap around the example prompt. The suffix will take in the User's query\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically depressed and in a bad mood, producing\n",
    "distant  and sad responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now put it all together in our fewshot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ") \n",
    "\n",
    "# Now visualize it\n",
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the fewshotprompt, it should output answers having seen the reinforced instructions. In our case, these reinforced instructions represent a bad mood, depressed assistant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Life? Let me think... It's just a series of disappointments and futile struggles, don't you think?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_openai(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, the AI understood from our fewshotprompt instructions that it needed to be depressed. Below is a regular answer for constrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an artificial intelligence, I don't have personal beliefs or experiences, but many people believe that the meaning of life is a philosophical and metaphysical question closely tied to various sets of beliefs. Different people, religions, and philosophies suggest different answers to this question, such as happiness, love, pleasure, personal growth, service to others, or fulfillment of potential. Others suggest that life may have no intrinsic meaning, and it's up to each individual to create their own purpose and meaning.\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why is this important?</b> We can certainly do all of this with f'string or dictionaries, but we might lose a benefit in langchain to maximize tokens and resources when prompting instructions. There is a functionality that <b>selects examples based on length need </b> which will allow to only use not all but examples matching lenght criteria. Since prompt windows have limits (ie. input + output), this will help maximize the usage and ensure appropriate choice when prompting. Let's check the exampel below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50 # No example should be beyond this length. This is the number of words, including newlines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"], #user input variable\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test how this works with shorter queries. This should maximize the number of examples seen by the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically depressed and in a bad mood, producing\n",
      "distant  and sad responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: Do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"Do birds fly?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should then minimize the examples seen by the AI with longer queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically depressed and in a bad mood, producing\n",
      "distant  and sad responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
    "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
    "what is the best way to do that?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This covers a way to prompt efficiently and give various example instructions to LLM AIs to optimize how it performs a task in a way we desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain/llms/openai.py:201: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/open_ai/lib/python3.9/site-packages/langchain/llms/openai.py:786: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm_openai= OpenAI(model_name=\"gpt-4-0613\", temperature=0, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "Paris Saint-Germain soccer superstar Neymar has reportedly agreed a two-year deal to join Saudi Arabian club Al-Hilal.\n",
    "\n",
    "France’s leading sports daily L’Equipe said on its website Sunday that the PSG and Brazil forward will receive a total of 160 million euros ($175 million) over two seasons. No details were given as to how much the transfer fee would be for Neymar, who has also been linked with a return to Barcelona.\n",
    "\n",
    "The Associated Press has asked defending French champion PSG to confirm the deal. The request was not immediately answered.\n",
    "\n",
    "Neymar missed PSG’s season-opening 0-0 draw against Lorient in the French league on Saturday after training alone on Friday, with the club saying it was because he was recovering from a viral infection. But Neymar is heading for a PSG exit and could leave during the summer transfer window.\n",
    "\n",
    "The 31-year-old Brazilian joined PSG from Barcelona for a world-record fee of 222 million euros (now $244 million) six years ago, the same year Kylian Mbappe joined from Monaco in a deal worth 180 million euros.\n",
    "\n",
    "PSG had already received a world-record $332 million bid from Al-Hilal for Mbappe, who is in the last year of his contract and has been mired in a transfer standoff. Mbappe, who reportedly refused to meet with representatives from the Saudi club when they were in Paris last month, has been strongly linked with a move to Real Madrid next season on a free transfer.\n",
    "\n",
    "Tensions between Mbappe and PSG eased a little on Sunday after he was allowed to return to training following “constructive and positive talks” between the two parties.\n",
    "\n",
    "Mbappe sat out Saturday’s draw with Lorient after being left out of training. Midfielder Marco Verratti was also not selected by new coach Luis Enrique, and has been linked with a move to the Saudi league after joining PSG in 2012.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1823"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_input\"],\n",
    "    template = \"Extract the key facts of this text. Don't include opinions. Give fact a number and keep them short sentences. :\\n\\n {text_input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Extract the key facts of this text. Don't include opinions. Give fact a number and keep them short sentences. :\\n\\n Doge\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what \"PromptTemplate\" is doing\n",
    "fact_extraction_prompt.format(text_input=\"Doge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_extraction_chain = LLMChain(llm=llm_openai, prompt=fact_extraction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Neymar has reportedly agreed a two-year deal to join Saudi Arabian club Al-Hilal.\\n2. The PSG and Brazil forward will receive a total of 160 million euros ($175 million) over two seasons.\\n3. No details were given as to how much the transfer fee would be for Neymar.\\n4. Neymar missed PSG’s season-opening 0-0 draw against Lorient in the French league after training alone.\\n5. Neymar joined PSG from Barcelona for a world-record fee of 222 million euros (now $244 million) six years ago.\\n6. PSG had received a world-record $332 million bid from Al-Hilal for Kylian Mbappe.\\n7. Mbappe is in the last year of his contract and has been mired in a transfer standoff.\\n8. Mbappe reportedly refused to meet with representatives from the Saudi club when they were in Paris last month.\\n9. Mbappe has been strongly linked with a move to Real Madrid next season on a free transfer.\\n10. Mbappe sat out Saturday’s draw with Lorient after being left out of training.\\n11. Midfielder Marco Verratti was also not selected by new coach Luis Enrique, and has been linked with a move to the Saudi league after joining PSG in 2012.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts = fact_extraction_chain.run(article)\n",
    "facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Neymar, a Paris Saint-Germain (PSG) soccer player, has reportedly agreed to a two-year deal with Saudi Arabian club Al-Hilal.\n",
      "2. Neymar will receive a total of 160 million euros ($175 million) over two seasons.\n",
      "3. No details were given about the transfer fee for Neymar.\n",
      "4. Neymar missed PSG’s season-opening 0-0 draw against Lorient due to recovery from a viral infection.\n",
      "5. Neymar joined PSG from Barcelona for a world-record fee of 222 million euros ($244 million) six years ago.\n",
      "6. PSG received a world-record $332 million bid from Al-Hilal for Kylian Mbappe.\n",
      "7. Mbappe is in the last year of his contract and has been involved in a transfer standoff.\n",
      "8. Mbappe reportedly refused to meet with representatives from Al-Hilal.\n",
      "9. Mbappe returned to training after \"constructive and positive talks\" with PSG.\n",
      "10. Marco Verratti, a midfielder, was not selected for Saturday's draw with Lorient and has been linked with a move to the Saudi league.\n"
     ]
    }
   ],
   "source": [
    "print(facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
