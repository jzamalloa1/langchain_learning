{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Research LLM AI\n",
    "\n",
    "Inspiration taken from [here](https://github.com/assafelovic/gpt-researcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to define two agents. (1) to define objective questions (queries) in parallel out of the user's question and (2) to web search for answers through url requests. Url searches done in paralle will be aggregate and summarized into a report. See below:\n",
    "\n",
    "<center><img src=\"https://camo.githubusercontent.com/a1db65299ca6ec3b203e42d47b225c46f36f54009246fe2cdae8fd74c68ab8d5/68747470733a2f2f636f7772697465722d696d616765732e73332e616d617a6f6e6177732e636f6d2f6172636869746563747572652e706e67\" width=30%></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Summarize the following question based on the context:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Summarize the following question based on the context:\\n\\nQuestion: {question}\\n\\nContext:\\n{context}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspiration from https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\n",
    "\n",
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://blog.langchain.dev/announcing-langsmith/\" # example url to play with\n",
    "\n",
    "page_content = scrape_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12520\n",
      "Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications Skip to content LangChain Blog Home By LangChain Release Notes GitHub Docs Case Studies Sign in Subscribe Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications 8 min read Jul 18, 2023 LangChain exists to make it as easy as possible to develop LLM-powered applications. We started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “ not enough tinkering happening .” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now ( Nat agrees )–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game. The blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production. Today, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs. LangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here . How did we get here? Given the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too): Understanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated) Understanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way) Understanding the exact sequence of calls to LLM (or other resources), and how they are chained together Tracking token usage Managing costs Tracking (and debugging) latency Not having a good dataset to evaluate their application over Not having good metrics with which to evaluate their application Understanding how users are interacting with the product All of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same. LangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways: Debugging LangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues. We’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general. This deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data. \"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake. Boston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure. “We are proud of being one of the early LangChain design partners and users of LangSmith,”  Said Dan Sack, Managing Director and Partner at BCG.  “The use of LangSmith has been key to bringing production-ready LLM applications to our clients.  LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.” In another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications. Testing One of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets. The first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition. Today, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur. In parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach. Evaluating LangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves. We are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably. Monitoring While debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like. Ambitious startups like Mendable , Multi-On and Quivr , who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues. “Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr . A unified platform While each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones. Fintual , a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena. “Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.” We can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more. Finally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there. We can’t wait to see what you build. Tags By LangChain Join our newsletter Updates from the LangChain team and community Enter your email Subscribe Processing your application... Success! Please check your inbox and click the link to confirm your subscription. Sorry, something went wrong. Please try again. You might also like [Week of 10/16] LangChain Release Notes Release Notes 3 min read [Week of 10/2] LangChain Release Notes By LangChain 3 min read [Week of 9/18] LangChain Release Notes By LangChain 4 min read [Week of 8/21] LangChain Release Notes Release Notes 3 min read Making Data Ingestion Production Ready: a LangChain-Powered Airbyte Destination 3 min read Conversational Retrieval Agents 4 min read Sign up © LangChain Blog 2023 - Powered by Ghost\n"
     ]
    }
   ],
   "source": [
    "print(len(page_content))\n",
    "print(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models from OpenAI can be found [here](https://platform.openai.com/docs/models/gpt-3-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=model_name) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question is asking for a definition or explanation of what LangSmith is.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith?\",\n",
    "        \"context\": page_content[:10000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a summary template and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"{text}\n",
    "\n",
    "Using the above text, answer in short the following question\n",
    "\n",
    "> {question}\n",
    "\n",
    "---\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats, etc.\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(summary_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "chain = summary_prompt | ChatOpenAI(model=model_name) | StrOutputParser()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith\",\n",
    "        \"text\":page_content\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll modify the chain and add more syntax to make it more involved but at the same time more <b>streamlined</b> using the <b>[RunnablePassthrogh](https://nanonets.com/blog/langchain/)</b> calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that scrape_text is the function we created and we are passing it through to be executed within the chain\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | summary_prompt |  ChatOpenAI(model=model_name) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a unified platform that helps developers debug, test, evaluate, and monitor their LLM (large language models) applications. It aims to address the challenge of taking an LLM-powered application from prototype to production by providing tools for understanding model inputs and outputs, tracking token usage and costs, debugging latency issues, evaluating application performance, and monitoring user interactions. LangSmith has been tested and used by various companies and organizations to build and improve their LLM applications.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith\",\n",
    "        \"url\":url\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the duckduckgo browser api to look up the url and pass it through the chain we built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_search = 3\n",
    "\n",
    "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "def web_search(query:str, num_results:int = results_per_search):\n",
    "    results = ddg_search.results(query, num_results)\n",
    "\n",
    "    return [r[\"link\"] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of what DDG can do - Notice the return objects: snippet, title and link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippet': 'Entertainment 17 December 2023 Global Link copied to clipboard Netflix today announced that the manga \"ONE PIECE\" is getting a new anime adaptation starting from the iconic East Blue saga, sending waves of excitement through the global anime community on the second day of Jump Festa 2024.',\n",
       "  'title': \"New Anime Series 'THE ONE PIECE' Starts Fresh Journey into the East ...\",\n",
       "  'link': 'https://about.netflix.com/en/news/new-anime-series-the-one-piece'},\n",
       " {'snippet': \"Based on Eiichiro Oda 's bestselling manga series, THE ONE PIECE follows Monkey D. Luffy, an aspiring pirate who sets sail with his Straw Hats crew in search of the mysterious One Piece treasure. Luffy isn't just like any pirate, however.\",\n",
       "  'title': \"'THE ONE PIECE' New Anime Series Announced - Netflix Tudum\",\n",
       "  'link': 'https://www.netflix.com/tudum/articles/the-one-piece-release-date'},\n",
       " {'snippet': 'One Piece is a manga and anime series that follows Monkey D. Luffy, a young boy with a dream to become the greatest pirate in the world. As a child, he eats a mystical plant called a Devil...',\n",
       "  'title': \"What is One Piece? The series' mega-popularity, explained - Polygon\",\n",
       "  'link': 'https://www.polygon.com/entertainment/23845804/one-piece-explained-anime-manga-netflix-live-action'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddg_search.results(\"what is the one piece?\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippet': \"Thanks to expanded qualifying with the first 48-team World Cup, Peru is still only four points off a qualification spot, but Juan Reynoso's team will be desperate for all three points against...\",\n",
       "  'title': 'Peru vs. Venezuela: How to watch 2026 World Cup qualifier - Pro Soccer Wire',\n",
       "  'link': 'https://prosoccerwire.usatoday.com/2023/11/20/how-to-watch-peru-vs-venezuela-conmebol-2026-world-cup-qualifier-tv-and-streaming/'},\n",
       " {'snippet': '11/21/2023 TAJ 11/21/2023 11/21/2023 124 99 PAL Venezuela 0 - 0 Bolivia Venezuela vs Peru LIVE Updates: Score, Stream Info, Lineups and How to Watch World Cup Qualifiers 2026 Match',\n",
       "  'title': 'Venezuela vs Peru LIVE Updates: Score, Stream Info, Lineups and How to ...',\n",
       "  'link': 'https://www.vavel.com/en-us/soccer/2023/11/22/1163696-venezuela-vs-peru-live-updates-score-stream-info-lineups-and-how-to-watch-world-cup-qualifiers-2026-match.html'},\n",
       " {'snippet': 'Conmebol odds courtesy of Tipico Sportsbook. Odds last updated Tuesday at 7:20 p.m. ET. Peru (+155) vs. Venezuela (+200) Want some action on the World Cup Qualifiers? Place your legal sports...',\n",
       "  'title': 'Peru vs. Venezuela live stream: TV channel, how to watch - For The Win',\n",
       "  'link': 'https://ftw.usatoday.com/2023/11/how-to-watch-world-cup-qualifiers-peru-vs-venezuela-time-tv-channel-live-stream'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddg_search.results(\"what was the score between peru and venezuela last night?\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly explore what the output from RunnabalePassthrough does with our ddg function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is the one piece?',\n",
       " 'urls': ['https://www.polygon.com/entertainment/23845804/one-piece-explained-anime-manga-netflix-live-action',\n",
       "  'https://www.cbr.com/what-is-one-piece-treasure/',\n",
       "  'https://collider.com/one-piece-arcs-in-order/']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"])\n",
    ")\n",
    "\n",
    "main_chain.invoke({\"question\": \"what is the one piece?\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It not only returns the output in dictionary format (urls), but also the input (question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now formulate the entire chain invoking it in each url. Notice how we are <b>>ensuring to pass a dictionary to each RunnablePassthrough</b> \n",
    "\n",
    "**NOTE** that line 6 below would have worked without lambda because **RunnablePassthrough.assign()** passess runable by default without the need of a lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URL: https://python.langchain.com/docs/get_started/introduction\\n\\nSummary: LangSmith is a developer platform that allows for debugging, testing, evaluating, and monitoring chains built on any Language Model (LLM) framework. It seamlessly integrates with LangChain and simplifies the application lifecycle, from development to production deployment.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## PLAYGROUND CELL #################\n",
    "scrape_and_summary_chain = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | RunnablePassthrough.assign(\n",
    "    url_output = lambda x : x[\"url\"],\n",
    "    summary_output = lambda x: RunnablePassthrough() | summary_prompt | ChatOpenAI(model=model_name) | StrOutputParser()\n",
    ") | (lambda x: f\"URL: {x['url_output']}\\n\\nSUMMARY: {x['summary_output']}\")\n",
    "\n",
    "## This doesn't work, lambda, only above option\n",
    "# scrape_and_summary_chain = RunnablePassthrough.assign(\n",
    "#     text = lambda x: scrape_text(x[\"url\"])[:10000]\n",
    "# ) | (lambda x: f\"URL: {x['url']}\\n\\nSUMMARY: {RunnablePassthrough() | summary_prompt | ChatOpenAI(model=model_name) | StrOutputParser()}\")\n",
    "\n",
    "link = \"https://python.langchain.com/docs/get_started/introduction\"\n",
    "\n",
    "test = scrape_and_summary_chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith?\",\n",
    "        \"url\":link\n",
    "    }\n",
    ")\n",
    "\n",
    "test\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Very important to observe that we used **RunnablePassthrough() within lambda** to retain runnable class and continue LCEL. Inspiration taken from *RunnableParallel* [here](https://python.langchain.com/docs/expression_language/how_to/map)\n",
    "\n",
    "This was needed to included both: *url* and *summary* in runnable output as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL: https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5\\n\\n SUMMARY: LangSmith is a product created by the team behind LangChain, the popular language model software tool. It aims to tackle the challenges of getting language model applications into production in a reliable and maintainable way. LangSmith focuses on providing features for debugging, testing, evaluating, monitoring, and tracking usage metrics of language models. It offers a simple and intuitive user interface to perform these tasks, reducing the barrier to entry for developers without a software background. While there may be potential competitors in the market, LangSmith aims to differentiate itself by integrating with various tools and services and providing value-added features. The author also mentions the need for extensibility and the potential for LangSmith to be built into other applications and services.',\n",
       " 'URL: https://blog.langchain.dev/announcing-langsmith/\\n\\n SUMMARY: LangSmith is a unified platform that helps developers debug, test, evaluate, and monitor their LLM (large language model) applications. It aims to close the gap between prototype and production by providing tools and features to address common challenges faced by LLM application developers. LangSmith offers visibility into model inputs and outputs, allows for experimentation with chains and prompt templates, helps identify issues like unexpected results or latency problems, enables easy change and rerun of examples, and provides performance tracking and monitoring capabilities. It has been tested by early design partners and found to be beneficial for teams developing complex applications. LangSmith also integrates with evaluation modules and supports monitoring of system-level performance, model/chain performance, and user interaction.',\n",
       " \"URL: https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k\\n\\n SUMMARY: LangSmith is a product developed by the team behind Langchain, which is a popular software tool for large language models (LLMs). LangSmith aims to address the challenges of building LLM applications for production by offering features related to debugging, testing, evaluating, monitoring, and usage metrics. It provides a simple and intuitive UI to perform these tasks, making it easier for developers without a software background. While there are potential competitors in the market, LangSmith's integrations with other tools and focus on extensibility can contribute to its success.\"]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From above - ORIGINAL\n",
    "# scrape_and_summary_chain = RunnablePassthrough.assign(\n",
    "#     text = lambda x: scrape_text(x[\"url\"])[:10000]\n",
    "# ) | summary_prompt |  ChatOpenAI(model=model_name) | StrOutputParser()\n",
    "\n",
    "# From above - MODIFIED \n",
    "scrape_and_summary_chain = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | RunnablePassthrough.assign(\n",
    "    url_output = lambda x : x[\"url\"],\n",
    "    summary_output = lambda x: RunnablePassthrough() | summary_prompt | ChatOpenAI(model=model_name) | StrOutputParser()\n",
    ") | (lambda x: f\"URL: {x['url_output']}\\n\\n SUMMARY: {x['summary_output']}\")\n",
    "\n",
    "# New - Recall from above cell that a dict is returned from RunnablePassthrough.assign() {\"question\", \"urls\"}\n",
    "web_url_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"])\n",
    ") | (lambda x: [{\"question\":x[\"question\"], \"url\":u} for u in x[\"urls\"]]) # What we want is a list of {\"url\":...}\n",
    "\n",
    "# Put together - Map applies chain to every input element. It takes a list of dictionaries\n",
    "question_to_summaries_chain = web_url_chain | scrape_and_summary_chain.map()\n",
    "\n",
    "# Invoke chain\n",
    "question_to_summaries_chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to do now is add on top a chain to generate a list of sub-questions to feed the main chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an additional prompt to generate such questions. Inspiration taken from [here](https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py). Refer [here](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) from PromptTemplate guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an AI Research assistant. Your name is {agent_name}\"),\n",
    "        (\"user\",\n",
    "         \"Write 3 google search queries to search online that form an objective opinion from \"\n",
    "         \"the following {question}\\n\"\n",
    "         \"You must respond with a list of strings in the following format:\"\n",
    "         '[\"query 1\", \"query 2\", \"query 3\"].'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it to see what the output is so that you know how to parse it within the same chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='[\"One Piece manga plot summary\", \"Opinions on One Piece anime adaptation\", \"Critiques of One Piece storyline\"]')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the one piece?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to modify to output a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"One Piece manga plot summary\", \"Analysis and critique of One Piece storyline\", \"Opinions and reviews on One Piece anime adaptation\"]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name) | StrOutputParser()\n",
    "\n",
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the one piece?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not actual list, but a string representing a list. We need to first convert the string type output, that has an embedded list, into an actual python list object. This can be achieved by [json.loads()](https://www.geeksforgeeks.org/python-difference-between-json-load-and-json-loads/)\n",
    "\n",
    "Notice that in order to so, we have to make sure that the Prompt instructions have the <b>\"</b> inside the embedded list, and the <b>'</b> at the ends of the string. Otherwise you'll get an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One Piece anime review',\n",
       " 'One Piece manga vs anime comparison',\n",
       " 'One Piece fan discussion']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads('[\"One Piece anime review\", \"One Piece manga vs anime comparison\", \"One Piece fan discussion\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative will produce an error (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JOSE\\OneDrive\\Documents\\PROJECTS\\LLM\\ra.ipynb Cell 40\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m json\u001b[39m.\u001b[39;49mloads(\u001b[39m\"\u001b[39;49m\u001b[39m[\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "json.loads(\"['a', 'b']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can continue and convert the output to a list of strings below (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UEFA Champions League format',\n",
       " 'UEFA Champions League vs Europa League',\n",
       " 'TV rights for UEFA Champions League and Europa League']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name) | StrOutputParser() | json.loads\n",
    "\n",
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between the UEFA Champions and Europe leagues?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we <b>need a list of dictionaries to pass it along the chain</b> so that it can be \"mapped.\" So we need to convert it to that. (i.e. [{\"question\" : \".....\"}])\n",
    "\n",
    "The <b>web_url_chain</b> within the <b>question_to_summaries_chain</b> requires a <b>\"question\"</b> key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Langsmith vs Langchain differences'},\n",
       " {'question': 'Comparison between Langsmith and Langchain'},\n",
       " {'question': 'Langsmith and Langchain features comparison'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name, temperature=0.3) | StrOutputParser() | json.loads | (\n",
    "    lambda x : [{\"question\":i} for i in x]\n",
    ")\n",
    "\n",
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between langsmith and langchain?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now <b>pass it to the question_to_summaries_chain</b>. We need to map it, which in essence will invoke the question_to_summaries_chain for each dict input coming out of the search_questions_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['URL: https://cheatsheet.md/langchain-tutorials/langsmith.en\\n\\nSUMMARY: LangSmith is a platform designed to build, test, and deploy intelligent agents and chains based on any Language Learning Model (LLM) framework. It offers features such as debugging, testing, and tracing capabilities. LangSmith integrates seamlessly with LangChain, an open-source LLM framework developed by the same company. LangSmith is user-friendly, versatile, and supported by a strong community. It provides interactive tutorials, a quick start guide, and a Cookbook filled with real-world examples to assist developers in their LLM projects. The Cookbook is a community-driven resource that covers topics like tracing code, using REST API features, evaluating Q&A systems, and exporting data for fine-tuning.',\n",
       "  \"URL: https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5\\n\\nSUMMARY: The text does not provide a direct comparison between LangSmith and LangChain. It only mentions that LangSmith is the latest product from the creators of LangChain. It explains that LangChain is a popular software tool for large language models (LLMs) that focuses on reducing the barrier to entry for building prototypes. LangSmith, on the other hand, aims to tackle the production challenges of LLM applications by providing features around debugging, testing, evaluating, monitoring, and usage metrics. It also emphasizes the value of LangSmith's simple and intuitive UI for these tasks. The text mentions potential competitors for LangSmith, such as Vercel and Embeddings providers, and highlights the importance of integrations with other tools. The author expresses a desire for LangSmith to be extensible and integrated into other applications and services.\",\n",
       "  'URL: https://blog.langchain.dev/announcing-langsmith/\\n\\nSUMMARY: The text does not provide a direct comparison between LangSmith and LangChain.'],\n",
       " ['URL: https://blog.logrocket.com/langsmith-test-llms-ai-applications/\\n\\nSUMMARY: The features of LangSmith, a testing framework for evaluating language models and AI applications, include:\\n\\n1. Quick debugging: LangSmith enhances the efficiency and performance of new chains and other sets of tools effortlessly.\\n\\n2. Customizable test scenarios: Users can create and run various testing scenarios with customization options.\\n\\n3. Metrics and analytics: LangSmith measures the strengths and weaknesses of language models using various metrics and analytics.\\n\\n4. Interactive visualization: LangSmith provides a visualization capability that enhances the understanding of response patterns and trends, aiding in performance evaluation.\\n\\nOverall, LangSmith offers a powerful solution for testing and evaluating language models, providing valuable insights into their performance and capabilities.',\n",
       "  'URL: https://cheatsheet.md/langchain-tutorials/langsmith.en\\n\\nSUMMARY: The key features of LangSmith include debugging and testing capabilities, API and environment setup, and tracing capabilities. It offers interactive tutorials and a quick start guide for easy usage. LangSmith is designed to be user-friendly and versatile, with strong community support.',\n",
       "  'URL: https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5\\n\\nSUMMARY: The features of LangSmith include debugging, testing, evaluating, monitoring, and usage metrics for large language models. It provides a simple and intuitive UI to perform these tasks, reducing the barrier to entry for developers without a software background. LangSmith also offers visualizations to help understand the output distribution and the process the language model system is going through. While there may be potential competitors in the market, LangSmith aims to differentiate itself by connecting with various tools and offering integrations with OpenAI evals and fine-tuning providers. The main ask from developers is for LangSmith to be extensible and integrated into other applications and services. Overall, LangSmith aims to solve the challenges of building and deploying language models in production.'],\n",
       " ['URL: https://notes.aimodels.fyi/a-complete-guide-to-langchain-building-powerful-applications-with-large-language-models/\\n\\nSUMMARY: The key features of LangChain include its ability to manage prompts and optimize them, support for chains of calls to language models or other utilities, data augmented generation by interacting with external data sources, the use of agents to make decisions and take actions, memory implementation to maintain state, and the evaluation of generative models using prompts and chains. LangChain supports various use cases like question answering, building chatbots, creating interactive agents, data summarization, document retrieval and clustering, and interacting with APIs.',\n",
       "  \"URL: https://nanonets.com/blog/langchain/\\n\\nSUMMARY: LangChain is an innovative framework that allows developers to create applications leveraging the capabilities of language models. It provides a unified approach to developing intelligent applications by simplifying the journey from concept to execution.\\n\\nLangChain is more than just a framework; it's a full-fledged ecosystem comprising several integral parts. The core components of LangChain include:\\n\\n1. LangChain Libraries: These libraries, available in Python and JavaScript, are the backbone of LangChain. They offer interfaces and integrations for various components, providing a basic runtime for combining these components into cohesive chains and agents.\\n\\n2. LangChain Templates: These are deployable reference architectures tailored for a wide array of tasks. They offer a solid starting point for building applications, whether it's a chatbot or a complex analytical tool.\\n\\n3. LangServe: This is a versatile library for deploying LangChain chains as REST APIs. It allows you to turn your LangChain projects into accessible and scalable web services.\\n\\n4. LangSmith: This serves as a developer platform for debugging, testing, evaluating, and monitoring chains built on any LLM framework. It seamlessly integrates with LangChain, making it an indispensable tool for refining and perfecting applications.\\n\\nWith these components, developers can easily develop, productionize, and deploy applications with ease. They can write applications using the LangChain libraries, reference templates for guidance, inspect and test chains using LangSmith, and deploy chains as APIs using LangServe.\\n\\nTo get started with LangChain, you need to install it using pip or conda. You can also install it directly from the source or use the LangChain CLI. LangChain often requires integrations with model providers, data stores, APIs, etc. For example, you can use OpenAI's model APIs and install the OpenAI Python package.\\n\\nOnce LangChain is installed, you can start developing applications using the model I/O module, which provides the building blocks to interface effectively with any language model. This module includes components like LLMs (Large Language Models), Chat Models, Prompts, and Output Parsers.\\n\\nLLMs are pure text completion models, while Chat Models are models that use a language model as a base but differ in input and output formats. Prompts are used to templatize, dynamically select, and manage model inputs, while Output Parsers extract and format information from model outputs.\\n\\nLangChain also supports the use of LangChain Expression Language (LCEL), which allows you to compose modules together using a declarative approach.\\n\\nIn the subsequent sections of the guide, you will delve deeper into each LangChain module, learn how to use LCEL, explore common use cases, and deploy an end-to-end application with LangServe. You will also discover LangSmith for debugging, testing, and monitoring your applications.\\n\\nLangChain is a powerful tool for automating manual tasks and workflows using AI-driven workflow automation. It empowers developers to create intelligent applications that can understand the context and perform complex reasoning tasks.\",\n",
       "  'URL: https://blog.enterprisedna.co/what-is-langchain-a-beginners-guide-with-examples/\\n\\nSUMMARY: The features of LangChain include its ability to connect AI models with different data sources, its support for large language models (LLMs) like OpenAI and Hugging Face, and its range of natural language processing (NLP) solutions. LangChain allows for the development of dynamic and data-responsive applications that harness the power of recent breakthroughs in NLP technology. It can be used for various use cases, such as sentiment analysis, chatbots, text summarization, and question answering. LangChain is an open-source framework that provides documentation, libraries, and modules to simplify the development process. It also offers advanced features and customization options, allowing developers to create unique applications tailored to their specific needs.']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_research_chain = search_questions_chain | question_to_summaries_chain.map()\n",
    "\n",
    "main_research_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between langsmith and langchain?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of lists. Now that we have it, all we have to do is pass it to a final prompt to summarize everything\n",
    "\n",
    "We need a function to flatten the lists of list and a Prompt that will summarize that text. Inspiration for prompt taken from [here](https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_system_prompt = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, \"\\\n",
    "\"critically acclaimed, objective and structured reports on given text.\"\n",
    "\n",
    "research_report_template = \"\"\"Information:\n",
    "---------\n",
    "{research_summary}\n",
    "---------\n",
    "\n",
    "Using the above information, answer the following question or topic: {question} in a detailed report. \\\n",
    "The report should focus on the answer to the question, should be well structured, informative, \\\n",
    "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
    "\n",
    "You should strive to write the report as long as you can using all relevant and necessary information provided. \\\n",
    "You must write the report with markdown syntax. \\\n",
    "Use an unbiased and journalistic tone. \\\n",
    "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions. \\\n",
    "You MUST write all used source urls at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each. \\\n",
    "You MUST write the report in apa format. \\\n",
    "Cite search results using inline notations. Only cite the most \\\n",
    "relevant results that answer the query accurately. Place these citations at the end \\\n",
    "of the sentence or paragraph that reference them. \\\n",
    "Please do your best, this is very important to my career.\n",
    "\"\"\"\n",
    "\n",
    "research_summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", writer_system_prompt),\n",
    "    (\"user\", research_report_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a helper function to collapse the list of lsits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to collapse list of lists\n",
    "def collapse_list_of_lists(lists):\n",
    "    \n",
    "    all_text = []\n",
    "    for i in lists:\n",
    "        all_text.append(\"\\n\\n\".join(i))\n",
    "\n",
    "    return \"\\n\\n\".join(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we write chain. We need to pass the list of lists through our collapsing function before feeding it to the summarization prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'list' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ra.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ra.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m [[\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m], [\u001b[39m\"\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39md\u001b[39;49m\u001b[39m\"\u001b[39;49m]] \u001b[39m|\u001b[39;49m collapse_list_of_lists\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'list' and 'function'"
     ]
    }
   ],
   "source": [
    "[[\"a\",\"b\"], [\"c\",\"d\"]] | collapse_list_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need runnable to do this and keep it in LCEL (pipe) format. This allows us to pass it as a variable in an assigned value (\"research_summary\") as a dict to the prompt.\n",
    "\n",
    "**Note** Our prompt does need a *question* though, does this mean that the main research question remains in the LCEL unless we change it? Because we are not passing a *question* in the list of lists. Let's explore this first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chain = RunnablePassthrough.assign(\n",
    "    research_summary = main_research_chain | collapse_list_of_lists\n",
    ")\n",
    "\n",
    "all_chain_result = all_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between langsmith and langchain?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important NOTE** Yes, the LCEL keeps memory of the *invoked elements* throughout the chain (I'm assuming unless we change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_name': 'El Bryan',\n",
       " 'question': 'what is the diffence between langsmith and langchain?',\n",
       " 'research_summary': \"The given text does not provide any information about Langsmith.\\n\\nThe text does not provide a direct comparison between LangSmith and LangChain. However, it mentions that LangSmith is the latest product from the creators of LangChain and aims to tackle production challenges for large language models (LLMs). LangChain, on the other hand, is described as the most popular LLM software tool and focuses on reducing the barrier to entry for building prototypes. LangSmith aims to provide features related to debugging, testing, evaluating, monitoring, and usage metrics for LLM applications, with a user-friendly UI. It is mentioned that other platforms, such as Vercel, may also build similar tooling in the future, and LangSmith's success will depend on its ability to expand and compete with multiple providers and other tooling ecosystems.\\n\\nThe given text does not provide any information about the comparison between Langsmith and Langchain.\\n\\nThe features of LangSmith as a testing tool include:\\n\\n1. Quick debugging: LangSmith enhances the efficiency and performance of new chains and other sets of tools effortlessly.\\n\\n2. Customizable test scenarios: Users can create and run various testing scenarios with customization options.\\n\\n3. Metrics and analytics: LangSmith measures the model's strengths and weaknesses using various metrics and analytics.\\n\\n4. Interactive visualization: LangSmith provides a visualization capability that enhances the understanding of response patterns and trends, aiding in performance evaluation.\\n\\nLangSmith provides new features around 5 core pillars: debugging, testing, evaluating, monitoring, and usage metrics. It offers a simple and intuitive UI to handle these tasks, reducing the barrier to entry for developers without a software background. Additionally, LangSmith aims to provide visualizations and insights into the LLM system's processes and command chains.\\n\\nLangSmith is a unified platform that allows developers to build production-grade LLM (Language Model) applications and test, evaluate, and monitor these applications. It integrates with the LangChain library and provides features such as tracing the runs associated with an active instance and testing and evaluating prompts or answers generated by the application. LangSmith is currently in beta and requires a sign-up and API key to access.\\n\\nSorry, but I'm unable to complete your request.\\n\\nThe features of LangChain include:\\n\\n1. Support for large language models (LLMs): LangChain allows developers to integrate and interact with powerful language models, such as those from OpenAI or Hugging Face, to create language-driven applications.\\n\\n2. Data-aware and agentic capabilities: LangChain goes beyond standard API calls by being data-aware and agentic, enabling connections with various data sources for richer, personalized experiences. It allows language models to interact dynamically with their environment.\\n\\n3. Framework and libraries: LangChain provides a framework and libraries that simplify the development process for language model-powered applications. It offers Python libraries to streamline rich, data-driven interactions with AI models by chaining different components together.\\n\\n4. Extensive documentation and modular construction: LangChain offers extensive documentation to assist developers in building applications. It facilitates high levels of customization by allowing developers to choose and combine modules according to their needs.\\n\\n5. Application examples: LangChain can be used to develop applications for various use cases, such as text summarization, question answering, and chatbots. It supports the creation of unique applications built around a large language model.\\n\\nIn summary, LangChain is a powerful open-source framework that simplifies the development of applications using large language models. It offers features like support for LLMs, data-awareness, agentic capabilities, a framework with libraries, extensive documentation, and modular construction. These features enable developers to create customized language-driven applications for various use cases.\\n\\nThe key features of LangChain include support for managing prompts and optimizing them, creating universal interfaces for large language models (LLMs), providing a standard interface for chains and agents, enabling interaction with external data sources, supporting memory implementation for maintaining state, and offering evaluation prompts and chains for assessing generative models. LangChain also supports various use cases such as question-answering applications, building chatbots, creating interactive agents, data summarization, document retrieval and clustering, and interacting with APIs.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chain_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are certain we'll execute the entire chain, all the way to the summarization of the collapsed lists in APA format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chain = RunnablePassthrough.assign(\n",
    "    research_summary = main_research_chain | collapse_list_of_lists \n",
    ") | research_summarization_prompt | ChatOpenAI(model=\"gpt-3.5-turbo-16k\") | StrOutputParser()\n",
    "\n",
    "all_chain_result = all_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between langsmith and langchain?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Report: The Difference Between LangSmith and LangChain\n",
       "\n",
       "## Introduction\n",
       "LangSmith and LangChain are two platforms developed by LangChain that cater to the needs of developers working with Language Learning Models (LLMs). While they share a common origin, they serve different purposes in the development and productionization of LLM applications. This report aims to provide a detailed analysis of the differences between LangSmith and LangChain, based on the information provided.\n",
       "\n",
       "## LangSmith: Elevating LLM Applications to Production Grade\n",
       "LangSmith is a platform developed by LangChain that focuses on providing features for debugging, testing, evaluating, and monitoring LLM applications. It is designed to elevate LLM applications to production-grade quality by addressing the challenges associated with getting these applications into production in a reliable and maintainable way.\n",
       "\n",
       "### Debugging and Testing Capabilities\n",
       "One of the key features of LangSmith is its debugging and testing capabilities. It offers tools for analyzing and extracting insights from LLM responses, allowing developers to refine and enhance their AI applications. This dynamic testing framework enhances the efficiency and performance of new chains and sets of tools effortlessly [^1^]. It allows users to create and run customizable test scenarios, enabling thorough testing of LLM applications [^6^].\n",
       "\n",
       "### API and Environment Setup\n",
       "LangSmith provides a seamless integration with the open-source LangChain framework, making it easier for developers to set up the necessary API and environment for their LLM applications. This integration ensures that developers can quickly get started with their projects without facing significant barriers to entry [^1^].\n",
       "\n",
       "### Tracing Capabilities for Code Improvement\n",
       "LangSmith incorporates tracing capabilities that enable developers to investigate and debug the execution steps of their LLM applications. This feature helps identify and rectify issues in the code, leading to improved performance and reliability [^3^].\n",
       "\n",
       "### Community Support and Cookbook\n",
       "LangSmith boasts strong community support, which is crucial for the growth and development of any platform. The LangSmith Cookbook serves as a community-driven resource that provides practical examples and use-cases for mastering LangSmith. It allows users to contribute their insights, shaping the community and fostering collaboration [^1^].\n",
       "\n",
       "## LangChain: Enabling Advanced Language Model Applications\n",
       "LangChain, on the other hand, is the underlying framework on which LangSmith is built. It provides a comprehensive toolkit for developers to build advanced language model applications that are adaptable, efficient, and capable of handling complex use cases [^7^]. LangChain consists of several integral parts, including LangChain Libraries, LangChain Templates, LangServe, and LangSmith.\n",
       "\n",
       "### LangChain Libraries and Templates\n",
       "LangChain Libraries serve as the backbone of the LangChain framework, offering interfaces and integrations for various components. They provide a basic runtime for combining these components into cohesive chains and agents. LangChain Templates, on the other hand, are deployable reference architectures tailored for different tasks, providing a solid starting point for application development [^7^].\n",
       "\n",
       "### LangServe\n",
       "LangServe is a versatile library that allows developers to deploy LangChain chains as REST APIs. It simplifies the process of turning LangChain projects into accessible and scalable web services, enabling developers to create context-aware applications capable of sophisticated reasoning [^7^].\n",
       "\n",
       "### LangSmith for Debugging, Testing, and Monitoring\n",
       "While LangSmith is a separate platform, it is closely integrated with LangChain. LangSmith serves as a developer platform for debugging, testing, evaluating, and monitoring chains built on any LLM framework [^7^]. It offers a range of features that help developers ensure the quality and reliability of their LLM applications.\n",
       "\n",
       "## Comparison and Conclusion\n",
       "Based on the information provided, LangSmith and LangChain serve complementary roles in the development and productionization of LLM applications. LangSmith focuses on providing features for debugging, testing, evaluating, and monitoring LLM applications, while LangChain provides a comprehensive toolkit for building advanced language model applications.\n",
       "\n",
       "LangSmith's key features include debugging and testing capabilities, API and environment setup, and tracing capabilities for code improvement. It is known for its ease of use, versatility, and strong community support. The LangSmith Cookbook, a community-driven resource, provides practical examples and use-cases for mastering LangSmith [^1^].\n",
       "\n",
       "LangChain, on the other hand, offers a range of features such as LLM and prompt management, chain creation, data augmented generation, agent support, memory management, and evaluation tools. It provides a comprehensive toolkit for developers to build advanced language model applications that are adaptable, efficient, and capable of handling complex use cases [^7^].\n",
       "\n",
       "In conclusion, LangSmith and LangChain are two essential platforms developed by LangChain to cater to the needs of developers working with Language Learning Models (LLMs). While LangSmith focuses on providing features for debugging, testing, evaluating, and monitoring LLM applications, LangChain provides a comprehensive toolkit for building advanced language model applications. Both platforms play crucial roles in simplifying the development, productionization, and deployment of intelligent, language model-powered applications.\n",
       "\n",
       "## References\n",
       "1. LangChain. (n.d.). LangSmith Tutorials. Retrieved from https://cheatsheet.md/langchain-tutorials/langsmith.en\n",
       "2. Kilpatrick, L. (2021, August 10). What is LangSmith, and why should I care as a developer? Retrieved from https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5\n",
       "3. Kilpatrick, L. (2021, September 28). LangSmith: Test LLMs & AI Applications. Retrieved from https://blog.logrocket.com/langsmith-test-llms-ai-applications/\n",
       "4. AIModels. (n.d.). A Complete Guide to LangChain: Building Powerful Applications with Large Language Models. Retrieved from https://notes.aimodels.fyi/a-complete-guide-to-langchain-building-powerful-applications-with-large-language-models/\n",
       "5. Nanonets. (n.d.). LangChain: A Comprehensive Guide. Retrieved from https://nanonets.com/blog/langchain/\n",
       "6. Kilpatrick, L. (2021, September 28). What is LangSmith, and why should I care as a developer? Retrieved from https://blog.logrocket.com/langsmith-test-llms-ai-applications/\n",
       "7. Enterprise DNA. (n.d.). What is LangChain? A Beginner's Guide with Examples. Retrieved from https://blog.enterprisedna.co/what-is-langchain-a-beginners-guide-with-examples/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(all_chain_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does a decent job at summarizing the content, and listing the references. These have to be manually checked however to increase confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All work up to here will be stored in python file within** [repo](https://github.com/jzamalloa1/langchain_learning/blob/main/ra_pilot.py) and will be **langserved**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we'll test another retriever, Arxiv\n",
    "\n",
    "Instead of the websearch retriever used above. Final .py langserved file will be hosted [here](https://github.com/jzamalloa1/langchain_learning/blob/main/ra_pilot_arxiv.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ArxivRetriever\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ArxivRetriever(top_k_results=5) #, get_full_documents=True, doc_content_chars_max=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = retriever.get_relevant_documents(\"housing markets in the US\")\n",
    "docs = retriever.get_summaries_as_docs(query=\"what are some advancements that have happened in llm prompting strategies?\")\n",
    "# docs = retriever.get_relevant_documents(query=\"what is the difference between openai and langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Large Language Models (LLMs) with strong abilities in natural language\\nprocessing tasks have emerged and have been applied in various kinds of areas\\nsuch as science, finance and software engineering. However, the capability of\\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\\nLLMs in a wide range of tasks across the chemistry domain. We identify three\\nkey chemistry-related capabilities including understanding, reasoning and\\nexplaining to explore in LLMs and establish a benchmark containing eight\\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\\na broad exploration of the capacities of LLMs within the context of practical\\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\\nsettings with carefully selected demonstration examples and specially crafted\\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\\nexhibit different competitive levels in eight chemistry tasks. In addition to\\nthe key findings from the comprehensive benchmark analysis, our work provides\\ninsights into the limitation of current LLMs and the impact of in-context\\nlearning settings on LLMs' performance across various chemistry tasks. The code\\nand datasets used in this study are available at\\nhttps://github.com/ChemFoundationModels/ChemLLMBench.\", metadata={'Published': datetime.date(2023, 9, 10), 'Title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'Authors': 'Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang'}),\n",
       " Document(page_content='Autoregressive Large Language Models have transformed the landscape of\\nNatural Language Processing. Pre-train and prompt paradigm has replaced the\\nconventional approach of pre-training and fine-tuning for many downstream NLP\\ntasks. This shift has been possible largely due to LLMs and innovative\\nprompting techniques. LLMs have shown great promise for a variety of downstream\\ntasks owing to their vast parameters and huge datasets that they are\\npre-trained on. However, in order to fully realize their potential, their\\noutputs must be guided towards the desired outcomes. Prompting, in which a\\nspecific input or instruction is provided to guide the LLMs toward the intended\\noutput, has become a tool for achieving this goal. In this paper, we discuss\\nthe various prompting techniques that have been applied to fully harness the\\npower of LLMs. We present a taxonomy of existing literature on prompting\\ntechniques and provide a concise survey based on this taxonomy. Further, we\\nidentify some open problems in the realm of prompting in autoregressive LLMs\\nwhich could serve as a direction for future research.', metadata={'Published': datetime.date(2023, 11, 28), 'Title': 'Prompting in Autoregressive Large Language Models', 'Authors': 'Prabin Bhandari'}),\n",
       " Document(page_content='Large Language Models (LLMs) have shown great success in code generation.\\nLLMs take as the input a prompt and output the code. A key question is how to\\nmake prompts (i.e., Prompting Techniques). Existing prompting techniques are\\ndesigned for natural language generation and have low accuracy in code\\ngeneration.\\n  In this paper, we propose a new prompting technique named AceCoder. Our\\nmotivation is that code generation meets two unique challenges (i.e.,\\nrequirement understanding and code implementation). AceCoder contains two novel\\nmechanisms (i.e., guided code generation and example retrieval) to solve these\\nchallenges. (1) Guided code generation asks LLMs first to analyze requirements\\nand output an intermediate preliminary (e.g., test cases). The preliminary is\\nused to clarify requirements and tell LLMs \"what to write\". (2) Example\\nretrieval selects similar programs as examples in prompts, which provide lots\\nof relevant content (e.g., algorithms, APIs) and teach LLMs \"how to write\". We\\napply AceCoder to three LLMs (e.g., Codex) and evaluate it on three public\\nbenchmarks using the Pass@k. Results show that AceCoder can significantly\\nimprove the performance of LLMs on code generation. (1) In terms of Pass@1,\\nAceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,\\n70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs with\\ndifferent sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,\\nand JavaScript). (3) Human evaluation shows human developers prefer programs\\nfrom AceCoder.', metadata={'Published': datetime.date(2023, 9, 7), 'Title': 'AceCoder: Utilizing Existing Code to Enhance Code Generation', 'Authors': 'Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, Zhi Jin'}),\n",
       " Document(page_content='This paper describes the DSBA submissions to the Prompting Large Language\\nModels as Explainable Metrics shared task, where systems were submitted to two\\ntracks: small and large summarization tracks. With advanced Large Language\\nModels (LLMs) such as GPT-4, evaluating the quality of Natural Language\\nGeneration (NLG) has become increasingly paramount. Traditional\\nsimilarity-based metrics such as BLEU and ROUGE have shown to misalign with\\nhuman evaluation and are ill-suited for open-ended generation tasks. To address\\nthis issue, we explore the potential capability of LLM-based metrics,\\nespecially leveraging open-source LLMs. In this study, wide range of prompts\\nand prompting techniques are systematically analyzed with three approaches:\\nprompting strategy, score aggregation, and explainability. Our research focuses\\non formulating effective prompt templates, determining the granularity of NLG\\nquality scores and assessing the impact of in-context examples on LLM-based\\nevaluation. Furthermore, three aggregation strategies are compared to identify\\nthe most reliable method for aggregating NLG quality scores. To examine\\nexplainability, we devise a strategy that generates rationales for the scores\\nand analyzes the characteristics of the explanation produced by the open-source\\nLLMs. Extensive experiments provide insights regarding evaluation capabilities\\nof open-source LLMs and suggest effective prompting strategies.', metadata={'Published': datetime.date(2023, 11, 7), 'Title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'Authors': 'Joonghoon Kim, Saeran Park, Kiyoon Jeong, Sangmin Lee, Seung Hun Han, Jiyoon Lee, Pilsung Kang'}),\n",
       " Document(page_content='We explore whether Large Language Models (LLMs) are capable of logical\\nreasoning with distorted facts, which we call Deduction under Perturbed\\nEvidence (DUPE). DUPE presents a unique challenge to LLMs since they typically\\nrely on their parameters, which encode mostly accurate information, to reason\\nand make inferences. However, in DUPE, LLMs must reason over manipulated or\\nfalsified evidence present in their prompts, which can result in false\\nconclusions that are valid only under the manipulated evidence. Our goal with\\nDUPE is to determine whether LLMs can arrive at these false conclusions and\\nidentify whether the dominant factor influencing the deduction process is the\\nencoded data in the parameters or the manipulated evidence in the prompts. To\\nevaluate the DUPE capabilities of LLMs, we create a DUPEd version of the\\nStrategyQA dataset, where facts are manipulated to reverse the answer to the\\nquestion. Our findings show that even the most advanced GPT models struggle to\\nreason on manipulated facts - showcasing poor DUPE skills - with accuracy\\ndropping by 45% compared to the original dataset. We also investigate prompt\\nsettings inspired from student simulation models, which mitigate the accuracy\\ndrop to some extent. Our findings have practical implications for understanding\\nthe performance of LLMs in real-world applications such as student simulation\\nmodels that involve reasoning over inaccurate information.', metadata={'Published': datetime.date(2023, 5, 23), 'Title': 'Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models', 'Authors': 'Shashank Sonkar, Richard G. Baraniuk'})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ra.ipynb Cell 70\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jzamalloa/Documents/PROJECTS/LLM/langchain_learning/ra.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m docs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mmetadata\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Gym is a toolkit for reinforcement learning (RL) research. It includes\n",
      "a large number of well-known problems that expose a common interface allowing\n",
      "to directly compare the performance results of different RL algorithms. Since\n",
      "many years, the ns-3 network simulation tool is the de-facto standard for\n",
      "academic and industry research into networking protocols and communications\n",
      "technology. Numerous scientific papers were written reporting results obtained\n",
      "using ns-3, and hundreds of models and modules were written and contributed to\n",
      "the ns-3 code base. Today as a major trend in network research we see the use\n",
      "of machine learning tools like RL. What is missing is the integration of a RL\n",
      "framework like OpenAI Gym into the network simulator ns-3. This paper presents\n",
      "the ns3-gym framework. First, we discuss design decisions that went into the\n",
      "software. Second, two illustrative examples implemented using ns3-gym are\n",
      "presented. Our software package is provided to the community as open source\n",
      "under a GPL license and hence can be easily extended.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
