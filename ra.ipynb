{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Research LLM AI\n",
    "\n",
    "Inspiration taken from [here](https://github.com/assafelovic/gpt-researcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to define two agents. (1) to define objective questions (queries) in parallel out of the user's question and (2) to web search for answers through url requests. Url searches done in paralle will be aggregate and summarized into a report. See below:\n",
    "\n",
    "<center><img src=\"https://camo.githubusercontent.com/a1db65299ca6ec3b203e42d47b225c46f36f54009246fe2cdae8fd74c68ab8d5/68747470733a2f2f636f7772697465722d696d616765732e73332e616d617a6f6e6177732e636f6d2f6172636869746563747572652e706e67\" width=30%></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Summarize the following question based on the context:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Summarize the following question based on the context:\\n\\nQuestion: {question}\\n\\nContext:\\n{context}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspiration from https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c\n",
    "\n",
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://blog.langchain.dev/announcing-langsmith/\" # example url to play with\n",
    "\n",
    "page_content = scrape_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12520\n",
      "Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications Skip to content LangChain Blog Home By LangChain Release Notes GitHub Docs Case Studies Sign in Subscribe Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications 8 min read Jul 18, 2023 LangChain exists to make it as easy as possible to develop LLM-powered applications. We started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “ not enough tinkering happening .” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now ( Nat agrees )–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game. The blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production. Today, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs. LangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here . How did we get here? Given the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too): Understanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated) Understanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way) Understanding the exact sequence of calls to LLM (or other resources), and how they are chained together Tracking token usage Managing costs Tracking (and debugging) latency Not having a good dataset to evaluate their application over Not having good metrics with which to evaluate their application Understanding how users are interacting with the product All of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same. LangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways: Debugging LangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues. We’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general. This deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data. \"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake. Boston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure. “We are proud of being one of the early LangChain design partners and users of LangSmith,”  Said Dan Sack, Managing Director and Partner at BCG.  “The use of LangSmith has been key to bringing production-ready LLM applications to our clients.  LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.” In another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications. Testing One of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets. The first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition. Today, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur. In parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach. Evaluating LangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves. We are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably. Monitoring While debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like. Ambitious startups like Mendable , Multi-On and Quivr , who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues. “Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr . A unified platform While each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing of the past. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones. Fintual , a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena. “Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.” We can’t wait to bring these benefits to more teams. And we’ve got a long list of features on the roadmap like analytics, playgrounds, collaboration, in-context learning, prompt creation, and more. Finally, we recognize that we cannot build ALL the functionality you will need to make it easy to make your applications production ready today. We’ve made it possible to export datasets in the format OpenAI evals expects so you can contribute them there. This data can be also used directly to fine tune your models on the Fireworks platform (and we aim to make it easy to plug into other fine-tuning systems as well). Finally, we’ve made logs exportable in a generic format and worked with teams like Context to ensure that you can load them into their analytics engine and run analytics over them in there. We can’t wait to see what you build. Tags By LangChain Join our newsletter Updates from the LangChain team and community Enter your email Subscribe Processing your application... Success! Please check your inbox and click the link to confirm your subscription. Sorry, something went wrong. Please try again. You might also like [Week of 10/16] LangChain Release Notes Release Notes 3 min read [Week of 10/2] LangChain Release Notes By LangChain 3 min read [Week of 9/18] LangChain Release Notes By LangChain 4 min read [Week of 8/21] LangChain Release Notes Release Notes 3 min read Making Data Ingestion Production Ready: a LangChain-Powered Airbyte Destination 3 min read Conversational Retrieval Agents 4 min read Sign up © LangChain Blog 2023 - Powered by Ghost\n"
     ]
    }
   ],
   "source": [
    "print(len(page_content))\n",
    "print(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models from OpenAI can be found [here](https://platform.openai.com/docs/models/gpt-3-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=model_name) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question is asking for an explanation of what LangSmith is.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith?\",\n",
    "        \"context\": page_content[:10000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a summary template and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"{text}\n",
    "\n",
    "Using the above text, answer in short the following question\n",
    "\n",
    "> {question}\n",
    "\n",
    "---\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats, etc.\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(summary_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a unified platform designed to help developers close the gap between prototype and production of LLM-powered applications. It provides tools for debugging, testing, evaluating, and monitoring LLM applications, offering visibility into model inputs and outputs, dataset creation, performance tracking, and integration with open source evaluation modules. LangSmith has been tested and used by companies such as Snowflake, Boston Consulting Group, DeepLearningAI, and ambitious startups like Mendable, Multi-On, and Quivr. It aims to simplify the development process and provide a single, fully-integrated hub for managing LLM applications.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "chain = summary_prompt | ChatOpenAI(model=model_name) | StrOutputParser()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith\",\n",
    "        \"text\":page_content\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll modify the chain and add more syntax to make it more involved but at the same time more <b>streamlined</b> using the <b>[RunnablePassthrogh](https://nanonets.com/blog/langchain/)</b> calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that scrape_text is the function we created and we are passing it through to be executed within the chain\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | summary_prompt |  ChatOpenAI(model=model_name) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM applications. It helps developers close the gap between prototype and production by providing visibility into model inputs and outputs, tools for creating and running datasets, seamless integration with evaluation modules, and monitoring system-level performance. It has been tested and used by various companies and organizations to improve their LLM applications.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith\",\n",
    "        \"url\":url\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the duckduckgo browser api to look up the url and pass it through the chain we built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_search = 3\n",
    "\n",
    "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "def web_search(query:str, num_results:int = results_per_search):\n",
    "    results = ddg_search.results(query, num_results)\n",
    "\n",
    "    return [r[\"link\"] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of what DDG can do - Notice the return objects: snippet, title and link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippet': 'One Piece is a manga and anime series that follows Monkey D. Luffy, a young boy with a dream to become the greatest pirate in the world. As a child, he eats a mystical plant called a Devil...',\n",
       "  'title': \"What is One Piece? The series' mega-popularity, explained - Polygon\",\n",
       "  'link': 'https://www.polygon.com/entertainment/23845804/one-piece-explained-anime-manga-netflix-live-action'},\n",
       " {'snippet': 'Eiichiro Oda\\'s One Piece debuted in \"Weekly Shonen Jump\" in 1997, with the anime adaptation of the series launching just two years later. Following the adventures of Luffy D Monkey and his pirate crew, the Straw Hats, they\\'re out on the open seas hunting for the amassed treasure of Gol D Roger.',\n",
       "  'title': 'What Is the One Piece? - CBR',\n",
       "  'link': 'https://www.cbr.com/what-is-one-piece-treasure/'},\n",
       " {'snippet': \"Anime One Piece: What Could The One Piece Be? As One Piece's ending looms in the manga, we take some guesses as to what the One Piece is. By Evan Valentine - August 9, 2023 06:25 pm...\",\n",
       "  'title': 'One Piece: What Could The One Piece Be? - ComicBook.com',\n",
       "  'link': 'https://comicbook.com/anime/news/one-piece-what-is-the-one-piece/'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddg_search.results(\"what is the one piece?\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippet': \"Thanks to expanded qualifying with the first 48-team World Cup, Peru is still only four points off a qualification spot, but Juan Reynoso's team will be desperate for all three points against...\",\n",
       "  'title': 'Peru vs. Venezuela: How to watch 2026 World Cup qualifier - Pro Soccer Wire',\n",
       "  'link': 'https://prosoccerwire.usatoday.com/2023/11/20/how-to-watch-peru-vs-venezuela-conmebol-2026-world-cup-qualifier-tv-and-streaming/'},\n",
       " {'snippet': '11/21/2023 TAJ 11/21/2023 11/21/2023 124 99 PAL Venezuela 0 - 0 Bolivia Venezuela vs Peru LIVE Updates: Score, Stream Info, Lineups and How to Watch World Cup Qualifiers 2026 Match',\n",
       "  'title': 'Venezuela vs Peru LIVE Updates: Score, Stream Info, Lineups and How to ...',\n",
       "  'link': 'https://www.vavel.com/en-us/soccer/2023/11/22/1163696-venezuela-vs-peru-live-updates-score-stream-info-lineups-and-how-to-watch-world-cup-qualifiers-2026-match.html'},\n",
       " {'snippet': 'Conmebol odds courtesy of Tipico Sportsbook. Odds last updated Tuesday at 7:20 p.m. ET. Peru (+155) vs. Venezuela (+200) Want some action on the World Cup Qualifiers? Place your legal sports...',\n",
       "  'title': 'Peru vs. Venezuela live stream: TV channel, how to watch - For The Win',\n",
       "  'link': 'https://ftw.usatoday.com/2023/11/how-to-watch-world-cup-qualifiers-peru-vs-venezuela-time-tv-channel-live-stream'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddg_search.results(\"what was the score between peru and venezuela last night?\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly explore what the output from RunnabalePassthrough does with our ddg function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is the one piece?',\n",
       " 'urls': ['https://www.polygon.com/entertainment/23845804/one-piece-explained-anime-manga-netflix-live-action',\n",
       "  'https://www.cbr.com/what-is-one-piece-treasure/',\n",
       "  'https://collider.com/one-piece-arcs-in-order/']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"])\n",
    ")\n",
    "\n",
    "main_chain.invoke({\"question\": \"what is the one piece?\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It not only return the output in dictionary format (urls), but also the input (question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now formulate the entire chain invoking it in each url. Notice how we are <b>>ensuring to pass a dictionary to each RunnablePassthrough</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangSmith is a platform for building production-grade LLM (Language Learning Models) applications. It allows developers to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. LangSmith seamlessly integrates with LangChain, an open-source framework for building with LLMs. It provides tracing and debugging capabilities to help developers understand the inputs and outputs of LLM calls, identify issues, and optimize performance. LangSmith can be used with different programming languages, including Python, JavaScript, and Go. It offers features such as visualization of LLM calls, metadata analysis, and an interactive Playground for editing inputs and parameters.',\n",
       " 'LangSmith is a product created by the team behind LangChain, which is a popular software tool for large language models (LLMs). LangSmith aims to tackle the challenges of getting LLM applications into production in a reliable and maintainable way. It focuses on providing features around debugging, testing, evaluating, monitoring, and usage metrics for LLM applications. LangSmith also offers a simple and intuitive user interface to make these tasks easier for developers, especially those without a software background. While there may be competitors in the market, LangSmith aims to differentiate itself by providing integrations with other tools and services, such as OpenAI evals and fine-tuning providers. The author also mentions the importance of extensibility for LangSmith, allowing it to be built into other applications and services for added value.',\n",
       " 'LangSmith is a product created by the team behind Langchain, a popular software tool for large language models (LLMs). While Langchain focuses on prototyping, LangSmith is designed to help developers bring LLM applications into production in a reliable and maintainable way. It provides features for debugging, testing, evaluating, monitoring, and tracking usage metrics. LangSmith aims to reduce the barrier to entry for developers without a software background by offering a simple and intuitive UI. It also plans to integrate with other tools and platforms in the LLM ecosystem.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From above\n",
    "scrape_and_summary_chain = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | summary_prompt |  ChatOpenAI(model=model_name) | StrOutputParser()\n",
    "\n",
    "# New - Recall from above cell that a dict is returned from RunnablePassthrough.assign() {\"question\", \"urls\"}\n",
    "web_url_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"])\n",
    ") | (lambda x: [{\"question\":x[\"question\"], \"url\":u} for u in x[\"urls\"]]) # What we want is a list of {\"url\":...}\n",
    "\n",
    "# Put together - Map applies chain to every input element. It takes a list of dictionaries\n",
    "question_to_summaries_chain = web_url_chain | scrape_and_summary_chain.map()\n",
    "\n",
    "# Invoke chain\n",
    "question_to_summaries_chain.invoke(\n",
    "    {\n",
    "        \"question\":\"what is langsmith?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to do now is add on top a chain to generate a list of sub-questions to feed the main chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an additional prompt to generate such questions. Inspiration taken from [here](https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py). Refer [here](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) from PromptTemplate guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an AI Research assistant. Your name is {agent_name}\"),\n",
    "        (\"user\",\n",
    "         \"Write 3 google search queries to search online that form an objective opinion from \"\n",
    "         \"the following {question}\\n\"\n",
    "         \"You must respond with a list of strings in the following format:\"\n",
    "         '[\"query 1\", \"query 2\", \"query 3\"].'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it to see what the output is so that you know how to parse it within the same chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='[\"What are the different theories about the meaning of \\'One Piece\\' in the One Piece anime/manga?\", \"What are the most popular fan interpretations of the \\'One Piece\\' in the One Piece series?\", \"What are the possible explanations for the \\'One Piece\\' mystery in the One Piece storyline?\"]')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the one piece?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to modify to output a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"What is the plot of One Piece anime/manga?\", \"One Piece: reviews and critiques\", \"Is One Piece worth watching?\"]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name) | StrOutputParser()\n",
    "\n",
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the one piece?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first convert the string type output, that has an embedded list, into an actual list. This can be achieved by [json.loads()](https://www.geeksforgeeks.org/python-difference-between-json-load-and-json-loads/)\n",
    "\n",
    "Notice that in order to so, we have to make sure that the Prompt instructions have the <b>\"</b> inside the embedded list, and the <b>'</b> at the ends of the string. Otherwise you'll get an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One Piece anime review',\n",
       " 'One Piece manga vs anime comparison',\n",
       " 'One Piece fan discussion']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads('[\"One Piece anime review\", \"One Piece manga vs anime comparison\", \"One Piece fan discussion\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce an error below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JOSE\\OneDrive\\Documents\\PROJECTS\\LLM\\ra.ipynb Cell 40\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m json\u001b[39m.\u001b[39;49mloads(\u001b[39m\"\u001b[39;49m\u001b[39m[\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "json.loads(\"['a', 'b']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can continue and convert the output to a list of strings below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'agent_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JOSE\\OneDrive\\Documents\\PROJECTS\\LLM\\ra.ipynb Cell 42\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m search_questions_chain \u001b[39m=\u001b[39m search_prompt \u001b[39m|\u001b[39m ChatOpenAI(model\u001b[39m=\u001b[39mmodel_name) \u001b[39m|\u001b[39m StrOutputParser() \u001b[39m|\u001b[39m json\u001b[39m.\u001b[39mloads\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m search_questions_chain\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# \"agent_name\":\"El Bryan\",\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mwhat is the diffence between the UEFA Champions and Europe leagues?\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     }\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JOSE/OneDrive/Documents/PROJECTS/LLM/ra.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1426\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1424\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1425\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1426\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1427\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1428\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1429\u001b[0m             patch_config(\n\u001b[0;32m   1430\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1431\u001b[0m             ),\n\u001b[0;32m   1432\u001b[0m         )\n\u001b[0;32m   1433\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\site-packages\\langchain\\schema\\prompt_template.py:60\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     62\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     63\u001b[0m         ),\n\u001b[0;32m     64\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     65\u001b[0m         config,\n\u001b[0;32m     66\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     67\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:847\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    841\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    842\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    843\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    844\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    845\u001b[0m )\n\u001b[0;32m    846\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 847\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    848\u001b[0m         func, \u001b[39minput\u001b[39m, config, run_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    849\u001b[0m     )\n\u001b[0;32m    850\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    851\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:308\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    307\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 308\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\site-packages\\langchain\\schema\\prompt_template.py:62\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 62\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     63\u001b[0m         ),\n\u001b[0;32m     64\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     65\u001b[0m         config,\n\u001b[0;32m     66\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JOSE\\anaconda3\\envs\\lang_ai\\lib\\site-packages\\langchain\\schema\\prompt_template.py:62\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 62\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     63\u001b[0m         ),\n\u001b[0;32m     64\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     65\u001b[0m         config,\n\u001b[0;32m     66\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'agent_name'"
     ]
    }
   ],
   "source": [
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name) | StrOutputParser() | json.loads\n",
    "\n",
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        # \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between the UEFA Champions and Europe leagues?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we <b>need a list of dictionaries to pass it along the chain</b> so that it can be \"mapped.\" So we need to convert it to that.\n",
    "\n",
    "The <b>web_url_chain</b> within the <b>question_to_summaries_chain</b> requires a <b>\"question\"</b> key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Langsmith vs Langchain comparison'},\n",
       " {'question': 'Features of Langsmith'},\n",
       " {'question': 'Features of Langchain'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_questions_chain = search_prompt | ChatOpenAI(model=model_name, temperature=0.3) | StrOutputParser() | json.loads | (\n",
    "    lambda x : [{\"question\":i} for i in x]\n",
    ")\n",
    "\n",
    "search_questions_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between langsmith and langchain?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now <b>pass it to the question_to_summaries_chain</b>. We need to map it, which in essence will invoke the question_to_summaries_chain for each dict input coming out of the search_questions_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['LangSmith is a framework built on top of LangChain. While LangChain is focused on developing LLM applications, LangSmith is a complementary platform that allows users to debug, monitor, test, evaluate, and collaborate on their LLM applications. It provides tools for debugging, testing, evaluating, and monitoring the inner workings of LLMs and AI agents. LangSmith also allows users to track and analyze traces, which are logs that show the text input and output of LLMs. It helps ensure the quality and reliability of AI outputs and has been instrumental in improving user experience. The author of the article states that they are putting a lot of trust in LangSmith and using it extensively for prototyping and debugging.',\n",
       "  'The text mentions that LangSmith is a platform for building production-grade LLM applications, while LangChain is an open-source framework for building with LLMs. LangSmith seamlessly integrates with LangChain, and both are used in the development of OpenGPTs.',\n",
       "  \"LangSmith and Langchain are both tools for working with large language models (LLMs). Langchain focuses on reducing the barrier to entry for building prototypes using LLMs, while LangSmith is designed to help get LLM applications into production in a reliable and maintainable way. LangSmith addresses challenges related to reliability, offering features for debugging, testing, evaluating, monitoring, and collecting usage metrics. It provides a simple and intuitive UI to make these tasks easier for developers, even those without a software background. While LangSmith doesn't have direct competitors, other platforms like Vercel and providers of embeddings may develop similar tooling in the future. The author suggests that LangSmith could benefit from extensibility, allowing its core features to be integrated into other applications and services.\"],\n",
       " ['The text provides a blog post about LangChain, a library for language models. The author mentions the following pros of LangChain: a big and active community, excellent support for retrieval augmented generation and agent execution. However, there are several cons mentioned, including the switch from completion APIs to chat completion APIs, inconsistent abstractions, obfuscated prompts, debugging issues, and competing abstractions in the documentation. The author concludes that despite these drawbacks, LangChain is a healthy open-source codebase worth using modularly. No information is provided about Langsmith, so we cannot comment on its pros and cons based on the given text.',\n",
       "  \"LangSmith is a new product created by the team behind Langchain, a popular software tool for large language models (LLMs). LangSmith aims to address the challenges of building LLM applications for production by providing features related to debugging, testing, evaluating, monitoring, and usage metrics. It offers a simple and intuitive user interface to make these tasks more accessible to developers. LangSmith's value lies in its ability to help developers deploy and maintain LLM applications reliably. While there are potential competitors in the market, LangSmith seeks to differentiate itself through integrations with other tools and platforms, as well as by providing a polished UI experience. As a developer, you should care about LangSmith because it simplifies the process of developing and deploying LLM applications, making it easier to bring them into production and ensure their reliability.\",\n",
       "  'The text does not provide specific information about the pros and cons of LangSmith and LangChain. It mainly discusses the purpose of LangSmith as a tool for production-level development of large language models and its features for debugging, testing, evaluating, monitoring, and usage metrics. It also mentions potential competitors and the need for LangSmith to continue expanding in scope to be competitive.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain = search_questions_chain | question_to_summaries_chain.map()\n",
    "\n",
    "main_chain.invoke(\n",
    "    {\n",
    "        \"agent_name\":\"El Bryan\",\n",
    "        \"question\":\"what is the diffence between langsmith and langchain?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of lists. Now that we have it, all we have to do is pass it to a final prompt to summarize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
