{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph with Multi-Agent Workflow 2 - Own MOD\n",
    "\n",
    "Multiple agents interacting in Workflow design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import operator\n",
    "import functools\n",
    "from loguru import logger\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage\n",
    ")\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import FunctionMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import TypedDict, Annotated, List, Sequence, Literal\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key solely to use embedding model\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = \"\"\"\n",
    "I want to estimate the impact of a drug dosage on an overall survival\n",
    "I don't want to incur in immortality bias.\n",
    "What would the rigth epidemeological approach to do this?\n",
    "\"\"\"\n",
    "sample_result = llm.invoke(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To estimate the impact of a drug dosage on overall survival while avoiding immortality bias, you should consider using time-dependent methods in your epidemiological analysis. Immortality bias occurs when there is a period during which the outcome (death, in this case) cannot occur, often because the exposure (drug dosage) has not yet been initiated. Here are some approaches to mitigate this bias:\n",
      "\n",
      "1. **Time-Dependent Cox Proportional Hazards Model**: This model allows you to account for changes in drug dosage over time. By treating drug dosage as a time-dependent covariate, you can more accurately estimate its impact on survival.\n",
      "\n",
      "2. **Landmark Analysis**: This involves selecting a specific time point (landmark) and including only those patients who have survived up to that point. You then analyze the impact of drug dosage on survival from the landmark time onward. This helps to ensure that all patients included in the analysis have had the opportunity to receive the drug.\n",
      "\n",
      "3. **Marginal Structural Models (MSMs)**: These models use inverse probability weighting to create a pseudo-population in which the treatment assignment is independent of the time-varying confounders. This approach can help to address both time-dependent confounding and immortal time bias.\n",
      "\n",
      "4. **Cumulative Incidence Function**: If competing risks are present (e.g., death from other causes), you might consider using a cumulative incidence function to estimate the probability of the event of interest (death due to the condition being treated by the drug) while accounting for the competing risks.\n",
      "\n",
      "5. **Sequential Stratification**: This involves stratifying the analysis by time intervals and comparing the survival within each interval. This method can help to control for the fact that patients must survive long enough to receive the drug.\n",
      "\n",
      "6. **Delayed-Entry Cox Model**: Also known as left-truncated Cox model, this approach includes patients in the risk set only after they have started the drug, thus avoiding the period of immortality.\n",
      "\n",
      "### Steps to Implement a Time-Dependent Cox Model:\n",
      "\n",
      "1. **Data Preparation**: Ensure your dataset includes time-varying information on drug dosage and survival times.\n",
      "2. **Define Time Intervals**: Split the follow-up time into intervals where the drug dosage is constant.\n",
      "3. **Create Time-Dependent Covariates**: For each interval, create covariates that reflect the drug dosage during that period.\n",
      "4. **Fit the Model**: Use a Cox proportional hazards model with the time-dependent covariates to estimate the hazard ratios for different dosages.\n",
      "\n",
      "### Example in R:\n",
      "\n",
      "```R\n",
      "library(survival)\n",
      "\n",
      "# Assuming 'data' is your dataset with columns: id, start_time, end_time, event, dosage\n",
      "cox_model <- coxph(Surv(start_time, end_time, event) ~ dosage, data = data)\n",
      "summary(cox_model)\n",
      "```\n",
      "\n",
      "By using these methods, you can more accurately estimate the impact of drug dosage on overall survival while minimizing the risk of immortality bias.\n"
     ]
    }
   ],
   "source": [
    "sample_result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl(code: Annotated[str, \"The python code to execute to generate your chart.\"]):\n",
    "    \n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "        \n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    \n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    \n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
